{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.0.0 in /home/jed/.local/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision==0.15.1 in /home/jed/.local/lib/python3.8/site-packages (0.15.1)\n",
      "Requirement already satisfied: filelock in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/jed/.local/lib/python3.8/site-packages (from torch==2.0.0) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/jed/.local/lib/python3.8/site-packages (from torchvision==0.15.1) (1.24.4)\n",
      "Requirement already satisfied: requests in /home/jed/.local/lib/python3.8/site-packages (from torchvision==0.15.1) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/jed/.local/lib/python3.8/site-packages (from torchvision==0.15.1) (9.2.0)\n",
      "Requirement already satisfied: setuptools in /home/jed/.local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.1.0)\n",
      "Requirement already satisfied: wheel in /home/jed/.local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.44.0)\n",
      "Requirement already satisfied: cmake in /home/jed/.local/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.0) (3.27.7)\n",
      "Requirement already satisfied: lit in /home/jed/.local/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.0) (17.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jed/.local/lib/python3.8/site-packages (from jinja2->torch==2.0.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jed/.local/lib/python3.8/site-packages (from requests->torchvision==0.15.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jed/.local/lib/python3.8/site-packages (from requests->torchvision==0.15.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jed/.local/lib/python3.8/site-packages (from requests->torchvision==0.15.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jed/.local/lib/python3.8/site-packages (from requests->torchvision==0.15.1) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jed/.local/lib/python3.8/site-packages (from sympy->torch==2.0.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.0.0 torchvision==0.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pycocoevalcap in /home/jed/.local/lib/python3.8/site-packages (1.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/jed/.local/lib/python3.8/site-packages (from pycocoevalcap) (2.0.7)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/jed/.local/lib/python3.8/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.4)\n",
      "Requirement already satisfied: numpy in /home/jed/.local/lib/python3.8/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.24.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jed/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jed/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jed/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jed/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jed/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jed/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jed/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jed/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/jed/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/jed/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer for sentence splitting\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size=256, device='cuda'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Load the pre-trained Mask R-CNN model\n",
    "        self.mask_rcnn = maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "        self.mask_rcnn.to(self.device)\n",
    "        self.mask_rcnn.eval()\n",
    "\n",
    "        # Freeze Mask R-CNN parameters\n",
    "        for param in self.mask_rcnn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Access the backbone, RPN, and ROI Heads from Mask R-CNN\n",
    "        self.backbone = self.mask_rcnn.backbone\n",
    "        self.rpn = self.mask_rcnn.rpn\n",
    "        self.roi_heads = self.mask_rcnn.roi_heads\n",
    "        self.transform = self.mask_rcnn.transform\n",
    "\n",
    "        # Global feature embedding\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.global_embed = nn.Linear(self.backbone.out_channels, embed_size)\n",
    "\n",
    "        # Object feature embedding\n",
    "        # The output of the box head is 1024-dimensional\n",
    "        self.obj_embed = nn.Linear(1024, embed_size)\n",
    "\n",
    "        # Initialize weights of the linear layers\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize weights of the linear layers using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.global_embed.weight)\n",
    "        nn.init.zeros_(self.global_embed.bias)\n",
    "        nn.init.xavier_uniform_(self.obj_embed.weight)\n",
    "        nn.init.zeros_(self.obj_embed.bias)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        # Override the train method to prevent Mask R-CNN from switching to train mode\n",
    "        super(EncoderCNN, self).train(mode)\n",
    "        self.mask_rcnn.eval()  # Ensure Mask R-CNN stays in eval mode\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "        Args:\n",
    "            images: Tensor of shape (batch_size, C, H, W)\n",
    "        Returns:\n",
    "            combined_features: Tensor of shape (batch_size, embed_size * 2)\n",
    "        \"\"\"\n",
    "        # Convert the batch tensor to a list of individual image tensors\n",
    "        images = list(images)\n",
    "\n",
    "        # Use Mask R-CNN's transform to preprocess the images\n",
    "        transformed_images, _ = self.transform(images)\n",
    "\n",
    "        # Extract features using the backbone\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(transformed_images.tensors)\n",
    "\n",
    "        # Global feature extraction\n",
    "        # Use the highest resolution feature map (assuming key '0')\n",
    "        feature_map = features['0']  # Shape: (batch_size, C, H, W)\n",
    "        global_features = self.global_pool(feature_map)  # Shape: (batch_size, C, 1, 1)\n",
    "        global_features = global_features.view(global_features.size(0), -1)  # Shape: (batch_size, C)\n",
    "        global_features = self.global_embed(global_features)  # Shape: (batch_size, embed_size)\n",
    "\n",
    "        # Object detection and feature extraction\n",
    "        # Get proposals from RPN\n",
    "        with torch.no_grad():\n",
    "            proposals, _ = self.rpn(transformed_images, features, None)\n",
    "\n",
    "        # Get detections from ROI Heads\n",
    "        detections, _ = self.roi_heads(features, proposals, transformed_images.image_sizes, None)\n",
    "\n",
    "        # Extract object features using the box head\n",
    "        object_features_list = []\n",
    "        for i in range(len(detections)):\n",
    "            boxes = detections[i]['boxes']  # Bounding boxes for detected objects\n",
    "\n",
    "            if boxes.shape[0] > 0:\n",
    "                # Perform RoI pooling on the detected boxes\n",
    "                box_features = self.roi_heads.box_roi_pool(\n",
    "                    features, [boxes], [transformed_images.image_sizes[i]]\n",
    "                )\n",
    "                # Pass through the box head to get object features\n",
    "                box_features = self.roi_heads.box_head(box_features)  # Shape: (num_boxes, 1024)\n",
    "                # Transform to embed_size\n",
    "                object_features = self.obj_embed(box_features)  # Shape: (num_boxes, embed_size)\n",
    "                # Aggregate object features by averaging\n",
    "                aggregated_feats = object_features.mean(dim=0)  # Shape: (embed_size,)\n",
    "            else:\n",
    "                # If no objects detected, use a zero vector\n",
    "                aggregated_feats = torch.zeros(self.obj_embed.out_features).to(self.device)\n",
    "\n",
    "            object_features_list.append(aggregated_feats)\n",
    "\n",
    "        # Combine object features into a tensor\n",
    "        object_features = torch.stack(object_features_list, dim=0)  # Shape: (batch_size, embed_size)\n",
    "\n",
    "        # Concatenate global and object features\n",
    "        combined_features = torch.cat([global_features, object_features], dim=1)  # Shape: (batch_size, embed_size * 2)\n",
    "        return combined_features\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Initialize weights for input, forget, cell, and output gates\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Input gate parameters\n",
    "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.U_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Forget gate parameters\n",
    "        self.W_f = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Cell gate parameters\n",
    "        self.W_c = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.U_c = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Output gate parameters\n",
    "        self.W_o = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize all weights and biases\n",
    "        for param in self.parameters():\n",
    "            if param.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            else:\n",
    "                nn.init.zeros_(param.data)\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        # Compute gates\n",
    "        i_t = torch.sigmoid(x @ self.W_i + h_prev @ self.U_i + self.b_i)\n",
    "        f_t = torch.sigmoid(x @ self.W_f + h_prev @ self.U_f + self.b_f)\n",
    "        g_t = torch.tanh(x @ self.W_c + h_prev @ self.U_c + self.b_c)\n",
    "        o_t = torch.sigmoid(x @ self.W_o + h_prev @ self.U_o + self.b_o)\n",
    "\n",
    "        # Update cell state\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        # Update hidden state\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, vocab_size, dropout=0.5):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Embedding layer to convert word indices to embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Adjusted input size for the feature projection layer\n",
    "        self.feature_proj = nn.Linear(input_size, embed_size)\n",
    "        # Custom LSTM cell\n",
    "        self.lstm_cell = LSTM(embed_size, hidden_size)\n",
    "        # Fully connected layer to project hidden state to vocabulary space\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize weights for embedding and fully connected layers\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.fc.weight, -0.1, 0.1)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.xavier_uniform_(self.feature_proj.weight)\n",
    "        nn.init.zeros_(self.feature_proj.bias)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "        Args:\n",
    "            features: Combined image features from the encoder, shape (batch_size, input_size)\n",
    "            captions: Caption sequences, shape (batch_size, max_seq_length)\n",
    "        Returns:\n",
    "            outputs: Predicted word distributions, shape (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # Project the combined features to embed_size\n",
    "        features = self.feature_proj(features)  # Shape: (batch_size, embed_size)\n",
    "\n",
    "        # Embed the captions (exclude the last word for teacher forcing)\n",
    "        embeddings = self.embedding(captions[:, :-1])  # Shape: (batch_size, seq_len - 1, embed_size)\n",
    "        # Concatenate image features as the first input\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        batch_size, seq_len, _ = embeddings.size()\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.fc.out_features).to(features.device)\n",
    "\n",
    "        # Initialize hidden and cell states to zeros\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size).to(features.device)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_size).to(features.device)\n",
    "\n",
    "        # Unroll the LSTM for seq_len time steps\n",
    "        for t in range(seq_len):\n",
    "            x_t = embeddings[:, t, :]  # Input at time step t\n",
    "            h_t, c_t = self.lstm_cell(x_t, h_t, c_t)  # Update hidden and cell states\n",
    "            output = self.fc(h_t)  # Compute output word distribution\n",
    "            outputs[:, t, :] = output  # Store output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, max_len=20, end_token_idx=None):\n",
    "        \"\"\"\n",
    "        Generate captions for given image features using greedy search.\n",
    "        Args:\n",
    "            features: Combined image features from the encoder, shape (1, input_size)\n",
    "            max_len: Maximum length of the generated caption\n",
    "            end_token_idx: Index of the <end> token\n",
    "        Returns:\n",
    "            sampled_ids: List of predicted word indices\n",
    "        \"\"\"\n",
    "        # Project the combined features to embed_size\n",
    "        features = self.feature_proj(features)  # Shape: (1, embed_size)\n",
    "\n",
    "        sampled_ids = []\n",
    "        inputs = features  # Initial input is the image features\n",
    "        h_t = torch.zeros(1, self.hidden_size).to(features.device)\n",
    "        c_t = torch.zeros(1, self.hidden_size).to(features.device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            h_t, c_t = self.lstm_cell(inputs, h_t, c_t)\n",
    "            outputs = self.fc(h_t)  # Compute word distribution\n",
    "            predicted = outputs.argmax(1)  # Get the index with the highest probability\n",
    "            sampled_ids.append(predicted.item())\n",
    "\n",
    "            if predicted.item() == end_token_idx:\n",
    "                break  # Stop if <end> token is generated\n",
    "\n",
    "            # Prepare input for next time step\n",
    "            inputs = self.embedding(predicted)\n",
    "            inputs = self.dropout(inputs)\n",
    "\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading Flickr images and captions.\n",
    "    This dataset handles the basic functionality of loading images and their corresponding captions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, image_ids, captions_seqs, transform=None, mode='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all the images.\n",
    "            image_ids (list): List of image filenames.\n",
    "            captions_seqs (dict): Dictionary mapping image filenames to caption sequences.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "            mode (str): Mode of the dataset, 'train' or 'test'.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "        self.image_ids = []\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # Pair each image with its captions for training\n",
    "            for img_id in image_ids:\n",
    "                captions = captions_seqs[img_id]\n",
    "                for caption_seq in captions:\n",
    "                    self.images.append(img_id)\n",
    "                    self.captions.append(caption_seq)\n",
    "        elif self.mode == 'test':\n",
    "            # For testing, pair each image with all its captions\n",
    "            for img_id in image_ids:\n",
    "                captions = captions_seqs[img_id]\n",
    "                # Optionally, you can choose to handle multiple captions per image\n",
    "                # Here, we'll keep one caption per image for simplicity\n",
    "                if captions:\n",
    "                    caption_seq = random.choice(captions)\n",
    "                else:\n",
    "                    caption_seq = []  # Handle images without captions appropriately\n",
    "                self.images.append(img_id)\n",
    "                self.captions.append(caption_seq)\n",
    "                self.image_ids.append(img_id)\n",
    "        else:\n",
    "            raise ValueError(\"Mode should be either 'train' or 'test'.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of image-caption pairs.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and caption at the specified index.\n",
    "        Args:\n",
    "            idx (int): Index\n",
    "        Returns:\n",
    "            image (Tensor): Transformed image tensor.\n",
    "            caption_seq (Tensor): Corresponding caption sequence tensor.\n",
    "            image_id (str): Filename of the image.\n",
    "        \"\"\"\n",
    "        img_id = self.images[idx]\n",
    "        caption_seq = self.captions[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_id)\n",
    "\n",
    "        # Open and convert image to RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption sequence to tensor\n",
    "        caption_seq = torch.tensor(caption_seq)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            return image, caption_seq\n",
    "        elif self.mode == 'test':\n",
    "            return image, caption_seq, img_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length captions.\n",
    "    This function pads captions to the length of the longest caption in the batch.\n",
    "    Args:\n",
    "        batch (list): List of tuples (image, caption_seq) or (image, caption_seq, image_id)\n",
    "    Returns:\n",
    "        If training:\n",
    "            images (Tensor): Batch of images.\n",
    "            targets (Tensor): Padded caption sequences.\n",
    "            lengths (list): Original lengths of each caption before padding.\n",
    "        If testing:\n",
    "            images (Tensor): Batch of images.\n",
    "            targets (Tensor): Padded caption sequences.\n",
    "            image_ids (list): List of image filenames.\n",
    "    \"\"\"\n",
    "    if len(batch[0]) == 3:\n",
    "        # Test mode\n",
    "        images, captions, image_ids = zip(*batch)\n",
    "    else:\n",
    "        # Train mode\n",
    "        images, captions = zip(*batch)\n",
    "        image_ids = None\n",
    "\n",
    "    # Stack images\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Get lengths of each caption\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "\n",
    "    # Pad captions to the length of the longest caption\n",
    "    max_length = max(lengths)\n",
    "    targets = torch.zeros(len(captions), max_length).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "\n",
    "    if image_ids is not None:\n",
    "        return images, targets, image_ids\n",
    "    else:\n",
    "        return images, targets, lengths\n",
    "\n",
    "def get_transform(train=True):\n",
    "    \"\"\"\n",
    "    Returns the image transformations for training or evaluation.\n",
    "    Args:\n",
    "        train (bool): Flag indicating whether transformations are for training or evaluation.\n",
    "    Returns:\n",
    "        transform (callable): Composed transformations.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                std=[0.229, 0.224, 0.225],   # ImageNet std\n",
    "            ),\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                std=[0.229, 0.224, 0.225],   # ImageNet std\n",
    "            ),\n",
    "        ])\n",
    "    return transform\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')  # Ensure the Punkt tokenizer is downloaded\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text into words.\n",
    "    Args:\n",
    "        text (str): Input caption text.\n",
    "    Returns:\n",
    "        tokens (list): List of word tokens.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def build_vocabulary(caption_df, vocab_size=5000):\n",
    "    \"\"\"\n",
    "    Builds word-to-index and index-to-word mappings based on caption data.\n",
    "    Args:\n",
    "        caption_df (DataFrame): DataFrame containing image filenames and their captions.\n",
    "        vocab_size (int): Maximum size of the vocabulary.\n",
    "    Returns:\n",
    "        word2idx (dict): Mapping from words to indices.\n",
    "        idx2word (dict): Mapping from indices to words.\n",
    "        image_captions (dict): Mapping from image filenames to their captions.\n",
    "    \"\"\"\n",
    "    # Group captions by image\n",
    "    image_captions = caption_df.groupby(\"image\")[\"caption\"].apply(list).to_dict()\n",
    "\n",
    "    # Collect all captions\n",
    "    all_captions = [\n",
    "        caption for captions in image_captions.values() for caption in captions\n",
    "    ]\n",
    "\n",
    "    # Tokenize all captions and count word frequencies\n",
    "    all_words = [token for caption in all_captions for token in tokenize(caption)]\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Define special tokens\n",
    "    special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "    # Initialize word-to-index and index-to-word mappings\n",
    "    word2idx = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "    idx2word = {idx: token for idx, token in enumerate(special_tokens)}\n",
    "\n",
    "    # Add most common words to the vocabulary\n",
    "    most_common = word_counts.most_common(vocab_size - len(special_tokens))\n",
    "    for idx, (word, _) in enumerate(most_common, start=len(special_tokens)):\n",
    "        word2idx[word] = idx\n",
    "        idx2word[idx] = word\n",
    "\n",
    "    return word2idx, idx2word, image_captions\n",
    "\n",
    "def convert_captions_to_sequences(image_captions, word2idx):\n",
    "    \"\"\"\n",
    "    Converts captions to sequences of word indices.\n",
    "    Args:\n",
    "        image_captions (dict): Mapping from image filenames to their captions.\n",
    "        word2idx (dict): Mapping from words to indices.\n",
    "    Returns:\n",
    "        captions_seqs (dict): Mapping from image filenames to sequences of word indices.\n",
    "        max_length (int): Maximum length of the captions.\n",
    "    \"\"\"\n",
    "    captions_seqs = {}\n",
    "    max_length = 0\n",
    "\n",
    "    for img_name, captions in image_captions.items():\n",
    "        seqs = []\n",
    "        for caption in captions:\n",
    "            # Tokenize and add start and end tokens\n",
    "            tokens = [\"<start>\"] + tokenize(caption) + [\"<end>\"]\n",
    "            # Convert tokens to indices, use <unk> for unknown words\n",
    "            seq = [word2idx.get(token, word2idx[\"<unk>\"]) for token in tokens]\n",
    "            seqs.append(seq)\n",
    "            # Update maximum caption length\n",
    "            max_length = max(max_length, len(seq))\n",
    "        captions_seqs[img_name] = seqs\n",
    "\n",
    "    return captions_seqs, max_length\n",
    "\n",
    "def get_splits(image_names, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training, validation, and test sets.\n",
    "    Args:\n",
    "        image_names (list): List of image filenames.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "    Returns:\n",
    "        train_images (list): List of training image filenames.\n",
    "        val_images (list): List of validation image filenames.\n",
    "        test_images (list): List of test image filenames.\n",
    "    \"\"\"\n",
    "    # Split into training and temp (validation + test) sets\n",
    "    train_images, temp_images = train_test_split(\n",
    "        image_names, test_size=test_size, random_state=42\n",
    "    )\n",
    "    # Split temp set into validation and test sets\n",
    "    val_images, test_images = train_test_split(\n",
    "        temp_images, test_size=0.1, random_state=42\n",
    "    )\n",
    "    return train_images, val_images, test_images\n",
    "\n",
    "def prepare_image2captions(image_ids, captions_seqs, idx2word):\n",
    "    \"\"\"\n",
    "    Prepares a mapping from image IDs to their corresponding captions in word form.\n",
    "    Args:\n",
    "        image_ids (list): List of image filenames.\n",
    "        captions_seqs (dict): Mapping from image filenames to sequences of word indices.\n",
    "        idx2word (dict): Mapping from indices to words.\n",
    "    Returns:\n",
    "        image2captions (dict): Mapping from image filenames to their captions as word lists.\n",
    "    \"\"\"\n",
    "    image2captions = {}\n",
    "    for img_id in image_ids:\n",
    "        seqs = captions_seqs[img_id]\n",
    "        captions_list = []\n",
    "        for seq in seqs:\n",
    "            # Convert indices back to words\n",
    "            caption = [idx2word.get(idx, \"<unk>\") for idx in seq]\n",
    "            # Remove special tokens\n",
    "            caption = [\n",
    "                word.lower()\n",
    "                for word in caption\n",
    "                if word not in [\"<start>\", \"<end>\", \"<pad>\"]\n",
    "            ]\n",
    "            captions_list.append(caption)\n",
    "        image2captions[img_id] = captions_list\n",
    "    return image2captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from PIL import Image\n",
    "\n",
    "# Evaluate function: Computes validation loss on a given dataset\n",
    "def evaluate(encoder, decoder, data_loader, criterion, device, vocab_size):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    Args:\n",
    "        encoder: Encoder model.\n",
    "        decoder: Decoder model.\n",
    "        data_loader: DataLoader for the validation set.\n",
    "        criterion: Loss function.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        vocab_size: Size of the vocabulary.\n",
    "    Returns:\n",
    "        average_loss: Average validation loss.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, captions, _ in data_loader:\n",
    "            # Move data to the computation device\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass through encoder and decoder\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Exclude the first time step from outputs and targets\n",
    "            outputs = outputs[:, 1:, :]  # Ensure outputs and targets have the same length\n",
    "            targets = captions[:, 1:]  # Exclude the first <start> token from targets\n",
    "\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "    # Calculate average loss\n",
    "    average_loss = total_loss / total_samples\n",
    "    return average_loss\n",
    "\n",
    "# Function to calculate BLEU score for generated captions\n",
    "def calculate_bleu_score(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    image_dir,\n",
    "    image_ids,\n",
    "    image2captions,\n",
    "    transform,\n",
    "    idx2word,\n",
    "    device,\n",
    "    word2idx,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for the generated captions.\n",
    "    Args:\n",
    "        encoder: Encoder model.\n",
    "        decoder: Decoder model.\n",
    "        image_dir: Directory containing images.\n",
    "        image_ids: List of image IDs.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        idx2word: Mapping from word indices to words.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        word2idx: Mapping from words to word indices.\n",
    "    Returns:\n",
    "        bleu_score: Corpus BLEU score for generated captions.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    references = []  # List to store reference captions\n",
    "    hypotheses = []  # List to store generated captions\n",
    "    smoothie = SmoothingFunction().method4  # Smoothing function for BLEU score\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_id in image_ids:\n",
    "            # Load and preprocess image\n",
    "            img_path = os.path.join(image_dir, img_id)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption\n",
    "            features = encoder(image)\n",
    "            end_token_idx = word2idx[\"<end>\"]\n",
    "            sampled_ids = decoder.sample(features, end_token_idx=end_token_idx)\n",
    "            sampled_caption = [\n",
    "                idx2word.get(word_id, \"<unk>\") for word_id in sampled_ids\n",
    "            ]\n",
    "\n",
    "            # Prepare hypothesis (generated caption tokens)\n",
    "            hypothesis = [\n",
    "                word.lower()\n",
    "                for word in sampled_caption\n",
    "                if word not in [\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"]\n",
    "            ]\n",
    "            hypotheses.append(hypothesis)\n",
    "\n",
    "            # Prepare references (list of lists of tokens)\n",
    "            ref_captions = image2captions[img_id]\n",
    "            refs = [\n",
    "                [\n",
    "                    word.lower()\n",
    "                    for word in ref\n",
    "                    if word not in [\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"]\n",
    "                ]\n",
    "                for ref in ref_captions\n",
    "            ]\n",
    "            references.append(refs)\n",
    "\n",
    "    # Compute corpus BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "    return bleu_score\n",
    "\n",
    "# Function to calculate METEOR score for generated captions\n",
    "def calculate_meteor_score(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    image_dir,\n",
    "    image_ids,\n",
    "    image2captions,\n",
    "    transform,\n",
    "    idx2word,\n",
    "    device,\n",
    "    word2idx,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate METEOR score for the generated captions.\n",
    "    Args:\n",
    "        encoder: Encoder model.\n",
    "        decoder: Decoder model.\n",
    "        image_dir: Directory containing images.\n",
    "        image_ids: List of image IDs.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        idx2word: Mapping from word indices to words.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        word2idx: Mapping from words to word indices.\n",
    "    Returns:\n",
    "        average_meteor: Average METEOR score.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    meteor_scores = []  # List to store METEOR scores\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_id in image_ids:\n",
    "            # Load and preprocess image\n",
    "            img_path = os.path.join(image_dir, img_id)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption\n",
    "            features = encoder(image)\n",
    "            end_token_idx = word2idx[\"<end>\"]\n",
    "            sampled_ids = decoder.sample(features, end_token_idx=end_token_idx)\n",
    "            sampled_caption = [\n",
    "                idx2word.get(word_id, \"<unk>\") for word_id in sampled_ids\n",
    "            ]\n",
    "\n",
    "            # Prepare hypothesis (generated caption tokens)\n",
    "            hypothesis = [\n",
    "                word.lower()\n",
    "                for word in sampled_caption\n",
    "                if word not in [\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"]\n",
    "            ]\n",
    "\n",
    "            # Prepare references (list of lists of tokens)\n",
    "            references = [\n",
    "                [\n",
    "                    word.lower()\n",
    "                    for word in ref\n",
    "                    if word not in [\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"]\n",
    "                ]\n",
    "                for ref in image2captions[img_id]\n",
    "            ]\n",
    "\n",
    "            # Calculate METEOR score for the current image\n",
    "            score = meteor_score(references, hypothesis)\n",
    "            meteor_scores.append(score)\n",
    "\n",
    "    # Compute average METEOR score\n",
    "    average_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "    return average_meteor\n",
    "\n",
    "# Function to calculate CIDEr score for generated captions\n",
    "def calculate_cider_score(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    image_dir,\n",
    "    image_ids,\n",
    "    image2captions,\n",
    "    transform,\n",
    "    idx2word,\n",
    "    device,\n",
    "    word2idx,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate CIDEr score for the generated captions.\n",
    "    Args:\n",
    "        encoder: Encoder model.\n",
    "        decoder: Decoder model.\n",
    "        image_dir: Directory containing images.\n",
    "        image_ids: List of image IDs.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        idx2word: Mapping from word indices to words.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        word2idx: Mapping from words to word indices.\n",
    "    Returns:\n",
    "        cider_score: CIDEr score for generated captions.\n",
    "    \"\"\"\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "    from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    gts = {}  # Ground truth captions\n",
    "    res = {}  # Generated captions\n",
    "    tokenizer = PTBTokenizer()  # Tokenizer for captions\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_id in image_ids:\n",
    "            # Load and preprocess image\n",
    "            img_path = os.path.join(image_dir, img_id)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption\n",
    "            features = encoder(image)\n",
    "            end_token_idx = word2idx[\"<end>\"]\n",
    "            sampled_ids = decoder.sample(features, end_token_idx=end_token_idx)\n",
    "            sampled_caption = [\n",
    "                idx2word.get(word_id, \"<unk>\") for word_id in sampled_ids\n",
    "            ]\n",
    "            # Prepare generated caption\n",
    "            sampled_caption = [\n",
    "                word.lower()\n",
    "                for word in sampled_caption\n",
    "                if word not in [\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"]\n",
    "            ]\n",
    "            sampled_caption_str = \" \".join(sampled_caption)\n",
    "\n",
    "            # Prepare references\n",
    "            references = [\n",
    "                \" \".join(\n",
    "                    [\n",
    "                        word.lower()\n",
    "                        for word in ref\n",
    "                        if word not in [\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"]\n",
    "                    ]\n",
    "                )\n",
    "                for ref in image2captions[img_id]\n",
    "            ]\n",
    "\n",
    "            # Update dictionaries with tokenized captions\n",
    "            gts[img_id] = [{'caption': ref} for ref in references]\n",
    "            res[img_id] = [{'caption': sampled_caption_str}]\n",
    "\n",
    "    # Tokenize captions\n",
    "    gts = tokenizer.tokenize(gts)\n",
    "    res = tokenizer.tokenize(res)\n",
    "\n",
    "    # Compute CIDEr score\n",
    "    cider_scorer = Cider()\n",
    "    cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "    return cider_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define dataset type\n",
    "    dataset = \"Flickr8k\"  # Change to \"Flickr30k\" if needed\n",
    "\n",
    "    # Paths\n",
    "    dataset_dir = f\"../../flickr_data/{dataset}_Dataset/Images\"\n",
    "    captions_file = f\"../../flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "    image_dir = dataset_dir\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    cider_scores = []\n",
    "    \n",
    "    # Load captions\n",
    "    caption_df = pd.read_csv(captions_file).dropna().drop_duplicates()\n",
    "    print(f\"Total captions loaded: {len(caption_df)}\")\n",
    "\n",
    "    # Build vocabulary\n",
    "    word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=5000)\n",
    "    print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "    # Convert captions to sequences\n",
    "    captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "    print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "    # Get data transformations\n",
    "    train_transform = get_transform(train=True)\n",
    "    val_transform = get_transform(train=False)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    image_names = list(image_captions.keys())\n",
    "    train_images, val_images, _ = get_splits(image_names, test_size=0.2)\n",
    "    print(f\"Training samples: {len(train_images)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = FlickrDataset(\n",
    "        image_dir, train_images, captions_seqs, transform=train_transform\n",
    "    )\n",
    "    val_dataset = FlickrDataset(\n",
    "        image_dir, val_images, captions_seqs, transform=val_transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32, \n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize models\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    vocab_size = len(word2idx)\n",
    "    input_size = embed_size * 2  # Combined feature size (global + object features)\n",
    "\n",
    "    # Initialize models\n",
    "    encoder = EncoderCNN(embed_size=embed_size, device=device).to(device)\n",
    "    decoder = DecoderRNN(\n",
    "        input_size=input_size,\n",
    "        embed_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        vocab_size=vocab_size\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
    "    params = list(filter(lambda p: p.requires_grad, encoder.parameters())) + list(\n",
    "        decoder.parameters()\n",
    "    )\n",
    "    optimizer = optim.Adam(params, lr=1e-4)\n",
    "    # Since you can only run once, we might not need a scheduler\n",
    "    # Adjust learning rate manually if needed\n",
    "\n",
    "    # Prepare image to captions mapping for evaluation\n",
    "    val_image2captions = prepare_image2captions(val_images, captions_seqs, idx2word)\n",
    "\n",
    "    # Training settings\n",
    "    num_epochs = 10  # Adjust as needed\n",
    "    total_step = len(train_loader)\n",
    "    end_token_idx = word2idx[\"<end>\"]\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (images, captions, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Exclude the first time step from outputs\n",
    "            outputs = outputs[:, 1:, :]  # Shape: (batch_size, seq_len -1 , vocab_size)\n",
    "            targets = captions[:, 1:]  # Exclude the first <start> token\n",
    "\n",
    "            # Reshape for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 300 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = total_loss / total_step\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate(encoder, decoder, val_loader, criterion, device, vocab_size)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        bleu = calculate_bleu_score(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_dir,\n",
    "            val_images,\n",
    "            val_image2captions,\n",
    "            val_transform,\n",
    "            idx2word,\n",
    "            device,\n",
    "            word2idx,\n",
    "        )\n",
    "        meteor = calculate_meteor_score(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_dir,\n",
    "            val_images,\n",
    "            val_image2captions,\n",
    "            val_transform,\n",
    "            idx2word,\n",
    "            device,\n",
    "            word2idx,\n",
    "        )\n",
    "        cider = calculate_cider_score(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_dir,\n",
    "            val_images,\n",
    "            val_image2captions,\n",
    "            val_transform,\n",
    "            idx2word,\n",
    "            device,\n",
    "            word2idx,\n",
    "        )\n",
    "\n",
    "        # Print epoch summary\n",
    "        epoch_duration = time.time() - start_time\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Training Loss: {avg_train_loss:.4f}, \"\n",
    "            f\"Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"BLEU: {bleu:.4f}, \"\n",
    "            f\"METEOR: {meteor:.4f}, \"\n",
    "            f\"CIDEr: {cider:.4f}, \"\n",
    "            f\"Time: {epoch_duration:.2f}s\"\n",
    "        )\n",
    "        \n",
    "        # **Append average training loss instead of total loss**\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu)\n",
    "        meteor_scores.append(meteor)\n",
    "        cider_scores.append(cider)\n",
    "\n",
    "    # Save the models\n",
    "    os.makedirs(\"models/model_2_image_segmentation_lstm\", exist_ok=True)\n",
    "    torch.save(encoder.state_dict(), \"models/model_2_image_segmentation_lstm/encoder.pth\")\n",
    "    torch.save(decoder.state_dict(), \"models/model_2_image_segmentation_lstm/decoder.pth\")\n",
    "    print(\"Models saved successfully.\")\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_2_image_segmentation_lstm/loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), bleu_scores, label='BLEU Score')\n",
    "    plt.plot(range(1, num_epochs + 1), meteor_scores, label='METEOR Score')\n",
    "    plt.plot(range(1, num_epochs + 1), cider_scores, label='CIDEr Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Evaluation Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_2_image_segmentation_lstm/metrics_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions loaded: 40445\n",
      "Vocabulary size: 5000\n",
      "Maximum caption length: 40\n",
      "Training samples: 6472\n",
      "Validation samples: 1457\n",
      "Number of training batches: 1011\n",
      "Number of validation batches: 228\n",
      "Using device: cuda\n",
      "Epoch [1/10], Step [0/1011], Loss: 8.5263\n",
      "Epoch [1/10], Step [300/1011], Loss: 4.7293\n",
      "Epoch [1/10], Step [600/1011], Loss: 4.2431\n",
      "Epoch [1/10], Step [900/1011], Loss: 4.0574\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions loaded: 40445\n",
      "Vocabulary size: 5000\n",
      "Maximum caption length: 40\n",
      "Training samples: 5663\n",
      "Validation samples: 1214\n",
      "Number of training batches: 443\n",
      "Number of validation batches: 95\n",
      "Using device: cuda\n",
      "Epoch [1/10], Step [0/443], Loss: 8.5226\n",
      "Epoch [1/10], Step [250/443], Loss: 4.8943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 604079.23 tokens per second.\n",
      "PTBTokenizer tokenized 12137 tokens at 178160.94 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 5.1644, Validation Loss: 4.5385, BLEU: 0.0628, METEOR: 0.2274, CIDEr: 0.0520, Time: 1652.05s\n",
      "Epoch [2/10], Step [0/443], Loss: 4.5203\n",
      "Epoch [2/10], Step [250/443], Loss: 4.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 600593.52 tokens per second.\n",
      "PTBTokenizer tokenized 18105 tokens at 240000.19 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 4.3273, Validation Loss: 4.1133, BLEU: 0.0548, METEOR: 0.2730, CIDEr: 0.0824, Time: 1648.78s\n",
      "Epoch [3/10], Step [0/443], Loss: 4.2273\n",
      "Epoch [3/10], Step [250/443], Loss: 3.7781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 602266.88 tokens per second.\n",
      "PTBTokenizer tokenized 18371 tokens at 240267.32 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 3.9480, Validation Loss: 3.8199, BLEU: 0.0737, METEOR: 0.3094, CIDEr: 0.1460, Time: 1650.13s\n",
      "Epoch [4/10], Step [0/443], Loss: 4.0033\n",
      "Epoch [4/10], Step [250/443], Loss: 3.6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 602651.70 tokens per second.\n",
      "PTBTokenizer tokenized 17944 tokens at 236690.10 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 3.7100, Validation Loss: 3.6409, BLEU: 0.0754, METEOR: 0.2974, CIDEr: 0.1628, Time: 1647.24s\n",
      "Epoch [5/10], Step [0/443], Loss: 3.5938\n",
      "Epoch [5/10], Step [250/443], Loss: 3.5168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 599218.59 tokens per second.\n",
      "PTBTokenizer tokenized 15235 tokens at 208185.08 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 3.5450, Validation Loss: 3.5092, BLEU: 0.1002, METEOR: 0.3227, CIDEr: 0.1924, Time: 1641.45s\n",
      "Epoch [6/10], Step [0/443], Loss: 3.3434\n",
      "Epoch [6/10], Step [250/443], Loss: 3.5189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 600269.87 tokens per second.\n",
      "PTBTokenizer tokenized 14860 tokens at 203758.95 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 3.4182, Validation Loss: 3.4157, BLEU: 0.1119, METEOR: 0.3361, CIDEr: 0.2282, Time: 1646.17s\n",
      "Epoch [7/10], Step [0/443], Loss: 3.3806\n",
      "Epoch [7/10], Step [250/443], Loss: 3.4217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 597234.62 tokens per second.\n",
      "PTBTokenizer tokenized 14921 tokens at 203617.09 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 3.3160, Validation Loss: 3.3350, BLEU: 0.1253, METEOR: 0.3585, CIDEr: 0.2606, Time: 1643.65s\n",
      "Epoch [8/10], Step [0/443], Loss: 3.4387\n",
      "Epoch [8/10], Step [250/443], Loss: 3.0857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 613918.33 tokens per second.\n",
      "PTBTokenizer tokenized 15646 tokens at 212247.02 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 3.2297, Validation Loss: 3.2728, BLEU: 0.1195, METEOR: 0.3495, CIDEr: 0.2583, Time: 1643.00s\n",
      "Epoch [9/10], Step [0/443], Loss: 2.9769\n",
      "Epoch [9/10], Step [250/443], Loss: 3.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 600416.36 tokens per second.\n",
      "PTBTokenizer tokenized 15075 tokens at 204673.55 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 3.1562, Validation Loss: 3.2245, BLEU: 0.1328, METEOR: 0.3646, CIDEr: 0.2890, Time: 1642.47s\n",
      "Epoch [10/10], Step [0/443], Loss: 3.0885\n",
      "Epoch [10/10], Step [250/443], Loss: 3.2401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 76799 tokens at 598609.04 tokens per second.\n",
      "PTBTokenizer tokenized 15604 tokens at 210714.61 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 3.0911, Validation Loss: 3.1788, BLEU: 0.1383, METEOR: 0.3685, CIDEr: 0.3079, Time: 1640.26s\n",
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
