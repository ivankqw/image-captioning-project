{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/jed/anaconda3/omscs/CS7643/image-captioning-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path(os.getcwd()).resolve()  # Get the current working directory\n",
    "project_root = notebook_dir.parents[1]  # Adjust the number to go up to the project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_2_image_segmentation_lstm.model import *\n",
    "from data.dataset import *\n",
    "from data.preprocessing import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define dataset type\n",
    "    dataset = \"Flickr8k\"  # Change to \"Flickr30k\" if needed\n",
    "\n",
    "    # Paths\n",
    "    dataset_dir = f\"../../../../flickr_data/{dataset}_Dataset/Images\"\n",
    "    captions_file = f\"../../../../flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "    image_dir = dataset_dir\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    cider_scores = []\n",
    "    \n",
    "    # Load captions\n",
    "    caption_df = pd.read_csv(captions_file).dropna().drop_duplicates()\n",
    "    print(f\"Total captions loaded: {len(caption_df)}\")\n",
    "\n",
    "    # Build vocabulary\n",
    "    word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=10000)\n",
    "    print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "    # Convert captions to sequences\n",
    "    captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "    print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "    # Get data transformations\n",
    "    train_transform = get_transform(train=True)\n",
    "    val_transform = get_transform(train=False)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    image_names = list(image_captions.keys())\n",
    "    train_images, val_images, _ = get_splits(image_names, test_size=0.2)\n",
    "    print(f\"Training samples: {len(train_images)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = FlickrDataset(\n",
    "        image_dir, train_images, captions_seqs, transform=train_transform\n",
    "    )\n",
    "    val_dataset = FlickrDataset(\n",
    "        image_dir, val_images, captions_seqs, transform=val_transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32, \n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize models\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    vocab_size = len(word2idx)\n",
    "    top_k = 5  # Number of objects to consider\n",
    "\n",
    "    # Initialize encoder and decoder\n",
    "    encoder = EncoderCNN(embed_size=embed_size, device=device, top_k=top_k).to(device)\n",
    "    decoder = DecoderRNN(\n",
    "        input_size=embed_size,\n",
    "        embed_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        vocab_size=vocab_size,\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
    "    params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = optim.Adam(params, lr=5e-4, weight_decay=3e-4)\n",
    "\n",
    "    # Initialize the learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',           # We want to minimize the validation loss\n",
    "        factor=0.5,           # Factor by which the learning rate will be reduced\n",
    "        patience=2,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    )\n",
    "    \n",
    "    # Prepare image to captions mapping for evaluation\n",
    "    val_image2captions = prepare_image2captions(val_images, captions_seqs, idx2word)\n",
    "\n",
    "    # Training settings\n",
    "    num_epochs = 10\n",
    "    total_step = len(train_loader)\n",
    "    end_token_idx = word2idx[\"<end>\"]\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (images, captions, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            global_features, object_features = encoder(images)\n",
    "            outputs = decoder(global_features, object_features, captions)  # No slicing\n",
    "\n",
    "            targets = captions[:, 1:]     # Shape: (batch_size, seq_len -1)\n",
    "\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)  # Shape: (batch_size * (seq_len -1), vocab_size)\n",
    "            targets = targets.reshape(-1)              # Shape: (batch_size * (seq_len -1))\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 300 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = total_loss / total_step\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate(encoder, decoder, val_loader, criterion, device, vocab_size)\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        bleu = calculate_bleu_score(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_dir,\n",
    "            val_images,\n",
    "            val_image2captions,\n",
    "            val_transform,\n",
    "            idx2word,\n",
    "            device,\n",
    "            word2idx,\n",
    "        )\n",
    "        meteor = calculate_meteor_score(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_dir,\n",
    "            val_images,\n",
    "            val_image2captions,\n",
    "            val_transform,\n",
    "            idx2word,\n",
    "            device,\n",
    "            word2idx,\n",
    "        )\n",
    "        cider = calculate_cider_score(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_dir,\n",
    "            val_images,\n",
    "            val_image2captions,\n",
    "            val_transform,\n",
    "            idx2word,\n",
    "            device,\n",
    "            word2idx,\n",
    "        )\n",
    "\n",
    "        # Print epoch summary\n",
    "        epoch_duration = time.time() - start_time\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Training Loss: {avg_train_loss:.4f}, \"\n",
    "            f\"Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"BLEU: {bleu:.4f}, \"\n",
    "            f\"METEOR: {meteor:.4f}, \"\n",
    "            f\"CIDEr: {cider:.4f}, \"\n",
    "            f\"Time: {epoch_duration:.2f}s\"\n",
    "        )\n",
    "\n",
    "        # Save metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu)\n",
    "        meteor_scores.append(meteor)\n",
    "        cider_scores.append(cider)\n",
    "\n",
    "    # Save the models\n",
    "    os.makedirs(\"models/model_2_image_segmentation_lstm\", exist_ok=True)\n",
    "    torch.save(encoder.state_dict(), \"models/model_2_image_segmentation_lstm/encoder.pth\")\n",
    "    torch.save(decoder.state_dict(), \"models/model_2_image_segmentation_lstm/decoder.pth\")\n",
    "    print(\"Models saved successfully.\")\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_2_image_segmentation_lstm/loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), bleu_scores, label='BLEU Score')\n",
    "    plt.plot(range(1, num_epochs + 1), meteor_scores, label='METEOR Score')\n",
    "    plt.plot(range(1, num_epochs + 1), cider_scores, label='CIDEr Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Evaluation Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_2_image_segmentation_lstm/metrics_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions loaded: 40445\n",
      "Vocabulary size: 8921\n",
      "Maximum caption length: 40\n",
      "Training samples: 6472\n",
      "Validation samples: 1457\n",
      "Number of training batches: 1011\n",
      "Number of validation batches: 228\n",
      "Using device: cuda\n",
      "Epoch [1/10], Step [0/1011], Loss: 9.1193\n",
      "Epoch [1/10], Step [300/1011], Loss: 4.3396\n",
      "Epoch [1/10], Step [600/1011], Loss: 4.2018\n",
      "Epoch [1/10], Step [900/1011], Loss: 3.4711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 419333.73 tokens per second.\n",
      "PTBTokenizer tokenized 23211 tokens at 160156.42 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 4.2250, Validation Loss: 3.7549, BLEU: 0.0720, METEOR: 0.3001, CIDEr: 0.1235, Time: 805.49s\n",
      "Epoch [2/10], Step [0/1011], Loss: 3.6582\n",
      "Epoch [2/10], Step [300/1011], Loss: 3.4671\n",
      "Epoch [2/10], Step [600/1011], Loss: 3.5692\n",
      "Epoch [2/10], Step [900/1011], Loss: 3.5124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 621947.09 tokens per second.\n",
      "PTBTokenizer tokenized 18965 tokens at 131861.51 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 3.6269, Validation Loss: 3.5632, BLEU: 0.0887, METEOR: 0.3111, CIDEr: 0.1398, Time: 784.51s\n",
      "Epoch [3/10], Step [0/1011], Loss: 3.6404\n",
      "Epoch [3/10], Step [300/1011], Loss: 3.5752\n",
      "Epoch [3/10], Step [600/1011], Loss: 3.3426\n",
      "Epoch [3/10], Step [900/1011], Loss: 3.6249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 440186.54 tokens per second.\n",
      "PTBTokenizer tokenized 21672 tokens at 162940.51 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 3.4779, Validation Loss: 3.4734, BLEU: 0.0785, METEOR: 0.2923, CIDEr: 0.1396, Time: 775.84s\n",
      "Epoch [4/10], Step [0/1011], Loss: 3.6791\n",
      "Epoch [4/10], Step [300/1011], Loss: 3.5684\n",
      "Epoch [4/10], Step [600/1011], Loss: 3.3883\n",
      "Epoch [4/10], Step [900/1011], Loss: 3.2462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 642159.65 tokens per second.\n",
      "PTBTokenizer tokenized 22150 tokens at 271484.96 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 3.3933, Validation Loss: 3.4015, BLEU: 0.0757, METEOR: 0.2819, CIDEr: 0.1410, Time: 774.47s\n",
      "Epoch [5/10], Step [0/1011], Loss: 3.4310\n",
      "Epoch [5/10], Step [300/1011], Loss: 3.3680\n",
      "Epoch [5/10], Step [600/1011], Loss: 3.4036\n",
      "Epoch [5/10], Step [900/1011], Loss: 3.3902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 679801.46 tokens per second.\n",
      "PTBTokenizer tokenized 18672 tokens at 213155.49 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 3.3354, Validation Loss: 3.3772, BLEU: 0.1034, METEOR: 0.3177, CIDEr: 0.1842, Time: 777.70s\n",
      "Epoch [6/10], Step [0/1011], Loss: 3.1641\n",
      "Epoch [6/10], Step [300/1011], Loss: 3.2723\n",
      "Epoch [6/10], Step [600/1011], Loss: 3.3838\n",
      "Epoch [6/10], Step [900/1011], Loss: 3.2807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 620864.84 tokens per second.\n",
      "PTBTokenizer tokenized 18530 tokens at 187795.85 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 3.2953, Validation Loss: 3.3460, BLEU: 0.0969, METEOR: 0.3128, CIDEr: 0.1763, Time: 754.44s\n",
      "Epoch [7/10], Step [0/1011], Loss: 3.2362\n",
      "Epoch [7/10], Step [300/1011], Loss: 3.0829\n",
      "Epoch [7/10], Step [600/1011], Loss: 3.3943\n",
      "Epoch [7/10], Step [900/1011], Loss: 3.0852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 684991.58 tokens per second.\n",
      "PTBTokenizer tokenized 22178 tokens at 272545.39 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 3.2650, Validation Loss: 3.3120, BLEU: 0.0888, METEOR: 0.3104, CIDEr: 0.1801, Time: 740.83s\n",
      "Epoch [8/10], Step [0/1011], Loss: 3.3377\n",
      "Epoch [8/10], Step [300/1011], Loss: 3.1427\n",
      "Epoch [8/10], Step [600/1011], Loss: 3.1793\n",
      "Epoch [8/10], Step [900/1011], Loss: 3.4772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 425800.80 tokens per second.\n",
      "PTBTokenizer tokenized 20918 tokens at 237473.70 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 3.2394, Validation Loss: 3.2999, BLEU: 0.0943, METEOR: 0.3164, CIDEr: 0.1833, Time: 731.18s\n",
      "Epoch [9/10], Step [0/1011], Loss: 3.3060\n",
      "Epoch [9/10], Step [300/1011], Loss: 3.3035\n",
      "Epoch [9/10], Step [600/1011], Loss: 3.3795\n",
      "Epoch [9/10], Step [900/1011], Loss: 2.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 646839.04 tokens per second.\n",
      "PTBTokenizer tokenized 21534 tokens at 263678.44 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 3.2212, Validation Loss: 3.2860, BLEU: 0.0846, METEOR: 0.3072, CIDEr: 0.1725, Time: 728.30s\n",
      "Epoch [10/10], Step [0/1011], Loss: 3.0577\n",
      "Epoch [10/10], Step [300/1011], Loss: 2.9297\n",
      "Epoch [10/10], Step [600/1011], Loss: 3.1617\n",
      "Epoch [10/10], Step [900/1011], Loss: 3.3832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 684904.09 tokens per second.\n",
      "PTBTokenizer tokenized 22304 tokens at 263332.20 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 3.2034, Validation Loss: 3.2720, BLEU: 0.0896, METEOR: 0.3106, CIDEr: 0.1598, Time: 727.12s\n",
      "Models saved successfully.\n",
      "CPU times: user 11h 26min 3s, sys: 1min 39s, total: 11h 27min 43s\n",
      "Wall time: 2h 6min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/2ll4226j2cdf65tbnf1cwhp00000gn/T/ipykernel_54723/434205140.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(encoder_path, map_location=device)\n",
      "/var/folders/ms/2ll4226j2cdf65tbnf1cwhp00000gn/T/ipykernel_54723/434205140.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(decoder_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 2714703706_d21c5cb8df.jpg\n",
      "Generated Caption: a dog is running through the water .\n",
      "Ground Truth Captions:\n",
      "- ['dogs', 'playing']\n",
      "- ['a', 'brown', 'dog', 'is', 'biting', 'a', 'white', 'and', 'tan', 'dog', 'on', 'the', 'snout', '.']\n",
      "- ['the', 'brown', 'dog', 'has', 'a', 'hold', 'of', 'the', 'other', 'dogs', 'cheek', 'with', 'its', 'teeth', '.']\n",
      "- ['two', 'dogs', 'are', 'nuzzling', 'each', 'other', 'nose', 'to', 'nose', '.']\n",
      "- ['two', 'dogs', 'bite', 'at', 'each', 'other', 'on', 'the', 'carpet', '.']\n",
      "------------------------------------\n",
      "Image ID: 3532194771_07faf20d76.jpg\n",
      "Generated Caption: a man is walking through the ocean .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'man', 'is', 'heading', 'out', 'to', 'see', 'with', 'his', 'surfboard', 'in', 'hand', '.']\n",
      "- ['a', 'man', 'with', 'a', 'white', 'surfboard', 'is', 'walking', 'into', 'the', 'water', '.']\n",
      "- ['a', 'person', 'walks', 'into', 'the', 'water', 'carrying', 'a', 'white', 'surfboard', '.']\n",
      "- ['a', 'surfer', 'walking', 'into', 'the', 'ocean']\n",
      "- ['surfer', 'with', 'board', 'marches', 'out', 'to', 'sea', 'on', 'gray', 'day', '.']\n",
      "------------------------------------\n",
      "Image ID: 2356574282_5078f08b58.jpg\n",
      "Generated Caption: a man in a black shirt and a white shirt is sitting on a bench .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'arabian', 'dressed', 'lady', 'leans', 'backwards', 'holding', 'a', 'skinny', 'crooked', 'sword', '.']\n",
      "- ['a', 'costumed', 'woman', 'with', 'a', 'sword', 'does', 'a', 'backbend', '.']\n",
      "- ['a', 'woman', 'bending', 'over', 'backwards', '.']\n",
      "- ['a', 'woman', 'in', 'a', 'belly', 'dancing', 'outfit', 'bending', 'over', 'backwards', '.']\n",
      "- ['a', 'woman', 'in', 'a', 'dance', 'costume', 'is', 'bending', 'over', 'backward', 'and', 'holding', 'a', 'sword', '.']\n",
      "------------------------------------\n",
      "Image ID: 3526150930_580908dab6.jpg\n",
      "Generated Caption: a man in a red shirt and a woman in a red shirt and a woman in a red shirt\n",
      "Ground Truth Captions:\n",
      "- ['a', 'woman', 'and', 'a', 'young', 'girl', 'pose', 'and', 'smile', 'for', 'a', 'photo', '.']\n",
      "- ['a', 'woman', 'and', 'a', 'young', 'girl', 'smiling', 'for', 'the', 'camera', ',', 'in', 'front', 'of', 'some', 'flowers', '.']\n",
      "- ['a', 'woman', 'and', 'girl', 'pose', 'together', 'in', 'a', 'garden', '.']\n",
      "- ['a', 'woman', 'poses', 'with', 'a', 'small', 'girl', 'on', 'her', 'lap', 'in', 'front', 'of', 'a', 'flower', 'bush', '.']\n",
      "- ['a', 'woman', 'with', 'brown', 'hair', 'is', 'sitting', 'with', 'a', 'little', 'girl', 'with', 'short', 'brown', 'hair', 'outside', 'next', 'to', 'some', 'red', 'flowers', '.']\n",
      "------------------------------------\n",
      "Image ID: 2448270671_5e0e391a80.jpg\n",
      "Generated Caption: a dog is running through the grass .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'brown', 'dog', 'is', 'leaping', 'over', 'a', 'fallen', 'tree', 'in', 'the', 'woods', '.']\n",
      "- ['a', 'brown', 'dog', 'with', 'tongue', 'sticking', 'out', 'jumping', 'over', 'a', 'log', '.']\n",
      "- ['a', 'dog', 'is', 'jumping', 'over', 'a', 'log', 'with', 'ears', 'flying', 'and', 'tongue', 'out', '.']\n",
      "- ['a', 'dog', 'leaps', 'over', 'a', 'log', 'in', 'the', 'woods', '.']\n",
      "- ['the', 'dog', 'with', 'big', 'ears', 'is', 'leaping', 'over', 'a', 'fallen', 'tree', '.']\n",
      "------------------------------------\n",
      "Image ID: 3482237861_605b4f0fd9.jpg\n",
      "Generated Caption: a man in a green shirt and a helmet is riding a bike on a beach .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'boy', 'is', 'riding', 'a', 'scooter', 'down', 'the', 'concrete', 'path', '.']\n",
      "- ['a', 'child', 'on', 'a', 'scooter', 'moving', 'down', 'the', 'sidewalk', '.']\n",
      "- ['a', 'youth', 'rides', 'a', 'scooter', 'on', 'a', 'sidewalk', 'near', 'a', 'building', '.']\n",
      "- ['the', 'boy', 'is', 'riding', 'his', 'scooter', 'on', 'the', 'sidewalk', '.']\n",
      "- ['young', 'boy', 'rides', 'his', 'scooter', 'on', 'drive', '.']\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Flickr8k\"\n",
    "\n",
    "captions_file_path = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file_path).dropna().drop_duplicates()\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=8921)\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "\n",
    "# Get data transformations\n",
    "test_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "image_names = list(captions_seqs.keys())\n",
    "_, _, test_images = get_splits(image_names, test_size=0.2)\n",
    "\n",
    "# Prepare image to captions mapping for ground truth captions\n",
    "test_image2captions = prepare_image2captions(test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Create test dataset and data loader\n",
    "test_dataset = FlickrDataset(\n",
    "    image_dir, test_images, captions_seqs, transform=test_transform, mode='test'\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Process one image at a time\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize models\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(word2idx)\n",
    "input_size = embed_size  # As per EncoderCNN's output\n",
    "\n",
    "# Initialize EncoderCNN with device\n",
    "encoder = EncoderCNN(embed_size=embed_size, device=device, top_k=5).to(device)\n",
    "decoder = DecoderRNN(\n",
    "    input_size=input_size,\n",
    "    embed_size=embed_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "# Load trained models\n",
    "encoder_path = os.path.join(project_root, \"models/model_2_image_segmentation_lstm/encoder.pth\")\n",
    "decoder_path = os.path.join(project_root, \"models/model_2_image_segmentation_lstm/decoder.pth\")\n",
    "\n",
    "encoder.load_state_dict(\n",
    "    torch.load(encoder_path, map_location=device)\n",
    ")\n",
    "decoder.load_state_dict(\n",
    "    torch.load(decoder_path, map_location=device)\n",
    ")\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Retrieve <end> and <start> token indices\n",
    "end_token_idx = word2idx.get('<end>', None)\n",
    "start_token_idx = word2idx.get('<start>', None)\n",
    "\n",
    "if end_token_idx is None:\n",
    "    raise ValueError(\"The '<end>' token was not found in the vocabulary.\")\n",
    "if start_token_idx is None:\n",
    "    raise ValueError(\"The '<start>' token was not found in the vocabulary.\")\n",
    "\n",
    "# Generate captions on test images\n",
    "for i, (images, captions, image_ids) in enumerate(test_loader):\n",
    "    if i >= 6:\n",
    "        break  # Stop after processing 10 images\n",
    "\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through encoder\n",
    "        global_features, object_features = encoder(images)\n",
    "\n",
    "        # Forward pass through decoder's sample method with correct arguments\n",
    "        sampled_ids = decoder.sample(\n",
    "            global_features,\n",
    "            object_features,\n",
    "            start_token_idx=start_token_idx,\n",
    "            end_token_idx=end_token_idx\n",
    "        )\n",
    "\n",
    "    # Convert word IDs to words\n",
    "    sampled_caption = [idx2word.get(word_id, '<unk>') for word_id in sampled_ids]\n",
    "\n",
    "    # Remove words after (and including) the '<end>' token\n",
    "    if '<end>' in sampled_caption:\n",
    "        end_index = sampled_caption.index('<end>')\n",
    "        sampled_caption = sampled_caption[:end_index]\n",
    "\n",
    "    generated_caption = ' '.join(sampled_caption)\n",
    "\n",
    "    # Get ground truth captions\n",
    "    image_name = image_ids[0]\n",
    "    gt_captions = test_image2captions.get(image_name, [])\n",
    "\n",
    "    if not gt_captions:\n",
    "        print(f'Image ID: {image_name}')\n",
    "        print('Generated Caption:', generated_caption)\n",
    "        print('Ground Truth Captions: None')\n",
    "        print('------------------------------------')\n",
    "        continue\n",
    "\n",
    "    print(f'Image ID: {image_name}')\n",
    "    print(f'Generated Caption: {generated_caption}')\n",
    "    print('Ground Truth Captions:')\n",
    "    for gt_caption in gt_captions:\n",
    "        print(f'- {gt_caption}')\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
