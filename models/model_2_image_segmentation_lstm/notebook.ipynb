{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/jed/anaconda3/omscs/CS7643/image-captioning-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path(os.getcwd()).resolve()  # Get the current working directory\n",
    "project_root = notebook_dir.parents[1]  # Adjust the number to go up to the project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.model_2_image_segmentation_lstm.model import *\n",
    "from data.dataset import *\n",
    "from data.preprocessing import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define dataset type\n",
    "    dataset = \"Flickr8k\"  # Change to \"Flickr30k\" if needed\n",
    "\n",
    "    # Paths\n",
    "    dataset_dir = f\"../../../../flickr_data/{dataset}_Dataset/Images\"\n",
    "    captions_file = f\"../../../../flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "    image_dir = dataset_dir\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    cider_scores = []\n",
    "    \n",
    "    # Load captions\n",
    "    caption_df = pd.read_csv(captions_file).dropna().drop_duplicates()\n",
    "    print(f\"Total captions loaded: {len(caption_df)}\")\n",
    "\n",
    "    # Build vocabulary\n",
    "    word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=10000)\n",
    "    print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "    # Convert captions to sequences\n",
    "    captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "    print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "    # Get data transformations\n",
    "    train_transform = get_transform(train=True)\n",
    "    val_transform = get_transform(train=False)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    image_names = list(image_captions.keys())\n",
    "    train_images, val_images, _ = get_splits(image_names, test_size=0.2)\n",
    "    print(f\"Training samples: {len(train_images)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = FlickrDataset(\n",
    "        image_dir, train_images, captions_seqs, transform=train_transform\n",
    "    )\n",
    "    val_dataset = FlickrDataset(\n",
    "        image_dir, val_images, captions_seqs, transform=val_transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32, \n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize models\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    vocab_size = len(word2idx)\n",
    "    input_size = embed_size  # Must match EncoderCNN's embed_size\n",
    "    top_k = 5  # Number of objects to consider\n",
    "\n",
    "    # Initialize encoder and decoder\n",
    "    encoder = EncoderCNN(embed_size=embed_size, device=device, top_k=top_k).to(device)\n",
    "    decoder = DecoderRNN(\n",
    "        input_size=input_size,\n",
    "        embed_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        vocab_size=vocab_size,\n",
    "        dropout=0.4\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
    "    params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = optim.Adam(params, lr=1e-3, weight_decay=5e-5)\n",
    "\n",
    "    # Initialize the learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',           # We want to minimize the validation loss\n",
    "        factor=0.5,           # Factor by which the learning rate will be reduced\n",
    "        patience=2,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "        verbose=True          # Print a message when the learning rate is updated\n",
    "    )\n",
    "    \n",
    "    # Prepare image to captions mapping for evaluation\n",
    "    val_image2captions = prepare_image2captions(val_images, captions_seqs, idx2word)\n",
    "\n",
    "    # Training settings\n",
    "    num_epochs = 10\n",
    "    total_step = len(train_loader)\n",
    "    end_token_idx = word2idx[\"<end>\"]\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (images, captions, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            global_features, object_features = encoder(images)\n",
    "            outputs = decoder(global_features, object_features, captions)\n",
    "\n",
    "            # Exclude the first time step from outputs and targets\n",
    "            outputs = outputs[:, 1:, :]  # Shape: (batch_size, seq_len -1, vocab_size)\n",
    "            targets = captions[:, 1:]     # Shape: (batch_size, seq_len -1)\n",
    "\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)  # Shape: (batch_size * (seq_len -1), vocab_size)\n",
    "            targets = targets.reshape(-1)              # Shape: (batch_size * (seq_len -1))\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 300 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = total_loss / total_step\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate(encoder, decoder, val_loader, criterion, device, vocab_size)\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        bleu = calculate_bleu_score(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_dir,\n",
    "            val_images,\n",
    "            val_image2captions,\n",
    "            val_transform,\n",
    "            idx2word,\n",
    "            device,\n",
    "            word2idx,\n",
    "        )\n",
    "        meteor = calculate_meteor_score(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_dir,\n",
    "            val_images,\n",
    "            val_image2captions,\n",
    "            val_transform,\n",
    "            idx2word,\n",
    "            device,\n",
    "            word2idx,\n",
    "        )\n",
    "        cider = calculate_cider_score(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_dir,\n",
    "            val_images,\n",
    "            val_image2captions,\n",
    "            val_transform,\n",
    "            idx2word,\n",
    "            device,\n",
    "            word2idx,\n",
    "        )\n",
    "\n",
    "        # Print epoch summary\n",
    "        epoch_duration = time.time() - start_time\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Training Loss: {avg_train_loss:.4f}, \"\n",
    "            f\"Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"BLEU: {bleu:.4f}, \"\n",
    "            f\"METEOR: {meteor:.4f}, \"\n",
    "            f\"CIDEr: {cider:.4f}, \"\n",
    "            f\"Time: {epoch_duration:.2f}s\"\n",
    "        )\n",
    "\n",
    "        # Save metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu)\n",
    "        meteor_scores.append(meteor)\n",
    "        cider_scores.append(cider)\n",
    "\n",
    "    # Save the models\n",
    "    os.makedirs(\"models/model_2_image_segmentation_lstm\", exist_ok=True)\n",
    "    torch.save(encoder.state_dict(), \"models/model_2_image_segmentation_lstm/encoder.pth\")\n",
    "    torch.save(decoder.state_dict(), \"models/model_2_image_segmentation_lstm/decoder.pth\")\n",
    "    print(\"Models saved successfully.\")\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_2_image_segmentation_lstm/loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), bleu_scores, label='BLEU Score')\n",
    "    plt.plot(range(1, num_epochs + 1), meteor_scores, label='METEOR Score')\n",
    "    plt.plot(range(1, num_epochs + 1), cider_scores, label='CIDEr Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Evaluation Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_2_image_segmentation_lstm/metrics_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions loaded: 40445\n",
      "Vocabulary size: 8921\n",
      "Maximum caption length: 40\n",
      "Training samples: 6472\n",
      "Validation samples: 1457\n",
      "Number of training batches: 1011\n",
      "Number of validation batches: 228\n",
      "Using device: cuda\n",
      "Epoch [1/10], Step [0/1011], Loss: 9.0604\n",
      "Epoch [1/10], Step [300/1011], Loss: 3.8427\n",
      "Epoch [1/10], Step [600/1011], Loss: 3.7357\n",
      "Epoch [1/10], Step [900/1011], Loss: 3.4613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 670934.36 tokens per second.\n",
      "PTBTokenizer tokenized 21329 tokens at 267976.51 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 3.9267, Validation Loss: 3.4723, BLEU: 0.0611, METEOR: 0.2740, CIDEr: 0.1020, Time: 838.20s\n",
      "Epoch [2/10], Step [0/1011], Loss: 3.3912\n",
      "Epoch [2/10], Step [300/1011], Loss: 3.4182\n",
      "Epoch [2/10], Step [600/1011], Loss: 3.3431\n",
      "Epoch [2/10], Step [900/1011], Loss: 3.1535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 678450.08 tokens per second.\n",
      "PTBTokenizer tokenized 20201 tokens at 254925.37 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 3.3222, Validation Loss: 3.2644, BLEU: 0.0768, METEOR: 0.2802, CIDEr: 0.1337, Time: 835.28s\n",
      "Epoch [3/10], Step [0/1011], Loss: 2.9873\n",
      "Epoch [3/10], Step [300/1011], Loss: 2.9588\n",
      "Epoch [3/10], Step [600/1011], Loss: 3.3234\n",
      "Epoch [3/10], Step [900/1011], Loss: 3.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 443964.61 tokens per second.\n",
      "PTBTokenizer tokenized 20335 tokens at 257877.16 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 3.1378, Validation Loss: 3.1608, BLEU: 0.0792, METEOR: 0.2913, CIDEr: 0.1290, Time: 832.71s\n",
      "Epoch [4/10], Step [0/1011], Loss: 2.7448\n",
      "Epoch [4/10], Step [300/1011], Loss: 2.9936\n",
      "Epoch [4/10], Step [600/1011], Loss: 3.0017\n",
      "Epoch [4/10], Step [900/1011], Loss: 2.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 662622.00 tokens per second.\n",
      "PTBTokenizer tokenized 20677 tokens at 259637.15 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 3.0213, Validation Loss: 3.1039, BLEU: 0.0716, METEOR: 0.2829, CIDEr: 0.1196, Time: 833.56s\n",
      "Epoch [5/10], Step [0/1011], Loss: 2.8381\n",
      "Epoch [5/10], Step [300/1011], Loss: 3.1110\n",
      "Epoch [5/10], Step [600/1011], Loss: 3.1529\n",
      "Epoch [5/10], Step [900/1011], Loss: 3.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 625816.37 tokens per second.\n",
      "PTBTokenizer tokenized 22167 tokens at 271565.39 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 2.9450, Validation Loss: 3.0626, BLEU: 0.0753, METEOR: 0.2934, CIDEr: 0.1216, Time: 851.76s\n",
      "Epoch [6/10], Step [0/1011], Loss: 2.9760\n",
      "Epoch [6/10], Step [300/1011], Loss: 2.8621\n",
      "Epoch [6/10], Step [600/1011], Loss: 3.1394\n",
      "Epoch [6/10], Step [900/1011], Loss: 2.9161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 640975.94 tokens per second.\n",
      "PTBTokenizer tokenized 19443 tokens at 184455.88 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 2.8842, Validation Loss: 3.0446, BLEU: 0.0738, METEOR: 0.2839, CIDEr: 0.1296, Time: 845.96s\n",
      "Epoch [7/10], Step [0/1011], Loss: 2.8646\n",
      "Epoch [7/10], Step [300/1011], Loss: 2.8090\n",
      "Epoch [7/10], Step [600/1011], Loss: 2.8453\n",
      "Epoch [7/10], Step [900/1011], Loss: 2.8014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 642207.29 tokens per second.\n",
      "PTBTokenizer tokenized 20378 tokens at 217364.22 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 2.8369, Validation Loss: 3.0225, BLEU: 0.0877, METEOR: 0.2895, CIDEr: 0.1495, Time: 843.42s\n",
      "Epoch [8/10], Step [0/1011], Loss: 2.8178\n",
      "Epoch [8/10], Step [300/1011], Loss: 2.6607\n",
      "Epoch [8/10], Step [600/1011], Loss: 2.9273\n",
      "Epoch [8/10], Step [900/1011], Loss: 2.7484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 681898.92 tokens per second.\n",
      "PTBTokenizer tokenized 22781 tokens at 280283.75 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 2.7949, Validation Loss: 3.0084, BLEU: 0.0721, METEOR: 0.2771, CIDEr: 0.1039, Time: 843.74s\n",
      "Epoch [9/10], Step [0/1011], Loss: 2.7224\n",
      "Epoch [9/10], Step [300/1011], Loss: 2.5997\n",
      "Epoch [9/10], Step [600/1011], Loss: 2.7574\n",
      "Epoch [9/10], Step [900/1011], Loss: 2.5210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 589373.57 tokens per second.\n",
      "PTBTokenizer tokenized 21954 tokens at 265437.45 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 2.7575, Validation Loss: 2.9986, BLEU: 0.0772, METEOR: 0.2845, CIDEr: 0.1184, Time: 843.67s\n",
      "Epoch [10/10], Step [0/1011], Loss: 2.6978\n",
      "Epoch [10/10], Step [300/1011], Loss: 2.6263\n",
      "Epoch [10/10], Step [600/1011], Loss: 2.6463\n",
      "Epoch [10/10], Step [900/1011], Loss: 2.7768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92805 tokens at 671204.43 tokens per second.\n",
      "PTBTokenizer tokenized 23748 tokens at 286621.65 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 2.7264, Validation Loss: 2.9893, BLEU: 0.0783, METEOR: 0.2909, CIDEr: 0.1302, Time: 842.33s\n",
      "Models saved successfully.\n",
      "CPU times: user 21h 44min 42s, sys: 1min 52s, total: 21h 46min 35s\n",
      "Wall time: 2h 20min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/2ll4226j2cdf65tbnf1cwhp00000gn/T/ipykernel_12919/2686678490.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(encoder_path, map_location=device)\n",
      "/var/folders/ms/2ll4226j2cdf65tbnf1cwhp00000gn/T/ipykernel_12919/2686678490.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(decoder_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 2714703706_d21c5cb8df.jpg\n",
      "Generated Caption: a dog is running through the grass .\n",
      "Ground Truth Captions:\n",
      "- ['dogs', 'playing']\n",
      "- ['a', 'brown', 'dog', 'is', 'biting', 'a', 'white', 'and', 'tan', 'dog', 'on', 'the', '<unk>', '.']\n",
      "- ['the', 'brown', 'dog', 'has', 'a', 'hold', 'of', 'the', 'other', 'dogs', 'cheek', 'with', 'its', 'teeth', '.']\n",
      "- ['two', 'dogs', 'are', 'nuzzling', 'each', 'other', 'nose', 'to', 'nose', '.']\n",
      "- ['two', 'dogs', 'bite', 'at', 'each', 'other', 'on', 'the', 'carpet', '.']\n",
      "------------------------------------\n",
      "Image ID: 3532194771_07faf20d76.jpg\n",
      "Generated Caption: a man is jumping on a beach .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'man', 'is', 'heading', 'out', 'to', 'see', 'with', 'his', 'surfboard', 'in', 'hand', '.']\n",
      "- ['a', 'man', 'with', 'a', 'white', 'surfboard', 'is', 'walking', 'into', 'the', 'water', '.']\n",
      "- ['a', 'person', 'walks', 'into', 'the', 'water', 'carrying', 'a', 'white', 'surfboard', '.']\n",
      "- ['a', 'surfer', 'walking', 'into', 'the', 'ocean']\n",
      "- ['surfer', 'with', 'board', 'marches', 'out', 'to', 'sea', 'on', 'gray', 'day', '.']\n",
      "------------------------------------\n",
      "Image ID: 2356574282_5078f08b58.jpg\n",
      "Generated Caption: a man in a red shirt and a hat is standing on a table .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'arabian', 'dressed', 'lady', 'leans', 'backwards', 'holding', 'a', 'skinny', 'crooked', 'sword', '.']\n",
      "- ['a', 'costumed', 'woman', 'with', 'a', 'sword', 'does', 'a', '<unk>', '.']\n",
      "- ['a', 'woman', 'bending', 'over', 'backwards', '.']\n",
      "- ['a', 'woman', 'in', 'a', 'belly', 'dancing', 'outfit', 'bending', 'over', 'backwards', '.']\n",
      "- ['a', 'woman', 'in', 'a', 'dance', 'costume', 'is', 'bending', 'over', 'backward', 'and', 'holding', 'a', 'sword', '.']\n",
      "------------------------------------\n",
      "Image ID: 3526150930_580908dab6.jpg\n",
      "Generated Caption: a man in a red shirt and a red shirt is sitting on a swing .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'woman', 'and', 'a', 'young', 'girl', 'pose', 'and', 'smile', 'for', 'a', 'photo', '.']\n",
      "- ['a', 'woman', 'and', 'a', 'young', 'girl', 'smiling', 'for', 'the', 'camera', ',', 'in', 'front', 'of', 'some', 'flowers', '.']\n",
      "- ['a', 'woman', 'and', 'girl', 'pose', 'together', 'in', 'a', 'garden', '.']\n",
      "- ['a', 'woman', 'poses', 'with', 'a', 'small', 'girl', 'on', 'her', 'lap', 'in', 'front', 'of', 'a', 'flower', 'bush', '.']\n",
      "- ['a', 'woman', 'with', 'brown', 'hair', 'is', 'sitting', 'with', 'a', 'little', 'girl', 'with', 'short', 'brown', 'hair', 'outside', 'next', 'to', 'some', 'red', 'flowers', '.']\n",
      "------------------------------------\n",
      "Image ID: 2448270671_5e0e391a80.jpg\n",
      "Generated Caption: a dog is running through the grass .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'brown', 'dog', 'is', 'leaping', 'over', 'a', 'fallen', 'tree', 'in', 'the', 'woods', '.']\n",
      "- ['a', 'brown', 'dog', 'with', 'tongue', 'sticking', 'out', 'jumping', 'over', 'a', 'log', '.']\n",
      "- ['a', 'dog', 'is', 'jumping', 'over', 'a', 'log', 'with', 'ears', 'flying', 'and', 'tongue', 'out', '.']\n",
      "- ['a', 'dog', 'leaps', 'over', 'a', 'log', 'in', 'the', 'woods', '.']\n",
      "- ['the', 'dog', 'with', 'big', 'ears', 'is', 'leaping', 'over', 'a', 'fallen', 'tree', '.']\n",
      "------------------------------------\n",
      "Image ID: 3482237861_605b4f0fd9.jpg\n",
      "Generated Caption: a man is jumping on a skateboard .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'boy', 'is', 'riding', 'a', 'scooter', 'down', 'the', 'concrete', 'path', '.']\n",
      "- ['a', 'child', 'on', 'a', 'scooter', 'moving', 'down', 'the', 'sidewalk', '.']\n",
      "- ['a', 'youth', 'rides', 'a', 'scooter', 'on', 'a', 'sidewalk', 'near', 'a', 'building', '.']\n",
      "- ['the', 'boy', 'is', 'riding', 'his', 'scooter', 'on', 'the', 'sidewalk', '.']\n",
      "- ['young', 'boy', 'rides', 'his', 'scooter', 'on', 'drive', '.']\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Flickr8k\"\n",
    "\n",
    "captions_file_path = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file_path).dropna().drop_duplicates()\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=5000)\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "\n",
    "# Get data transformations\n",
    "test_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "image_names = list(image_captions.keys())\n",
    "_, _, test_images = get_splits(image_names, test_size=0.2)\n",
    "\n",
    "# Prepare image to captions mapping for ground truth captions\n",
    "test_image2captions = prepare_image2captions(test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Create test dataset and data loader\n",
    "test_dataset = FlickrDataset(\n",
    "    image_dir, test_images, captions_seqs, transform=test_transform, mode='test'\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Process one image at a time\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize models\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(word2idx)\n",
    "input_size = embed_size  # As per EncoderCNN's output\n",
    "\n",
    "# Initialize EncoderCNN with device\n",
    "encoder = EncoderCNN(embed_size=embed_size, device=device, top_k=5).to(device)\n",
    "decoder = DecoderRNN(\n",
    "    input_size=input_size,\n",
    "    embed_size=embed_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size\n",
    ").to(device)\n",
    "\n",
    "# Load trained models\n",
    "encoder_path = os.path.join(project_root, \"models/model_2_image_segmentation_lstm/encoder.pth\")\n",
    "decoder_path = os.path.join(project_root, \"models/model_2_image_segmentation_lstm/decoder.pth\")\n",
    "\n",
    "encoder.load_state_dict(\n",
    "    torch.load(encoder_path, map_location=device)\n",
    ")\n",
    "decoder.load_state_dict(\n",
    "    torch.load(decoder_path, map_location=device)\n",
    ")\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Retrieve <end> and <start> token indices\n",
    "end_token_idx = word2idx.get('<end>', None)\n",
    "start_token_idx = word2idx.get('<start>', None)\n",
    "\n",
    "if end_token_idx is None:\n",
    "    raise ValueError(\"The '<end>' token was not found in the vocabulary.\")\n",
    "if start_token_idx is None:\n",
    "    raise ValueError(\"The '<start>' token was not found in the vocabulary.\")\n",
    "\n",
    "# Generate captions on test images\n",
    "for i, (images, captions, image_ids) in enumerate(test_loader):\n",
    "    if i >= 6:\n",
    "        break  # Stop after processing 10 images\n",
    "\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through encoder\n",
    "        global_features, object_features = encoder(images)\n",
    "\n",
    "        # Forward pass through decoder's sample method with correct arguments\n",
    "        sampled_ids = decoder.sample(\n",
    "            global_features,\n",
    "            object_features,\n",
    "            start_token_idx=start_token_idx,\n",
    "            end_token_idx=end_token_idx\n",
    "        )\n",
    "\n",
    "    # Convert word IDs to words\n",
    "    sampled_caption = [idx2word.get(word_id, '<unk>') for word_id in sampled_ids]\n",
    "\n",
    "    # Remove words after (and including) the '<end>' token\n",
    "    if '<end>' in sampled_caption:\n",
    "        end_index = sampled_caption.index('<end>')\n",
    "        sampled_caption = sampled_caption[:end_index]\n",
    "\n",
    "    generated_caption = ' '.join(sampled_caption)\n",
    "\n",
    "    # Get ground truth captions\n",
    "    image_name = image_ids[0]\n",
    "    gt_captions = test_image2captions.get(image_name, [])\n",
    "\n",
    "    if not gt_captions:\n",
    "        print(f'Image ID: {image_name}')\n",
    "        print('Generated Caption:', generated_caption)\n",
    "        print('Ground Truth Captions: None')\n",
    "        print('------------------------------------')\n",
    "        continue\n",
    "\n",
    "    print(f'Image ID: {image_name}')\n",
    "    print(f'Generated Caption: {generated_caption}')\n",
    "    print('Ground Truth Captions:')\n",
    "    for gt_caption in gt_captions:\n",
    "        print(f'- {gt_caption}')\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
