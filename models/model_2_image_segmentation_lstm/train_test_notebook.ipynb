{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/jed/anaconda3/omscs/CS7643/image-captioning-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path(os.getcwd()).resolve()  # Get the current working directory\n",
    "project_root = notebook_dir.parents[1]  # Adjust the number to go up to the project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.model_2_image_segmentation_lstm.model import *\n",
    "from data.dataset import *\n",
    "from data.preprocessing import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, data_loader, criterion, device, vocab_size):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "\n",
    "    Args:\n",
    "        encoder (EncoderCNN): The encoder model.\n",
    "        decoder (DecoderRNN): The decoder model.\n",
    "        data_loader (DataLoader): DataLoader for the validation set.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to run the evaluation on.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        float: Average validation loss.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            memory = encoder(images)  # memory: (batch_size, memory_seq_len, embed_size)\n",
    "            captions_input = captions[:, :-1]\n",
    "            targets = captions[:, 1:]\n",
    "\n",
    "            outputs = decoder(captions_input, memory)  # outputs: (batch_size, seq_len -1, vocab_size)\n",
    "\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)  # Shape: (batch_size * (seq_len -1), vocab_size)\n",
    "            targets = targets.reshape(-1)              # Shape: (batch_size * (seq_len -1))\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "    average_loss = total_loss / total_batches\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define dataset type\n",
    "    dataset = \"Flickr8k\"  # Change to \"Flickr30k\" if needed\n",
    "\n",
    "    random.seed(7643)\n",
    "    \n",
    "    # Paths\n",
    "    dataset_dir = f\"../../../../flickr_data/{dataset}_Dataset/Images\"\n",
    "    captions_file = f\"../../../../flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "    image_dir = dataset_dir\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    cider_scores = []\n",
    "    \n",
    "    # Load captions\n",
    "    caption_df = pd.read_csv(captions_file).dropna().drop_duplicates()\n",
    "\n",
    "    # Build vocabulary\n",
    "    word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=8000)\n",
    "    print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "    # Convert captions to sequences\n",
    "    captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "    print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "    # Get data transformations\n",
    "    train_transform = get_transform(train=True)\n",
    "    val_transform = get_transform(train=False)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    image_names = list(image_captions.keys())\n",
    "    random.shuffle(image_names)\n",
    "    val_size = int(0.2 * len(image_names))  # 20% for validation\n",
    "    train_images = image_names[val_size:]\n",
    "    val_images = image_names[:val_size]\n",
    "    print(f\"Training samples: {len(train_images)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    # Note the mode='train' for val_dataset to ensure it behaves like the training dataset\n",
    "    train_dataset = FlickrDataset(\n",
    "        image_dir, train_images, captions_seqs, transform=train_transform, mode='train'\n",
    "    )\n",
    "    val_dataset = FlickrDataset(\n",
    "        image_dir, val_images, captions_seqs, transform=val_transform, mode='train'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize models\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    vocab_size = len(word2idx)\n",
    "    encoder = Encoder(embed_size=embed_size).to(device)\n",
    "    decoder = Decoder(\n",
    "        embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
    "    params = list(filter(lambda p: p.requires_grad, encoder.parameters())) + list(\n",
    "        decoder.parameters()\n",
    "    )\n",
    "    optimizer = optim.Adam(params, lr=3e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    num_epochs = 10\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    val_image_ids = val_images\n",
    "    image2captions = prepare_image2captions(val_image_ids, captions_seqs, idx2word)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Prepare targets\n",
    "            targets = captions[:, 1:]  # Exclude the first <start> token\n",
    "\n",
    "            # Exclude the first time step from outputs\n",
    "            outputs = outputs[:, 1:, :]\n",
    "\n",
    "            # Reshape for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 300 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_loss / total_step\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate(encoder, decoder, val_loader, criterion, device, vocab_size)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        bleu = calculate_bleu_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        meteor = calculate_meteor_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        cider = calculate_cider_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds.\"\n",
    "        )\n",
    "        print(\n",
    "            f\"BLEU Score: {bleu:.4f}, METEOR Score: {meteor:.4f}, CIDEr Score: {cider:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu)\n",
    "        meteor_scores.append(meteor)\n",
    "        cider_scores.append(cider)\n",
    "\n",
    "    # Save the models\n",
    "    os.makedirs(\"models/model_2_image_segmentation_lstm\", exist_ok=True)\n",
    "    torch.save(encoder.state_dict(), \"models/model_2_image_segmentation_lstm/encoder.pth\")\n",
    "    torch.save(decoder.state_dict(), \"models/model_2_image_segmentation_lstm/decoder.pth\")\n",
    "    print(\"Models saved successfully.\")\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_2_image_segmentation_lstm/loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), bleu_scores, label='BLEU Score')\n",
    "    plt.plot(range(1, num_epochs + 1), meteor_scores, label='METEOR Score')\n",
    "    plt.plot(range(1, num_epochs + 1), cider_scores, label='CIDEr Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Evaluation Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_2_image_segmentation_lstm/metrics_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8000\n",
      "Maximum caption length: 40\n",
      "Training samples: 6473\n",
      "Validation samples: 1618\n",
      "Using device: cuda\n",
      "Epoch [1/10], Step [0/1012], Loss: 8.9864\n",
      "Epoch [1/10], Step [300/1012], Loss: 4.3169\n",
      "Epoch [1/10], Step [600/1012], Loss: 3.6412\n",
      "Epoch [1/10], Step [900/1012], Loss: 3.5921\n",
      "Epoch [1/10], Training Loss: 4.2353, Validation Loss: 3.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 731726.90 tokens per second.\n",
      "PTBTokenizer tokenized 19633 tokens at 251619.97 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] completed in 338.07 seconds.\n",
      "BLEU Score: 0.0968, METEOR Score: 0.3177, CIDEr Score: 0.1680\n",
      "\n",
      "Epoch [2/10], Step [0/1012], Loss: 3.6844\n",
      "Epoch [2/10], Step [300/1012], Loss: 3.3987\n",
      "Epoch [2/10], Step [600/1012], Loss: 3.4978\n",
      "Epoch [2/10], Step [900/1012], Loss: 3.4225\n",
      "Epoch [2/10], Training Loss: 3.4205, Validation Loss: 3.3537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 731919.47 tokens per second.\n",
      "PTBTokenizer tokenized 24556 tokens at 293989.62 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] completed in 315.61 seconds.\n",
      "BLEU Score: 0.0822, METEOR Score: 0.3166, CIDEr Score: 0.1535\n",
      "\n",
      "Epoch [3/10], Step [0/1012], Loss: 3.0825\n",
      "Epoch [3/10], Step [300/1012], Loss: 3.2001\n",
      "Epoch [3/10], Step [600/1012], Loss: 3.1571\n",
      "Epoch [3/10], Step [900/1012], Loss: 2.9089\n",
      "Epoch [3/10], Training Loss: 3.1857, Validation Loss: 3.2249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 727355.68 tokens per second.\n",
      "PTBTokenizer tokenized 23698 tokens at 288130.64 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] completed in 340.15 seconds.\n",
      "BLEU Score: 0.0968, METEOR Score: 0.3309, CIDEr Score: 0.1840\n",
      "\n",
      "Epoch [4/10], Step [0/1012], Loss: 3.0111\n",
      "Epoch [4/10], Step [300/1012], Loss: 2.9187\n",
      "Epoch [4/10], Step [600/1012], Loss: 2.9521\n",
      "Epoch [4/10], Step [900/1012], Loss: 3.5380\n",
      "Epoch [4/10], Training Loss: 3.0294, Validation Loss: 3.1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 662865.49 tokens per second.\n",
      "PTBTokenizer tokenized 25863 tokens at 308172.51 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] completed in 400.92 seconds.\n",
      "BLEU Score: 0.0940, METEOR Score: 0.3304, CIDEr Score: 0.1802\n",
      "\n",
      "Epoch [5/10], Step [0/1012], Loss: 2.8132\n",
      "Epoch [5/10], Step [300/1012], Loss: 2.9351\n",
      "Epoch [5/10], Step [600/1012], Loss: 3.0704\n",
      "Epoch [5/10], Step [900/1012], Loss: 2.7597\n",
      "Epoch [5/10], Training Loss: 2.9112, Validation Loss: 3.0780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 722502.89 tokens per second.\n",
      "PTBTokenizer tokenized 23971 tokens at 290312.47 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] completed in 385.19 seconds.\n",
      "BLEU Score: 0.1017, METEOR Score: 0.3293, CIDEr Score: 0.1874\n",
      "\n",
      "Epoch [6/10], Step [0/1012], Loss: 2.8013\n",
      "Epoch [6/10], Step [300/1012], Loss: 2.7281\n",
      "Epoch [6/10], Step [600/1012], Loss: 3.0820\n",
      "Epoch [6/10], Step [900/1012], Loss: 2.7048\n",
      "Epoch [6/10], Training Loss: 2.7905, Validation Loss: 3.0552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 729631.37 tokens per second.\n",
      "PTBTokenizer tokenized 22786 tokens at 279105.05 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] completed in 362.72 seconds.\n",
      "BLEU Score: 0.1010, METEOR Score: 0.3300, CIDEr Score: 0.2012\n",
      "\n",
      "Epoch [7/10], Step [0/1012], Loss: 2.6420\n",
      "Epoch [7/10], Step [300/1012], Loss: 2.6843\n",
      "Epoch [7/10], Step [600/1012], Loss: 2.6753\n",
      "Epoch [7/10], Step [900/1012], Loss: 2.8778\n",
      "Epoch [7/10], Training Loss: 2.7721, Validation Loss: 3.0496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 723954.52 tokens per second.\n",
      "PTBTokenizer tokenized 22698 tokens at 279797.00 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] completed in 327.76 seconds.\n",
      "BLEU Score: 0.0995, METEOR Score: 0.3305, CIDEr Score: 0.2025\n",
      "\n",
      "Epoch [8/10], Step [0/1012], Loss: 2.7047\n",
      "Epoch [8/10], Step [300/1012], Loss: 2.9350\n",
      "Epoch [8/10], Step [600/1012], Loss: 2.8687\n",
      "Epoch [8/10], Step [900/1012], Loss: 2.9722\n",
      "Epoch [8/10], Training Loss: 2.7609, Validation Loss: 3.0455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 729351.80 tokens per second.\n",
      "PTBTokenizer tokenized 22654 tokens at 278315.47 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] completed in 313.63 seconds.\n",
      "BLEU Score: 0.1028, METEOR Score: 0.3305, CIDEr Score: 0.2049\n",
      "\n",
      "Epoch [9/10], Step [0/1012], Loss: 2.4623\n",
      "Epoch [9/10], Step [300/1012], Loss: 2.6595\n",
      "Epoch [9/10], Step [600/1012], Loss: 2.6937\n",
      "Epoch [9/10], Step [900/1012], Loss: 2.7075\n",
      "Epoch [9/10], Training Loss: 2.7480, Validation Loss: 3.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 720846.40 tokens per second.\n",
      "PTBTokenizer tokenized 22747 tokens at 276437.58 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] completed in 360.59 seconds.\n",
      "BLEU Score: 0.1010, METEOR Score: 0.3313, CIDEr Score: 0.2005\n",
      "\n",
      "Epoch [10/10], Step [0/1012], Loss: 2.7413\n",
      "Epoch [10/10], Step [300/1012], Loss: 2.8509\n",
      "Epoch [10/10], Step [600/1012], Loss: 2.8172\n",
      "Epoch [10/10], Step [900/1012], Loss: 2.9725\n",
      "Epoch [10/10], Training Loss: 2.7384, Validation Loss: 3.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 725554.42 tokens per second.\n",
      "PTBTokenizer tokenized 22407 tokens at 276139.12 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] completed in 403.17 seconds.\n",
      "BLEU Score: 0.1068, METEOR Score: 0.3327, CIDEr Score: 0.2094\n",
      "\n",
      "Models saved successfully.\n",
      "CPU times: user 13h 40min 30s, sys: 1min 42s, total: 13h 42min 12s\n",
      "Wall time: 59min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/2ll4226j2cdf65tbnf1cwhp00000gn/T/ipykernel_95280/1119653669.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(encoder_path, map_location=device)\n",
      "/var/folders/ms/2ll4226j2cdf65tbnf1cwhp00000gn/T/ipykernel_95280/1119653669.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(decoder_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 2661294481_b86058b504.jpg\n",
      "Generated Caption: a man in a blue shirt is walking down a street .\n",
      "Ground Truth Captions:\n",
      "- a crowd begins to march in a small downtown setting .\n",
      "- a group of people holding signs and marching through the streets .\n",
      "- a group of people with signs and banners walking down the street\n",
      "- a protest march is crossing the intersection of two streets near to a building with red awnings .\n",
      "- many people hold signs and march down the street .\n",
      "------------------------------------\n",
      "Image ID: 3334537556_a2cf4e9b9a.jpg\n",
      "Generated Caption: a group of people are standing on a rocky beach .\n",
      "Ground Truth Captions:\n",
      "- a group of people on skis with two dogs .\n",
      "- a man dressed in a horned hat poses for a picture on skis with three other people and a dog .\n",
      "- four people holding each others shoulders with a brown dog in front of them , standing on snow .\n",
      "- people dressed in costumes are on skis .\n",
      "- people , some dressed in costumes , and dogs on a snowy mountain .\n",
      "------------------------------------\n",
      "Image ID: 415118186_64defc96f3.jpg\n",
      "Generated Caption: a man in a red jacket is standing on a rock overlooking the ocean .\n",
      "Ground Truth Captions:\n",
      "- a man in black looks out over a snowy landscape .\n",
      "- a man looks out over frozen <unk> .\n",
      "- a man wearing black clothes , standing and looking out over a snowy scene .\n",
      "- a person in dark clothing takes a <unk> of a snowy scene .\n",
      "- a person taking pictures outside in the snow .\n",
      "------------------------------------\n",
      "Image ID: 464506846_1734302b58.jpg\n",
      "Generated Caption: a dog runs through the grass .\n",
      "Ground Truth Captions:\n",
      "- a black dog is running across a grassy field in front of some bushes .\n",
      "- a dog in a grassy park jumping and playing .\n",
      "- a dog with a muzzle on .\n",
      "- a muzzled greyhound leaps over grassy ground .\n",
      "- the black dog leaps into the air in the grass .\n",
      "------------------------------------\n",
      "Image ID: 349889354_4b2889a9bd.jpg\n",
      "Generated Caption: a man in a red jacket is standing on a rock overlooking the ocean .\n",
      "Ground Truth Captions:\n",
      "- a person on what appears to be a surfboard , rides the waves onto shore .\n",
      "- a surfer in the distance is at the base of the wave close to the shore .\n",
      "- a surfer is caught in an incoming wave along a dirty beach nearby mountains .\n",
      "- a surfer rides a wave as it crashes on the beach .\n",
      "- a surfer takes on a high wave beyond a rocky beach .\n",
      "------------------------------------\n",
      "Image ID: 2218843713_cf28ea319e.jpg\n",
      "Generated Caption: a man in a red jacket is surfing .\n",
      "Ground Truth Captions:\n",
      "- a boy in a red shirt sits on a rock on the edge of a river .\n",
      "- a child on a rock in a stream .\n",
      "- a girl sitting on a rock in the middle of a body of water\n",
      "- child wearing a red shirt sits on a rock in the water .\n",
      "- one boy sitting on a rock in the middle of a body of water .\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "dataset = \"Flickr8k\"\n",
    "\n",
    "captions_file_path = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file_path).dropna().drop_duplicates()\n",
    "\n",
    "# Build vocabulary with vocab_size=5000\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=8000)\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "\n",
    "# Get data transformations\n",
    "test_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "image_names = list(image_captions.keys())\n",
    "random.shuffle(image_names)\n",
    "val_size = int(0.2 * len(image_names))  # 20% for validation\n",
    "test_images = image_names[:val_size]\n",
    "\n",
    "# Randomly select 6 images from the test_images\n",
    "sampled_test_images = random.sample(test_images, 6)\n",
    "\n",
    "# Prepare image to captions mapping for ground truth captions\n",
    "test_image2captions = prepare_image2captions(sampled_test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Create test dataset and data loader for only those 6 randomly selected images\n",
    "test_dataset = FlickrDataset(\n",
    "    image_dir, sampled_test_images, captions_seqs, transform=test_transform, mode='test'\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Process one image at a time\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize models\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "encoder = Encoder(embed_size=embed_size).to(device)\n",
    "decoder = Decoder(\n",
    "    embed_size=embed_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size\n",
    ").to(device)\n",
    "\n",
    "# Load trained models\n",
    "encoder_path = os.path.join(project_root, \"models/model_2_image_segmentation_lstm/encoder.pth\")\n",
    "decoder_path = os.path.join(project_root, \"models/model_2_image_segmentation_lstm/decoder.pth\")\n",
    "\n",
    "encoder.load_state_dict(\n",
    "    torch.load(encoder_path, map_location=device)\n",
    ")\n",
    "decoder.load_state_dict(\n",
    "    torch.load(decoder_path, map_location=device)\n",
    ")\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "end_token_idx = word2idx.get('<end>', None)\n",
    "\n",
    "# Generate captions on the randomly selected test images\n",
    "for i, (images, captions, image_ids) in enumerate(test_loader):\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(images)\n",
    "        sampled_ids = decoder.sample(features, end_token_idx=end_token_idx)\n",
    "    \n",
    "    # Convert word IDs to words\n",
    "    sampled_caption = [idx2word.get(word_id, '<unk>') for word_id in sampled_ids]\n",
    "    \n",
    "    # Remove words after (and including) the '<end>' token\n",
    "    if '<end>' in sampled_caption:\n",
    "        end_index = sampled_caption.index('<end>')\n",
    "        sampled_caption = sampled_caption[:end_index]\n",
    "    \n",
    "    generated_caption = ' '.join(sampled_caption)\n",
    "\n",
    "    # Get ground truth captions\n",
    "    image_name = image_ids[0]\n",
    "    gt_captions = test_image2captions.get(image_name, [])\n",
    "\n",
    "    if not gt_captions:\n",
    "        print(f'Image ID: {image_name}')\n",
    "        print('Generated Caption:', generated_caption)\n",
    "        print('Ground Truth Captions: None')\n",
    "        print('------------------------------------')\n",
    "        continue\n",
    "\n",
    "    print(f'Image ID: {image_name}')\n",
    "    print(f'Generated Caption: {generated_caption}')\n",
    "    print('Ground Truth Captions:')\n",
    "    for gt_caption in gt_captions:\n",
    "        print(f'- {\" \".join(gt_caption)}')\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103060 tokens at 880286.97 tokens per second.\n",
      "PTBTokenizer tokenized 22341 tokens at 317247.62 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.1144375291394959\n",
      "METEOR Score: 0.3384300144639662\n",
      "CIDEr Score: 0.2268791450509689\n"
     ]
    }
   ],
   "source": [
    "image2captions = prepare_image2captions(test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Calculate BLEU, METEOR, and CIDEr scores on the sampled test images\n",
    "bleu_score = calculate_bleu_score(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_dir=image_dir,\n",
    "    image_ids=test_images,\n",
    "    image2captions=image2captions,\n",
    "    transform=test_transform,\n",
    "    idx2word=idx2word,\n",
    "    device=device,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "meteor = calculate_meteor_score(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_dir=image_dir,\n",
    "    image_ids=test_images,\n",
    "    image2captions=image2captions,\n",
    "    transform=test_transform,\n",
    "    idx2word=idx2word,\n",
    "    device=device,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "cider = calculate_cider_score(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_dir=image_dir,\n",
    "    image_ids=test_images,\n",
    "    image2captions=image2captions,\n",
    "    transform=test_transform,\n",
    "    idx2word=idx2word,\n",
    "    device=device,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "print(\"METEOR Score:\", meteor)\n",
    "print(\"CIDEr Score:\", cider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
