{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/jed/anaconda3/omscs/CS7643/image-captioning-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path(os.getcwd()).resolve()  # Get the current working directory\n",
    "project_root = notebook_dir.parents[1]  # Adjust the number to go up to the project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.model_3_image_segmentation_attention_decoder.model import *\n",
    "from data.dataset import *\n",
    "from data.preprocessing import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function: Computes validation loss on a given dataset\n",
    "def evaluate(encoder, decoder, data_loader, criterion, device, vocab_size):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    Args:\n",
    "        encoder: Encoder model.\n",
    "        decoder: Decoder model.\n",
    "        data_loader: DataLoader for the validation set.\n",
    "        criterion: Loss function.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        vocab_size: Size of the vocabulary.\n",
    "    Returns:\n",
    "        average_loss: Average validation loss.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, captions, lengths in data_loader:\n",
    "            # Move data to the computation device\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass through encoder and decoder\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Prepare targets\n",
    "            targets = captions[:, 1:]\n",
    "\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "    # Calculate average loss\n",
    "    average_loss = total_loss / total_samples\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define dataset type\n",
    "    dataset = \"Flickr8k\"  # Change to \"Flickr30k\" if needed\n",
    "\n",
    "    random.seed(7643)\n",
    "    \n",
    "    # Paths\n",
    "    dataset_dir = f\"../../flickr_data/{dataset}_Dataset/Images\"\n",
    "    captions_file = f\"../../flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "    image_dir = dataset_dir\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    cider_scores = []\n",
    "    \n",
    "    # Load captions\n",
    "    caption_df = pd.read_csv(captions_file).dropna().drop_duplicates()\n",
    "\n",
    "    # Build vocabulary\n",
    "    word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=8000)\n",
    "    print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "    # Convert captions to sequences\n",
    "    captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "    print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "    # Get data transformations\n",
    "    train_transform = get_transform(train=True)\n",
    "    val_transform = get_transform(train=False)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    image_names = list(image_captions.keys())\n",
    "    random.shuffle(image_names)\n",
    "    val_size = int(0.2 * len(image_names))  # 20% for validation\n",
    "    train_images = image_names[val_size:]\n",
    "    val_images = image_names[:val_size]\n",
    "    print(f\"Training samples: {len(train_images)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    # Note the mode='train' for val_dataset to ensure it behaves like the training dataset\n",
    "    train_dataset = FlickrDataset(\n",
    "        image_dir, train_images, captions_seqs, transform=train_transform, mode='train'\n",
    "    )\n",
    "    val_dataset = FlickrDataset(\n",
    "        image_dir, val_images, captions_seqs, transform=val_transform, mode='train'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize models\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    vocab_size = len(word2idx)\n",
    "    encoder = Encoder(embed_size=embed_size).to(device)\n",
    "    decoder = Decoder(\n",
    "        embed_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        vocab_size=vocab_size,\n",
    "        num_heads=8,\n",
    "        num_layers=2,\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
    "    params = list(filter(lambda p: p.requires_grad, encoder.parameters())) + list(\n",
    "        decoder.parameters()\n",
    "    )\n",
    "    optimizer = optim.Adam(params, lr=3e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    num_epochs = 10\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    val_image_ids = val_images\n",
    "    image2captions = prepare_image2captions(val_image_ids, captions_seqs, idx2word)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Prepare targets\n",
    "            targets = captions[:, 1:]  # Exclude the first <start> token\n",
    "\n",
    "            # Reshape for loss computation without excluding any time steps\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 300 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_loss / total_step\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate(encoder, decoder, val_loader, criterion, device, vocab_size)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        bleu = calculate_bleu_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        meteor = calculate_meteor_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        cider = calculate_cider_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds.\"\n",
    "        )\n",
    "        print(\n",
    "            f\"BLEU Score: {bleu:.4f}, METEOR Score: {meteor:.4f}, CIDEr Score: {cider:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu)\n",
    "        meteor_scores.append(meteor)\n",
    "        cider_scores.append(cider)\n",
    "\n",
    "    # Save the models\n",
    "    os.makedirs(\"models/transformer_model\", exist_ok=True)\n",
    "    torch.save(encoder.state_dict(), \"models/transformer_model/encoder.pth\")\n",
    "    torch.save(decoder.state_dict(), \"models/transformer_model/decoder.pth\")\n",
    "    print(\"Models saved successfully.\")\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/transformer_model/loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), bleu_scores, label='BLEU Score')\n",
    "    plt.plot(range(1, num_epochs + 1), meteor_scores, label='METEOR Score')\n",
    "    plt.plot(range(1, num_epochs + 1), cider_scores, label='CIDEr Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Evaluation Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/transformer_model/metrics_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8000\n",
      "Maximum caption length: 40\n",
      "Training samples: 6473\n",
      "Validation samples: 1618\n",
      "Using device: cuda\n",
      "Epoch [1/10], Step [0/1012], Loss: 9.7795\n",
      "Epoch [1/10], Step [300/1012], Loss: 4.6037\n",
      "Epoch [1/10], Step [600/1012], Loss: 3.4147\n",
      "Epoch [1/10], Step [900/1012], Loss: 3.1340\n",
      "Epoch [1/10], Training Loss: 4.0127, Validation Loss: 3.4985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 726853.61 tokens per second.\n",
      "PTBTokenizer tokenized 20308 tokens at 248518.18 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] completed in 414.41 seconds.\n",
      "BLEU Score: 0.1041, METEOR Score: 0.3254, CIDEr Score: 0.1986\n",
      "\n",
      "Epoch [2/10], Step [0/1012], Loss: 3.3632\n",
      "Epoch [2/10], Step [300/1012], Loss: 3.6082\n",
      "Epoch [2/10], Step [600/1012], Loss: 3.2547\n",
      "Epoch [2/10], Step [900/1012], Loss: 3.3890\n",
      "Epoch [2/10], Training Loss: 3.3526, Validation Loss: 3.2753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 726412.30 tokens per second.\n",
      "PTBTokenizer tokenized 22224 tokens at 276874.12 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] completed in 427.90 seconds.\n",
      "BLEU Score: 0.1012, METEOR Score: 0.3319, CIDEr Score: 0.1889\n",
      "\n",
      "Epoch [3/10], Step [0/1012], Loss: 3.2842\n",
      "Epoch [3/10], Step [300/1012], Loss: 3.0268\n",
      "Epoch [3/10], Step [600/1012], Loss: 3.0981\n",
      "Epoch [3/10], Step [900/1012], Loss: 2.7776\n",
      "Epoch [3/10], Training Loss: 3.1417, Validation Loss: 3.1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 614643.46 tokens per second.\n",
      "PTBTokenizer tokenized 27127 tokens at 246910.70 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] completed in 471.73 seconds.\n",
      "BLEU Score: 0.0800, METEOR Score: 0.3222, CIDEr Score: 0.1995\n",
      "\n",
      "Epoch [4/10], Step [0/1012], Loss: 2.9163\n",
      "Epoch [4/10], Step [300/1012], Loss: 3.1673\n",
      "Epoch [4/10], Step [600/1012], Loss: 2.9491\n",
      "Epoch [4/10], Step [900/1012], Loss: 3.3108\n",
      "Epoch [4/10], Training Loss: 3.0120, Validation Loss: 3.1264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 728899.53 tokens per second.\n",
      "PTBTokenizer tokenized 20539 tokens at 259546.38 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] completed in 430.74 seconds.\n",
      "BLEU Score: 0.1018, METEOR Score: 0.3294, CIDEr Score: 0.2038\n",
      "\n",
      "Epoch [5/10], Step [0/1012], Loss: 2.9262\n",
      "Epoch [5/10], Step [300/1012], Loss: 2.6092\n",
      "Epoch [5/10], Step [600/1012], Loss: 2.6975\n",
      "Epoch [5/10], Step [900/1012], Loss: 3.3131\n",
      "Epoch [5/10], Training Loss: 2.9205, Validation Loss: 3.1248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 728198.39 tokens per second.\n",
      "PTBTokenizer tokenized 20942 tokens at 263670.03 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] completed in 425.80 seconds.\n",
      "BLEU Score: 0.1200, METEOR Score: 0.3405, CIDEr Score: 0.2443\n",
      "\n",
      "Epoch [6/10], Step [0/1012], Loss: 2.9137\n",
      "Epoch [6/10], Step [300/1012], Loss: 2.4160\n",
      "Epoch [6/10], Step [600/1012], Loss: 2.7559\n",
      "Epoch [6/10], Step [900/1012], Loss: 2.7258\n",
      "Epoch [6/10], Training Loss: 2.7706, Validation Loss: 3.0596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 727765.79 tokens per second.\n",
      "PTBTokenizer tokenized 22660 tokens at 279572.15 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] completed in 424.21 seconds.\n",
      "BLEU Score: 0.1155, METEOR Score: 0.3484, CIDEr Score: 0.2418\n",
      "\n",
      "Epoch [7/10], Step [0/1012], Loss: 2.8240\n",
      "Epoch [7/10], Step [300/1012], Loss: 2.7229\n",
      "Epoch [7/10], Step [600/1012], Loss: 3.0268\n",
      "Epoch [7/10], Step [900/1012], Loss: 2.9078\n",
      "Epoch [7/10], Training Loss: 2.7342, Validation Loss: 3.0475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 719047.73 tokens per second.\n",
      "PTBTokenizer tokenized 22754 tokens at 280086.27 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] completed in 440.42 seconds.\n",
      "BLEU Score: 0.1115, METEOR Score: 0.3462, CIDEr Score: 0.2409\n",
      "\n",
      "Epoch [8/10], Step [0/1012], Loss: 2.5466\n",
      "Epoch [8/10], Step [300/1012], Loss: 2.7575\n",
      "Epoch [8/10], Step [600/1012], Loss: 2.4985\n",
      "Epoch [8/10], Step [900/1012], Loss: 2.6328\n",
      "Epoch [8/10], Training Loss: 2.7163, Validation Loss: 3.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 729663.45 tokens per second.\n",
      "PTBTokenizer tokenized 22036 tokens at 273005.65 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] completed in 441.83 seconds.\n",
      "BLEU Score: 0.1186, METEOR Score: 0.3485, CIDEr Score: 0.2488\n",
      "\n",
      "Epoch [9/10], Step [0/1012], Loss: 2.5299\n",
      "Epoch [9/10], Step [300/1012], Loss: 2.7497\n",
      "Epoch [9/10], Step [600/1012], Loss: 2.8348\n",
      "Epoch [9/10], Step [900/1012], Loss: 2.8622\n",
      "Epoch [9/10], Training Loss: 2.6997, Validation Loss: 3.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 730151.73 tokens per second.\n",
      "PTBTokenizer tokenized 22192 tokens at 272883.73 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] completed in 401.47 seconds.\n",
      "BLEU Score: 0.1190, METEOR Score: 0.3483, CIDEr Score: 0.2452\n",
      "\n",
      "Epoch [10/10], Step [0/1012], Loss: 2.6733\n",
      "Epoch [10/10], Step [300/1012], Loss: 2.8143\n",
      "Epoch [10/10], Step [600/1012], Loss: 2.5130\n",
      "Epoch [10/10], Step [900/1012], Loss: 2.7729\n",
      "Epoch [10/10], Training Loss: 2.6848, Validation Loss: 3.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103083 tokens at 732243.43 tokens per second.\n",
      "PTBTokenizer tokenized 21574 tokens at 267513.28 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] completed in 401.14 seconds.\n",
      "BLEU Score: 0.1236, METEOR Score: 0.3535, CIDEr Score: 0.2596\n",
      "\n",
      "Models saved successfully.\n",
      "CPU times: user 1d 7h 3min 10s, sys: 1min 23s, total: 1d 7h 4min 33s\n",
      "Wall time: 1h 11min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Image ID: 3228960484_9aab98b91a.jpg\n",
      "Generated Caption: a man in a black shirt is standing on a sidewalk .\n",
      "Ground Truth Captions:\n",
      "- children walking on a sidewalk with yellow backpacks .\n",
      "- two children are walking along a street wearing yellow backpacks .\n",
      "- two children are walking on a sidewalk wearing yellow and red backpacks .\n",
      "- two kids walk up the sidewalk with their backpacks .\n",
      "- two young children with yellow backpacks walking down a sidewalk\n",
      "-------------------------------------\n",
      "Image ID: 2656749876_e32495bd8c.jpg\n",
      "Generated Caption: a man in a black shirt and a woman walking down a sidewalk .\n",
      "Ground Truth Captions:\n",
      "- a man talks on his cellphone while he walks down the street .\n",
      "- a member of the clergy carries a blue bag in his hand and talks on the phone as he walks .\n",
      "- a priest carrying a small blue bag walking down the street talking on a cellphone .\n",
      "- priest in black walking on sidewalk carrying a blue bag and talking on cellphone .\n",
      "- priest walking with blue bag while talking on cellphone .\n",
      "-------------------------------------\n",
      "Image ID: 2422302286_385725e3cf.jpg\n",
      "Generated Caption: a man in a red shirt is sitting on a bench .\n",
      "Ground Truth Captions:\n",
      "- a child sits in a plastic toy car .\n",
      "- a little kid in a toy car .\n",
      "- small girl riding in play car .\n",
      "- there is a child in a red and yellow toy car .\n",
      "- the small child is sitting in a red and yellow toy car .\n",
      "-------------------------------------\n",
      "Image ID: 1659396176_ced00a549f.jpg\n",
      "Generated Caption: a dog is running through the grass .\n",
      "Ground Truth Captions:\n",
      "- a white and brown dog with his mouth open and tongue out facing a second , almost off-camera dog .\n",
      "- two dogs , one with long brown fur , both dogs have their mouths open and teeth showing , one dogs tongue is hanging out .\n",
      "- two dogs play in the grass .\n",
      "- two furry dogs make snarly faces as they play together .\n",
      "- two multicoloured dogs fighting .\n",
      "-------------------------------------\n",
      "Image ID: 2533010184_ef2fd71297.jpg\n",
      "Generated Caption: a man in a white shirt is sitting on a bench .\n",
      "Ground Truth Captions:\n",
      "- a woman in a headscarf stands by a garden of white flowers .\n",
      "- a woman in a head wrap is standing behind a tree and some beautiful white flowers .\n",
      "- a woman in a white headscarf sits near white flowers .\n",
      "- a woman wearing a headscarf is near many tulips .\n",
      "- a woman wearing a scarf over her hair in the distance behind some flowers .\n",
      "-------------------------------------\n",
      "Image ID: 3724623861_2bb6c23641.jpg\n",
      "Generated Caption: a man in a blue shirt is holding a drink .\n",
      "Ground Truth Captions:\n",
      "- a girl in a colorful dress and sunglasses is playing a violin on the street .\n",
      "- a red haired woman playing the violin\n",
      "- a woman in a dress plays violin outside .\n",
      "- a woman plays a violin outdoors .\n",
      "- a woman wearing a plaid dress plays an instrument while standing near a tree .\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Flickr8k\"\n",
    "\n",
    "captions_file_path = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file_path).dropna().drop_duplicates()\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=8000)\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "\n",
    "# Get data transformations\n",
    "test_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "image_names = list(image_captions.keys())\n",
    "random.shuffle(image_names)\n",
    "val_size = int(0.2 * len(image_names))  # 20% for validation\n",
    "test_images = image_names[:val_size]\n",
    "\n",
    "# Randomly select 6 images from the test_images\n",
    "sampled_test_images = random.sample(test_images, 6)\n",
    "\n",
    "# Prepare image to captions mapping for ground truth captions\n",
    "test_image2captions = prepare_image2captions(sampled_test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Create test dataset and data loader for only those 6 randomly selected images\n",
    "test_dataset = FlickrDataset(\n",
    "    image_dir, sampled_test_images, captions_seqs, transform=test_transform, mode='test'\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Process one image at a time\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define hyperparameters (ensure they match training)\n",
    "embed_size = 256\n",
    "num_layers = 2\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# Initialize EncoderCNN with device\n",
    "encoder = Encoder(embed_size=embed_size).to(device)\n",
    "decoder = Decoder(\n",
    "    embed_size=embed_size,\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# Load trained models\n",
    "encoder_path = os.path.join(project_root, \"models/model_3_image_segmentation_attention_decoder/encoder.pth\")\n",
    "decoder_path = os.path.join(project_root, \"models/model_3_image_segmentation_attention_decoder/decoder.pth\")\n",
    "\n",
    "if not os.path.exists(encoder_path):\n",
    "    raise FileNotFoundError(f\"Encoder model not found at {encoder_path}\")\n",
    "if not os.path.exists(decoder_path):\n",
    "    raise FileNotFoundError(f\"Decoder model not found at {decoder_path}\")\n",
    "\n",
    "encoder.load_state_dict(\n",
    "    torch.load(encoder_path, map_location=device, weights_only=True)\n",
    ")\n",
    "decoder.load_state_dict(\n",
    "    torch.load(decoder_path, map_location=device, weights_only=True)\n",
    ")\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Retrieve <end> and <start> token indices\n",
    "end_token_idx = word2idx.get('<end>', None)\n",
    "start_token_idx = word2idx.get('<start>', None)\n",
    "\n",
    "if end_token_idx is None:\n",
    "    raise ValueError(\"The '<end>' token was not found in the vocabulary.\")\n",
    "if start_token_idx is None:\n",
    "    raise ValueError(\"The '<start>' token was not found in the vocabulary.\")\n",
    "\n",
    "# Generate captions on test images\n",
    "for i, (images, captions, image_ids) in enumerate(test_loader):\n",
    "    if i >= 6:  # Process only the first 6 images\n",
    "        break\n",
    "\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through encoder\n",
    "        memory = encoder(images)  # memory: (batch_size, memory_seq_len, embed_size)\n",
    "\n",
    "        # Forward pass through decoder's sample method with correct arguments\n",
    "        # Assuming TransformerDecoder's sample method signature:\n",
    "        # def sample(self, memory, start_token_idx, end_token_idx, max_len=20)\n",
    "        sampled_ids = decoder.sample(\n",
    "            memory,\n",
    "            end_token_idx=end_token_idx,\n",
    "            max_len=50\n",
    "        )\n",
    "\n",
    "    # Convert word IDs to words\n",
    "    sampled_caption = [idx2word.get(word_id, '<unk>') for word_id in sampled_ids]\n",
    "\n",
    "    # Remove words after (and including) the '<end>' token\n",
    "    if '<end>' in sampled_caption:\n",
    "        end_index = sampled_caption.index('<end>')\n",
    "        sampled_caption = sampled_caption[:end_index]\n",
    "\n",
    "    # Remove words before (and including) the '<start>' token\n",
    "    if '<start>' in sampled_caption:\n",
    "        start_index = sampled_caption.index('<start>')\n",
    "        sampled_caption = sampled_caption[start_index+1:]\n",
    "        \n",
    "    generated_caption = ' '.join(sampled_caption)\n",
    "\n",
    "    # Get ground truth captions\n",
    "    image_name = image_ids[0]\n",
    "    gt_captions = test_image2captions.get(image_name, [])\n",
    "\n",
    "    if not gt_captions:\n",
    "        print(f'Image ID: {image_name}')\n",
    "        print('Generated Caption:', generated_caption)\n",
    "        print('Ground Truth Captions: None')\n",
    "        print('------------------------------------')\n",
    "        continue\n",
    "\n",
    "    print(f'Image ID: {image_name}')\n",
    "    print(f'Generated Caption: {generated_caption}')\n",
    "    print('Ground Truth Captions:')\n",
    "    for gt_caption in gt_captions:\n",
    "        print(f'- {\" \".join(gt_caption)}')  # Join words for ground truth captions\n",
    "    print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103345 tokens at 953155.03 tokens per second.\n",
      "PTBTokenizer tokenized 21347 tokens at 347864.11 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.12747498835024573\n",
      "METEOR Score: 0.3561023538912437\n",
      "CIDEr Score: 0.2857489304258189\n"
     ]
    }
   ],
   "source": [
    "image2captions = prepare_image2captions(test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Calculate BLEU, METEOR, and CIDEr scores on the sampled test images\n",
    "bleu_score = calculate_bleu_score(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_dir=image_dir,\n",
    "    image_ids=test_images,\n",
    "    image2captions=image2captions,\n",
    "    transform=test_transform,\n",
    "    idx2word=idx2word,\n",
    "    device=device,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "meteor = calculate_meteor_score(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_dir=image_dir,\n",
    "    image_ids=test_images,\n",
    "    image2captions=image2captions,\n",
    "    transform=test_transform,\n",
    "    idx2word=idx2word,\n",
    "    device=device,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "cider = calculate_cider_score(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_dir=image_dir,\n",
    "    image_ids=test_images,\n",
    "    image2captions=image2captions,\n",
    "    transform=test_transform,\n",
    "    idx2word=idx2word,\n",
    "    device=device,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "print(\"METEOR Score:\", meteor)\n",
    "print(\"CIDEr Score:\", cider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
