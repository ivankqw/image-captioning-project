{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/jed/anaconda3/omscs/CS7643/image-captioning-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path(os.getcwd()).resolve()  # Get the current working directory\n",
    "project_root = notebook_dir.parents[1]  # Adjust the number to go up to the project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.model_1_baseline_cnn_lstm.model import *\n",
    "from data.dataset import *\n",
    "from data.preprocessing import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function: Computes validation loss on a given dataset\n",
    "def evaluate(encoder, decoder, data_loader, criterion, device, vocab_size):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    Args:\n",
    "        encoder: Encoder model.\n",
    "        decoder: Decoder model.\n",
    "        data_loader: DataLoader for the validation set.\n",
    "        criterion: Loss function.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        vocab_size: Size of the vocabulary.\n",
    "    Returns:\n",
    "        average_loss: Average validation loss.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, captions, _ in data_loader:\n",
    "            # Move data to the computation device\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass through encoder and decoder\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Exclude the first time step from outputs and targets\n",
    "            outputs = outputs[:, 1:, :]  # Ensure outputs and targets have the same length\n",
    "            targets = captions[:, 1:]  # Exclude the first <start> token from targets\n",
    "\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "    # Calculate average loss\n",
    "    average_loss = total_loss / total_samples\n",
    "    return average_loss\n",
    "\n",
    "def main():\n",
    "    # Define dataset type\n",
    "    dataset = \"Flickr8k\"  # Change to \"Flickr30k\" if needed\n",
    "\n",
    "    # Paths\n",
    "    captions_file_path = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "    image_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    cider_scores = []\n",
    "\n",
    "    # Load captions\n",
    "    caption_df = pd.read_csv(captions_file_path).dropna().drop_duplicates()\n",
    "\n",
    "    # Build vocabulary\n",
    "    word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=5000)\n",
    "\n",
    "    # Convert captions to sequences\n",
    "    captions_seqs, max_length = convert_captions_to_sequences(\n",
    "        image_captions, word2idx\n",
    "    )\n",
    "\n",
    "    # Get data transformations\n",
    "    train_transform = get_transform(train=True)\n",
    "    val_transform = get_transform(train=False)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    image_names = list(image_captions.keys())\n",
    "    train_images, val_images, _ = get_splits(image_names, test_size=0.2)\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = FlickrDataset(\n",
    "        image_dir, train_images, captions_seqs, transform=train_transform\n",
    "    )\n",
    "    val_dataset = FlickrDataset(\n",
    "        image_dir, val_images, captions_seqs, transform=val_transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize models\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    vocab_size = len(word2idx)\n",
    "    encoder = EncoderCNN(embed_size=embed_size).to(device)\n",
    "    decoder = DecoderRNN(\n",
    "        embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
    "    params = list(filter(lambda p: p.requires_grad, encoder.parameters())) + list(\n",
    "        decoder.parameters()\n",
    "    )\n",
    "    optimizer = optim.Adam(params, lr=3e-5)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    # Training settings\n",
    "    num_epochs = 10\n",
    "    total_step = len(train_loader)\n",
    "    end_token_idx = word2idx[\"<end>\"]\n",
    "\n",
    "    # Prepare validation image IDs and references for metrics\n",
    "    val_image_ids = val_images\n",
    "    image2captions = prepare_image2captions(val_image_ids, captions_seqs, idx2word)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Prepare targets\n",
    "            targets = captions[:, 1:]  # Exclude the first <start> token\n",
    "\n",
    "            # Exclude the first time step from outputs\n",
    "            outputs = outputs[:, 1:, :]  # Now outputs and targets have the same sequence length\n",
    "\n",
    "            # Reshape for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 300 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_loss / total_step\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate(encoder, decoder, val_loader, criterion, device, vocab_size)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        bleu = calculate_bleu_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        meteor = calculate_meteor_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        cider = calculate_cider_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds.\"\n",
    "        )\n",
    "        print(\n",
    "            f\"BLEU Score: {bleu:.4f}, METEOR Score: {meteor:.4f}, CIDEr Score: {cider:.4f}\\n\"\n",
    "        )\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu)\n",
    "        meteor_scores.append(meteor)\n",
    "        cider_scores.append(cider)\n",
    "\n",
    "    # Save the models\n",
    "    torch.save(encoder.state_dict(), f\"{project_root}/models/model_1_baseline_cnn_lstm/encoder.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"{project_root}/models/model_1_baseline_cnn_lstm/decoder.pth\")\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{project_root}/models/model_1_baseline_cnn_lstm/loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), bleu_scores, label='BLEU Score')\n",
    "    plt.plot(range(1, num_epochs + 1), meteor_scores, label='METEOR Score')\n",
    "    plt.plot(range(1, num_epochs + 1), cider_scores, label='CIDEr Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Evaluation Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{project_root}/models/model_1_baseline_cnn_lstm/metrics_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/10], Step [0/1011], Loss: 8.5159\n",
      "Epoch [1/10], Step [300/1011], Loss: 4.9667\n",
      "Epoch [1/10], Step [600/1011], Loss: 4.8035\n",
      "Epoch [1/10], Step [900/1011], Loss: 4.9348\n",
      "Epoch [1/10], Training Loss: 5.3401, Validation Loss: 4.7018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 676314.06 tokens per second.\n",
      "PTBTokenizer tokenized 15957 tokens at 221611.79 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] completed in 194.49 seconds.\n",
      "BLEU Score: 0.0555, METEOR Score: 0.1973, CIDEr Score: 0.0523\n",
      "\n",
      "Epoch [2/10], Step [0/1011], Loss: 4.6950\n",
      "Epoch [2/10], Step [300/1011], Loss: 4.8153\n",
      "Epoch [2/10], Step [600/1011], Loss: 4.5185\n",
      "Epoch [2/10], Step [900/1011], Loss: 4.3503\n",
      "Epoch [2/10], Training Loss: 4.5759, Validation Loss: 4.4510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 677939.03 tokens per second.\n",
      "PTBTokenizer tokenized 16013 tokens at 222323.06 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] completed in 195.56 seconds.\n",
      "BLEU Score: 0.0633, METEOR Score: 0.2389, CIDEr Score: 0.1170\n",
      "\n",
      "Epoch [3/10], Step [0/1011], Loss: 4.4257\n",
      "Epoch [3/10], Step [300/1011], Loss: 4.5446\n",
      "Epoch [3/10], Step [600/1011], Loss: 4.3620\n",
      "Epoch [3/10], Step [900/1011], Loss: 4.4452\n",
      "Epoch [3/10], Training Loss: 4.3319, Validation Loss: 4.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 682342.21 tokens per second.\n",
      "PTBTokenizer tokenized 22604 tokens at 281506.69 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] completed in 207.49 seconds.\n",
      "BLEU Score: 0.0327, METEOR Score: 0.2220, CIDEr Score: 0.0651\n",
      "\n",
      "Epoch [4/10], Step [0/1011], Loss: 4.5496\n",
      "Epoch [4/10], Step [300/1011], Loss: 3.9361\n",
      "Epoch [4/10], Step [600/1011], Loss: 3.9469\n",
      "Epoch [4/10], Step [900/1011], Loss: 4.2073\n",
      "Epoch [4/10], Training Loss: 4.1181, Validation Loss: 4.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 678154.89 tokens per second.\n",
      "PTBTokenizer tokenized 24170 tokens at 290910.99 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] completed in 212.46 seconds.\n",
      "BLEU Score: 0.0501, METEOR Score: 0.2550, CIDEr Score: 0.0953\n",
      "\n",
      "Epoch [5/10], Step [0/1011], Loss: 3.8538\n",
      "Epoch [5/10], Step [300/1011], Loss: 4.0162\n",
      "Epoch [5/10], Step [600/1011], Loss: 4.0143\n",
      "Epoch [5/10], Step [900/1011], Loss: 3.9329\n",
      "Epoch [5/10], Training Loss: 3.9509, Validation Loss: 3.8874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 673874.23 tokens per second.\n",
      "PTBTokenizer tokenized 23161 tokens at 285962.68 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] completed in 209.47 seconds.\n",
      "BLEU Score: 0.0673, METEOR Score: 0.3086, CIDEr Score: 0.1281\n",
      "\n",
      "Epoch [6/10], Step [0/1011], Loss: 4.0383\n",
      "Epoch [6/10], Step [300/1011], Loss: 4.1280\n",
      "Epoch [6/10], Step [600/1011], Loss: 3.7116\n",
      "Epoch [6/10], Step [900/1011], Loss: 3.7947\n",
      "Epoch [6/10], Training Loss: 3.8663, Validation Loss: 3.8708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 676357.75 tokens per second.\n",
      "PTBTokenizer tokenized 22001 tokens at 276481.06 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] completed in 208.24 seconds.\n",
      "BLEU Score: 0.0712, METEOR Score: 0.3188, CIDEr Score: 0.1382\n",
      "\n",
      "Epoch [7/10], Step [0/1011], Loss: 3.9954\n",
      "Epoch [7/10], Step [300/1011], Loss: 3.8535\n",
      "Epoch [7/10], Step [600/1011], Loss: 3.9468\n",
      "Epoch [7/10], Step [900/1011], Loss: 3.6664\n",
      "Epoch [7/10], Training Loss: 3.8525, Validation Loss: 3.8585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 668102.87 tokens per second.\n",
      "PTBTokenizer tokenized 22142 tokens at 271929.79 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] completed in 207.46 seconds.\n",
      "BLEU Score: 0.0718, METEOR Score: 0.3209, CIDEr Score: 0.1440\n",
      "\n",
      "Epoch [8/10], Step [0/1011], Loss: 3.8600\n",
      "Epoch [8/10], Step [300/1011], Loss: 3.9928\n",
      "Epoch [8/10], Step [600/1011], Loss: 3.8367\n",
      "Epoch [8/10], Step [900/1011], Loss: 3.8964\n",
      "Epoch [8/10], Training Loss: 3.8399, Validation Loss: 3.8471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 652659.83 tokens per second.\n",
      "PTBTokenizer tokenized 22541 tokens at 274991.92 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] completed in 208.61 seconds.\n",
      "BLEU Score: 0.0743, METEOR Score: 0.3215, CIDEr Score: 0.1481\n",
      "\n",
      "Epoch [9/10], Step [0/1011], Loss: 3.5798\n",
      "Epoch [9/10], Step [300/1011], Loss: 3.9434\n",
      "Epoch [9/10], Step [600/1011], Loss: 3.8031\n",
      "Epoch [9/10], Step [900/1011], Loss: 3.8459\n",
      "Epoch [9/10], Training Loss: 3.8283, Validation Loss: 3.8356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 672525.38 tokens per second.\n",
      "PTBTokenizer tokenized 22455 tokens at 278572.85 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] completed in 208.06 seconds.\n",
      "BLEU Score: 0.0757, METEOR Score: 0.3252, CIDEr Score: 0.1536\n",
      "\n",
      "Epoch [10/10], Step [0/1011], Loss: 4.0527\n",
      "Epoch [10/10], Step [300/1011], Loss: 3.9571\n",
      "Epoch [10/10], Step [600/1011], Loss: 3.8751\n",
      "Epoch [10/10], Step [900/1011], Loss: 3.9422\n",
      "Epoch [10/10], Training Loss: 3.8159, Validation Loss: 3.8251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 92084 tokens at 680738.39 tokens per second.\n",
      "PTBTokenizer tokenized 22716 tokens at 279755.23 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] completed in 207.69 seconds.\n",
      "BLEU Score: 0.0771, METEOR Score: 0.3266, CIDEr Score: 0.1555\n",
      "\n",
      "CPU times: user 6h 19min 13s, sys: 1min 27s, total: 6h 20min 40s\n",
      "Wall time: 34min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 2714703706_d21c5cb8df.jpg\n",
      "Generated Caption: a two dog is running in the grass .\n",
      "Ground Truth Captions:\n",
      "- ['dogs', 'playing']\n",
      "- ['a', 'brown', 'dog', 'is', 'biting', 'a', 'white', 'and', 'tan', 'dog', 'on', 'the', '<unk>', '.']\n",
      "- ['the', 'brown', 'dog', 'has', 'a', 'hold', 'of', 'the', 'other', 'dogs', 'cheek', 'with', 'its', 'teeth', '.']\n",
      "- ['two', 'dogs', 'are', 'nuzzling', 'each', 'other', 'nose', 'to', 'nose', '.']\n",
      "- ['two', 'dogs', 'bite', 'at', 'each', 'other', 'on', 'the', 'carpet', '.']\n",
      "------------------------------------\n",
      "Image ID: 3532194771_07faf20d76.jpg\n",
      "Generated Caption: a two dog is running in the water .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'man', 'is', 'heading', 'out', 'to', 'see', 'with', 'his', 'surfboard', 'in', 'hand', '.']\n",
      "- ['a', 'man', 'with', 'a', 'white', 'surfboard', 'is', 'walking', 'into', 'the', 'water', '.']\n",
      "- ['a', 'person', 'walks', 'into', 'the', 'water', 'carrying', 'a', 'white', 'surfboard', '.']\n",
      "- ['a', 'surfer', 'walking', 'into', 'the', 'ocean']\n",
      "- ['surfer', 'with', 'board', 'marches', 'out', 'to', 'sea', 'on', 'gray', 'day', '.']\n",
      "------------------------------------\n",
      "Image ID: 2356574282_5078f08b58.jpg\n",
      "Generated Caption: a man in a man in a red shirt is is on a red .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'arabian', 'dressed', 'lady', 'leans', 'backwards', 'holding', 'a', 'skinny', 'crooked', 'sword', '.']\n",
      "- ['a', 'costumed', 'woman', 'with', 'a', 'sword', 'does', 'a', '<unk>', '.']\n",
      "- ['a', 'woman', 'bending', 'over', 'backwards', '.']\n",
      "- ['a', 'woman', 'in', 'a', 'belly', 'dancing', 'outfit', 'bending', 'over', 'backwards', '.']\n",
      "- ['a', 'woman', 'in', 'a', 'dance', 'costume', 'is', 'bending', 'over', 'backward', 'and', 'holding', 'a', 'sword', '.']\n",
      "------------------------------------\n",
      "Image ID: 3526150930_580908dab6.jpg\n",
      "Generated Caption: a man in a woman in a red shirt is is on a red .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'woman', 'and', 'a', 'young', 'girl', 'pose', 'and', 'smile', 'for', 'a', 'photo', '.']\n",
      "- ['a', 'woman', 'and', 'a', 'young', 'girl', 'smiling', 'for', 'the', 'camera', ',', 'in', 'front', 'of', 'some', 'flowers', '.']\n",
      "- ['a', 'woman', 'and', 'girl', 'pose', 'together', 'in', 'a', 'garden', '.']\n",
      "- ['a', 'woman', 'poses', 'with', 'a', 'small', 'girl', 'on', 'her', 'lap', 'in', 'front', 'of', 'a', 'flower', 'bush', '.']\n",
      "- ['a', 'woman', 'with', 'brown', 'hair', 'is', 'sitting', 'with', 'a', 'little', 'girl', 'with', 'short', 'brown', 'hair', 'outside', 'next', 'to', 'some', 'red', 'flowers', '.']\n",
      "------------------------------------\n",
      "Image ID: 2448270671_5e0e391a80.jpg\n",
      "Generated Caption: a two dog is running in the water .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'brown', 'dog', 'is', 'leaping', 'over', 'a', 'fallen', 'tree', 'in', 'the', 'woods', '.']\n",
      "- ['a', 'brown', 'dog', 'with', 'tongue', 'sticking', 'out', 'jumping', 'over', 'a', 'log', '.']\n",
      "- ['a', 'dog', 'is', 'jumping', 'over', 'a', 'log', 'with', 'ears', 'flying', 'and', 'tongue', 'out', '.']\n",
      "- ['a', 'dog', 'leaps', 'over', 'a', 'log', 'in', 'the', 'woods', '.']\n",
      "- ['the', 'dog', 'with', 'big', 'ears', 'is', 'leaping', 'over', 'a', 'fallen', 'tree', '.']\n",
      "------------------------------------\n",
      "Image ID: 3482237861_605b4f0fd9.jpg\n",
      "Generated Caption: a man in a woman is on a red shirt and a man in a red .\n",
      "Ground Truth Captions:\n",
      "- ['a', 'boy', 'is', 'riding', 'a', 'scooter', 'down', 'the', 'concrete', 'path', '.']\n",
      "- ['a', 'child', 'on', 'a', 'scooter', 'moving', 'down', 'the', 'sidewalk', '.']\n",
      "- ['a', 'youth', 'rides', 'a', 'scooter', 'on', 'a', 'sidewalk', 'near', 'a', 'building', '.']\n",
      "- ['the', 'boy', 'is', 'riding', 'his', 'scooter', 'on', 'the', 'sidewalk', '.']\n",
      "- ['young', 'boy', 'rides', 'his', 'scooter', 'on', 'drive', '.']\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Flickr8k\"\n",
    "\n",
    "captions_file_path = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file_path).dropna().drop_duplicates()\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=5000)\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "\n",
    "# Get data transformations\n",
    "test_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "image_names = list(image_captions.keys())\n",
    "_, _, test_images = get_splits(image_names, test_size=0.2)\n",
    "\n",
    "# Prepare image to captions mapping for ground truth captions\n",
    "test_image2captions = prepare_image2captions(test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Create test dataset and data loader\n",
    "test_dataset = FlickrDataset(\n",
    "    image_dir, test_images, captions_seqs, transform=test_transform, mode='test'\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Process one image at a time\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "encoder = EncoderCNN(embed_size=embed_size).to(device)\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=embed_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size\n",
    ").to(device)\n",
    "\n",
    "# Load trained models\n",
    "encoder_path = os.path.join(project_root, \"models/model_1_baseline_cnn_lstm/encoder.pth\")\n",
    "decoder_path = os.path.join(project_root, \"models/model_1_baseline_cnn_lstm/decoder.pth\")\n",
    "\n",
    "encoder.load_state_dict(\n",
    "    torch.load(encoder_path, map_location=device, weights_only=True)\n",
    ")\n",
    "decoder.load_state_dict(\n",
    "    torch.load(decoder_path, map_location=device, weights_only=True)\n",
    ")\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "end_token_idx = word2idx.get('<end>', None)\n",
    "\n",
    "if end_token_idx is None:\n",
    "    raise ValueError(\"The '<end>' token was not found in the vocabulary.\")\n",
    "\n",
    "# Generate captions on test images\n",
    "for i, (images, captions, image_ids) in enumerate(test_loader):\n",
    "    if i >= 6:\n",
    "        break  # Stop after processing 6 images\n",
    "\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(images)\n",
    "        sampled_ids = decoder.sample(features, end_token_idx=end_token_idx)\n",
    "    \n",
    "    # Convert word IDs to words\n",
    "    sampled_caption = [idx2word.get(word_id, '<unk>') for word_id in sampled_ids]\n",
    "    \n",
    "    # Remove words after (and including) the '<end>' token\n",
    "    if '<end>' in sampled_caption:\n",
    "        end_index = sampled_caption.index('<end>')\n",
    "        sampled_caption = sampled_caption[:end_index]\n",
    "    \n",
    "    generated_caption = ' '.join(sampled_caption)\n",
    "\n",
    "    # Get ground truth captions\n",
    "    image_name = image_ids[0]\n",
    "    gt_captions = test_image2captions.get(image_name, [])\n",
    "\n",
    "    if not gt_captions:\n",
    "        print(f'Image ID: {image_name}')\n",
    "        print('Generated Caption:', generated_caption)\n",
    "        print('Ground Truth Captions: None')\n",
    "        print('------------------------------------')\n",
    "        continue\n",
    "\n",
    "    print(f'Image ID: {image_name}')\n",
    "    print(f'Generated Caption: {generated_caption}')\n",
    "    print('Ground Truth Captions:')\n",
    "    for gt_caption in gt_captions:\n",
    "        print(f'- {gt_caption}')\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
