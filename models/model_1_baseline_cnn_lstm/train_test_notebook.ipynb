{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/jed/anaconda3/omscs/CS7643/image-captioning-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path(os.getcwd()).resolve()  # Get the current working directory\n",
    "project_root = notebook_dir.parents[1]  # Adjust the number to go up to the project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.model_1_baseline_cnn_lstm.model import *\n",
    "from data.dataset import *\n",
    "from data.preprocessing import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function: Computes validation loss on a given dataset\n",
    "def evaluate(encoder, decoder, data_loader, criterion, device, vocab_size):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    Args:\n",
    "        encoder: Encoder model.\n",
    "        decoder: Decoder model.\n",
    "        data_loader: DataLoader for the validation set.\n",
    "        criterion: Loss function.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        vocab_size: Size of the vocabulary.\n",
    "    Returns:\n",
    "        average_loss: Average validation loss.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, captions, _ in data_loader:\n",
    "            # Move data to the computation device\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass through encoder and decoder\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Exclude the first time step from outputs and targets\n",
    "            outputs = outputs[:, 1:, :]  # Ensure outputs and targets have the same length\n",
    "            targets = captions[:, 1:]  # Exclude the first <start> token from targets\n",
    "\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "    # Calculate average loss\n",
    "    average_loss = total_loss / total_samples\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define dataset type\n",
    "    dataset = \"Flickr8k\"  # or Flickr30k if needed\n",
    "    \n",
    "    random.seed(7643)\n",
    "\n",
    "    # Paths\n",
    "    dataset_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "    captions_file = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "    image_dir = dataset_dir\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    cider_scores = []\n",
    "\n",
    "    # Load captions\n",
    "    caption_df = pd.read_csv(captions_file).dropna().drop_duplicates()\n",
    "\n",
    "    # Build vocabulary\n",
    "    word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=8000)\n",
    "    print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "    # Convert captions to sequences\n",
    "    captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "    print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "    # Get data transformations\n",
    "    train_transform = get_transform(train=True)\n",
    "    val_transform = get_transform(train=False)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    image_names = list(image_captions.keys())\n",
    "    random.shuffle(image_names)\n",
    "    val_size = int(0.2 * len(image_names))  # 20% for validation\n",
    "    train_images = image_names[val_size:]\n",
    "    val_images = image_names[:val_size]\n",
    "    print(f\"Training samples: {len(train_images)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    # Note the mode='train' for val_dataset to ensure it behaves like the training dataset\n",
    "    train_dataset = FlickrDataset(\n",
    "        image_dir, train_images, captions_seqs, transform=train_transform, mode='train'\n",
    "    )\n",
    "    val_dataset = FlickrDataset(\n",
    "        image_dir, val_images, captions_seqs, transform=val_transform, mode='train'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize models\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    vocab_size = len(word2idx)\n",
    "    encoder = Encoder(embed_size=embed_size).to(device)\n",
    "    decoder = Decoder(\n",
    "        embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
    "    params = list(filter(lambda p: p.requires_grad, encoder.parameters())) + list(\n",
    "        decoder.parameters()\n",
    "    )\n",
    "    optimizer = optim.Adam(params, lr=3e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    num_epochs = 10\n",
    "    total_step = len(train_loader)\n",
    "    end_token_idx = word2idx[\"<end>\"]\n",
    "\n",
    "    val_image_ids = val_images\n",
    "    image2captions = prepare_image2captions(val_image_ids, captions_seqs, idx2word)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Prepare targets\n",
    "            targets = captions[:, 1:]  # Exclude the first <start> token\n",
    "\n",
    "            # Exclude the first time step from outputs\n",
    "            outputs = outputs[:, 1:, :]\n",
    "\n",
    "            # Reshape for loss computation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 300 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_loss / total_step\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate(encoder, decoder, val_loader, criterion, device, vocab_size)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        bleu = calculate_bleu_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        meteor = calculate_meteor_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        cider = calculate_cider_score(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            image_dir=image_dir,\n",
    "            image_ids=val_image_ids,\n",
    "            image2captions=image2captions,\n",
    "            transform=val_transform,\n",
    "            idx2word=idx2word,\n",
    "            device=device,\n",
    "            word2idx=word2idx,\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds.\"\n",
    "        )\n",
    "        print(\n",
    "            f\"BLEU Score: {bleu:.4f}, METEOR Score: {meteor:.4f}, CIDEr Score: {cider:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu)\n",
    "        meteor_scores.append(meteor)\n",
    "        cider_scores.append(cider)\n",
    "\n",
    "    # Save the models\n",
    "    os.makedirs(\"models/model_1_baseline_cnn_lstm\", exist_ok=True)\n",
    "    torch.save(encoder.state_dict(), \"models/model_1_baseline_cnn_lstm/encoder.pth\")\n",
    "    torch.save(decoder.state_dict(), \"models/model_1_baseline_cnn_lstm/decoder.pth\")\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_1_baseline_cnn_lstm/loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), bleu_scores, label='BLEU Score')\n",
    "    plt.plot(range(1, num_epochs + 1), meteor_scores, label='METEOR Score')\n",
    "    plt.plot(range(1, num_epochs + 1), cider_scores, label='CIDEr Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Evaluation Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/model_1_baseline_cnn_lstm/metrics_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/10], Step [0/506], Loss: 8.9884\n",
      "Epoch [1/10], Step [300/506], Loss: 4.2047\n",
      "Epoch [1/10], Training Loss: 4.5941, Validation Loss: 3.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 737051.85 tokens per second.\n",
      "PTBTokenizer tokenized 25461 tokens at 305143.86 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] completed in 325.10 seconds.\n",
      "BLEU Score: 0.0750, METEOR Score: 0.2961, CIDEr Score: 0.1440\n",
      "\n",
      "Epoch [2/10], Step [0/506], Loss: 3.9051\n",
      "Epoch [2/10], Step [300/506], Loss: 3.5489\n",
      "Epoch [2/10], Training Loss: 3.6125, Validation Loss: 3.4748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 725576.81 tokens per second.\n",
      "PTBTokenizer tokenized 22131 tokens at 274827.31 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] completed in 314.77 seconds.\n",
      "BLEU Score: 0.0920, METEOR Score: 0.3070, CIDEr Score: 0.1711\n",
      "\n",
      "Epoch [3/10], Step [0/506], Loss: 3.3565\n",
      "Epoch [3/10], Step [300/506], Loss: 3.5784\n",
      "Epoch [3/10], Training Loss: 3.3238, Validation Loss: 3.2837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 725878.16 tokens per second.\n",
      "PTBTokenizer tokenized 23096 tokens at 283143.32 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] completed in 316.39 seconds.\n",
      "BLEU Score: 0.1031, METEOR Score: 0.3239, CIDEr Score: 0.2145\n",
      "\n",
      "Epoch [4/10], Step [0/506], Loss: 3.0823\n",
      "Epoch [4/10], Step [300/506], Loss: 3.4129\n",
      "Epoch [4/10], Training Loss: 3.1408, Validation Loss: 3.1623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 721784.90 tokens per second.\n",
      "PTBTokenizer tokenized 25094 tokens at 298166.41 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] completed in 322.50 seconds.\n",
      "BLEU Score: 0.0965, METEOR Score: 0.3154, CIDEr Score: 0.2121\n",
      "\n",
      "Epoch [5/10], Step [0/506], Loss: 2.8671\n",
      "Epoch [5/10], Step [300/506], Loss: 3.1885\n",
      "Epoch [5/10], Training Loss: 3.0091, Validation Loss: 3.0762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 729525.75 tokens per second.\n",
      "PTBTokenizer tokenized 22572 tokens at 277436.43 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] completed in 320.14 seconds.\n",
      "BLEU Score: 0.1267, METEOR Score: 0.3569, CIDEr Score: 0.2776\n",
      "\n",
      "Epoch [6/10], Step [0/506], Loss: 2.8784\n",
      "Epoch [6/10], Step [300/506], Loss: 2.9786\n",
      "Epoch [6/10], Training Loss: 2.8976, Validation Loss: 3.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 731172.43 tokens per second.\n",
      "PTBTokenizer tokenized 23036 tokens at 279777.02 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] completed in 318.46 seconds.\n",
      "BLEU Score: 0.1262, METEOR Score: 0.3557, CIDEr Score: 0.2839\n",
      "\n",
      "Epoch [7/10], Step [0/506], Loss: 2.8493\n",
      "Epoch [7/10], Step [300/506], Loss: 2.8290\n",
      "Epoch [7/10], Training Loss: 2.8821, Validation Loss: 3.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 731738.16 tokens per second.\n",
      "PTBTokenizer tokenized 22423 tokens at 268795.62 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] completed in 319.11 seconds.\n",
      "BLEU Score: 0.1308, METEOR Score: 0.3605, CIDEr Score: 0.2875\n",
      "\n",
      "Epoch [8/10], Step [0/506], Loss: 2.7277\n",
      "Epoch [8/10], Step [300/506], Loss: 2.7741\n",
      "Epoch [8/10], Training Loss: 2.8712, Validation Loss: 3.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 733707.25 tokens per second.\n",
      "PTBTokenizer tokenized 22985 tokens at 281248.35 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] completed in 316.73 seconds.\n",
      "BLEU Score: 0.1291, METEOR Score: 0.3563, CIDEr Score: 0.2861\n",
      "\n",
      "Epoch [9/10], Step [0/506], Loss: 2.9517\n",
      "Epoch [9/10], Step [300/506], Loss: 2.9790\n",
      "Epoch [9/10], Training Loss: 2.8599, Validation Loss: 3.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 730037.37 tokens per second.\n",
      "PTBTokenizer tokenized 22840 tokens at 275565.72 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] completed in 318.02 seconds.\n",
      "BLEU Score: 0.1289, METEOR Score: 0.3581, CIDEr Score: 0.2900\n",
      "\n",
      "Epoch [10/10], Step [0/506], Loss: 2.8555\n",
      "Epoch [10/10], Step [300/506], Loss: 2.9985\n",
      "Epoch [10/10], Training Loss: 2.8482, Validation Loss: 3.0280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103464 tokens at 726141.14 tokens per second.\n",
      "PTBTokenizer tokenized 22620 tokens at 279787.09 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] completed in 399.83 seconds.\n",
      "BLEU Score: 0.1305, METEOR Score: 0.3579, CIDEr Score: 0.2937\n",
      "\n",
      "CPU times: user 7h 26min 58s, sys: 12min 43s, total: 7h 39min 42s\n",
      "Wall time: 54min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/2ll4226j2cdf65tbnf1cwhp00000gn/T/ipykernel_95388/1850273071.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(encoder_path, map_location=device)\n",
      "/var/folders/ms/2ll4226j2cdf65tbnf1cwhp00000gn/T/ipykernel_95388/1850273071.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(decoder_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 2371809188_b805497cba.jpg\n",
      "Generated Caption: a man in a red shirt is standing on a bench with a red and white dog .\n",
      "Ground Truth Captions:\n",
      "- a boy climbs an indoor rock climbing wall .\n",
      "- a boy climbs a rock wall .\n",
      "- a boy is climbing up a rock-climbing wall while an older boy stands on the ground\n",
      "- boy rock climbing on a blue wall while an adult looks away .\n",
      "- little boy climbing an indoor rock climbing wall .\n",
      "------------------------------------\n",
      "Image ID: 3430607596_7e4f74e3ff.jpg\n",
      "Generated Caption: a boy in a red shirt is jumping into the water .\n",
      "Ground Truth Captions:\n",
      "- a boy in a red suit plays in the water .\n",
      "- a boy in a red swimsuit jumps into the water to join two people .\n",
      "- a boy takes a flying leap into the water .\n",
      "- the boy in the red shorts jumps into the water to join other people .\n",
      "- the boy wearing red shorts is jumping into the river as other children swim .\n",
      "------------------------------------\n",
      "Image ID: 3545779287_8f52e06909.jpg\n",
      "Generated Caption: a black dog is running through the grass .\n",
      "Ground Truth Captions:\n",
      "- a black and brown dog has rope in its mouth .\n",
      "- a dog jumps to catch a rope toy .\n",
      "- a dog with a rope toy in its mouth runs on the grass .\n",
      "- a large dog is running on the grass with a rope in its mouth .\n",
      "- the dog is running with a rope toy .\n",
      "------------------------------------\n",
      "Image ID: 451597318_4f370b1339.jpg\n",
      "Generated Caption: a dog is running through a field of a fence .\n",
      "Ground Truth Captions:\n",
      "- a brown dog leaps up to catch an orange toy .\n",
      "- a dog catches a disk in the air .\n",
      "- a dog is jumping in the air to catch an orange frisbee .\n",
      "- a dog leaping to catch a frisbee in the yard .\n",
      "- brown dog leaping up with orange disc in mouth with blue and yellow toy boat in background .\n",
      "------------------------------------\n",
      "Image ID: 2194806429_ca4c3770c1.jpg\n",
      "Generated Caption: a man in a red shirt and a hat is standing on a bench .\n",
      "Ground Truth Captions:\n",
      "- the two girls ride together on the bicycle while sightseeing .\n",
      "- two girls on a tandem bicycle , looking at wall art .\n",
      "- two woman ride a bicycle together as they pass a painting on a wall .\n",
      "- two women on a red two seater bike looking at a painting on wall\n",
      "- two women riding a tandem bicycle go by a wall painting .\n",
      "------------------------------------\n",
      "Image ID: 2789648482_1df61f224a.jpg\n",
      "Generated Caption: a boy in a red shirt is jumping off a rock slide .\n",
      "Ground Truth Captions:\n",
      "- a boy jumps off a platform whilst two other boys stand and watch .\n",
      "- a child is jumping off a platform into a pool .\n",
      "- three boys jumping off of a platform .\n",
      "- three children on diving board , with one jumping off .\n",
      "- two boys standing on top of a diving platform and one captured midair .\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "dataset = \"Flickr8k\"\n",
    "\n",
    "captions_file_path = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file_path).dropna().drop_duplicates()\n",
    "\n",
    "# Build vocabulary with vocab_size=5000\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=8000)\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "\n",
    "# Get data transformations\n",
    "test_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "image_names = list(image_captions.keys())\n",
    "random.shuffle(image_names)\n",
    "val_size = int(0.2 * len(image_names))  # 20% for validation\n",
    "test_images = image_names[:val_size]\n",
    "\n",
    "# Randomly select 6 images from the test_images\n",
    "sampled_test_images = random.sample(test_images, 6)\n",
    "\n",
    "# Prepare image to captions mapping for ground truth captions\n",
    "test_image2captions = prepare_image2captions(sampled_test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Create test dataset and data loader for only those 6 randomly selected images\n",
    "test_dataset = FlickrDataset(\n",
    "    image_dir, sampled_test_images, captions_seqs, transform=test_transform, mode='test'\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Process one image at a time\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize models\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "encoder = Encoder(embed_size=embed_size).to(device)\n",
    "decoder = Decoder(\n",
    "    embed_size=embed_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size\n",
    ").to(device)\n",
    "\n",
    "# Load trained models\n",
    "encoder_path = os.path.join(project_root, \"models/model_1_baseline_cnn_lstm/encoder.pth\")\n",
    "decoder_path = os.path.join(project_root, \"models/model_1_baseline_cnn_lstm/decoder.pth\")\n",
    "\n",
    "encoder.load_state_dict(\n",
    "    torch.load(encoder_path, map_location=device)\n",
    ")\n",
    "decoder.load_state_dict(\n",
    "    torch.load(decoder_path, map_location=device)\n",
    ")\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "end_token_idx = word2idx.get('<end>', None)\n",
    "\n",
    "# Generate captions on the randomly selected test images\n",
    "for i, (images, captions, image_ids) in enumerate(test_loader):\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(images)\n",
    "        sampled_ids = decoder.sample(features, end_token_idx=end_token_idx)\n",
    "    \n",
    "    # Convert word IDs to words\n",
    "    sampled_caption = [idx2word.get(word_id, '<unk>') for word_id in sampled_ids]\n",
    "    \n",
    "    # Remove words after (and including) the '<end>' token\n",
    "    if '<end>' in sampled_caption:\n",
    "        end_index = sampled_caption.index('<end>')\n",
    "        sampled_caption = sampled_caption[:end_index]\n",
    "    \n",
    "    generated_caption = ' '.join(sampled_caption)\n",
    "\n",
    "    # Get ground truth captions\n",
    "    image_name = image_ids[0]\n",
    "    gt_captions = test_image2captions.get(image_name, [])\n",
    "\n",
    "    if not gt_captions:\n",
    "        print(f'Image ID: {image_name}')\n",
    "        print('Generated Caption:', generated_caption)\n",
    "        print('Ground Truth Captions: None')\n",
    "        print('------------------------------------')\n",
    "        continue\n",
    "\n",
    "    print(f'Image ID: {image_name}')\n",
    "    print(f'Generated Caption: {generated_caption}')\n",
    "    print('Ground Truth Captions:')\n",
    "    for gt_caption in gt_captions:\n",
    "        print(f'- {\" \".join(gt_caption)}')\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 102583 tokens at 819776.73 tokens per second.\n",
      "PTBTokenizer tokenized 24623 tokens at 333123.07 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.1271150728175305\n",
      "METEOR Score: 0.36926633570867284\n",
      "CIDEr Score: 0.323913207173524\n"
     ]
    }
   ],
   "source": [
    "image2captions = prepare_image2captions(test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Calculate BLEU, METEOR, and CIDEr scores on the sampled test images\n",
    "bleu_score = calculate_bleu_score(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_dir=image_dir,\n",
    "    image_ids=test_images,\n",
    "    image2captions=image2captions,\n",
    "    transform=test_transform,\n",
    "    idx2word=idx2word,\n",
    "    device=device,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "meteor = calculate_meteor_score(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_dir=image_dir,\n",
    "    image_ids=test_images,\n",
    "    image2captions=image2captions,\n",
    "    transform=test_transform,\n",
    "    idx2word=idx2word,\n",
    "    device=device,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "cider = calculate_cider_score(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_dir=image_dir,\n",
    "    image_ids=test_images,\n",
    "    image2captions=image2captions,\n",
    "    transform=test_transform,\n",
    "    idx2word=idx2word,\n",
    "    device=device,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "print(\"METEOR Score:\", meteor)\n",
    "print(\"CIDEr Score:\", cider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
