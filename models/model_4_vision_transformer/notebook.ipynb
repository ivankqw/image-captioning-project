{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"dataset\": \"Flickr8k\",  # Options: 'Flickr8k', 'Flickr30k'\n",
    "    \"image_dir\": \"./flickr_data/Flickr8k_Dataset/Images\",  # Path to images\n",
    "    \"captions_file\": \"./flickr_data/Flickr8k_Dataset/captions.txt\",  # Path to captions\n",
    "    \"vocab_size\": 5000,  # Maximum vocabulary size\n",
    "    \"embed_size\": 256,  # Embedding size (optional if not using separate embeddings)\n",
    "    \"hidden_size\": 512,  # Hidden size for decoder (not directly used with Hugging Face models)\n",
    "    \"batch_size\": 32,  # Batch size\n",
    "    \"num_epochs\": 10,  # Number of training epochs\n",
    "    \"learning_rate\": 5e-5,  # Learning rate\n",
    "    \"weight_decay\": 1e-4,  # Weight decay for optimizer\n",
    "    \"max_length\": 50,  # Maximum caption length for generation\n",
    "    \"num_beams\": 5,  # Number of beams for beam search\n",
    "    \"save_dir\": \"models/\",  # Directory to save models and plots\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/ivankoh/Personal/image-captioning-project\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ivankoh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    GPT2Tokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "notebook_dir = Path(os.getcwd()).resolve()  # Get the current working directory\n",
    "project_root = notebook_dir.parents[1]  # Adjust the number to go up to the project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Some additional settings for full reproducibility (optional)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(hyperparams[\"seed\"])\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions loaded: 40445\n",
      "Vocabulary size: 8921\n",
      "Maximum caption length: 40\n",
      "Training samples: 6472\n",
      "Validation samples: 1457\n",
      "Number of training batches: 1011\n",
      "Number of validation batches: 228\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import *\n",
    "from data.preprocessing import *\n",
    "\n",
    "\n",
    "dataset = \"Flickr8k\"  # Change to \"Flickr30k\" if needed\n",
    "\n",
    "# Paths\n",
    "dataset_dir = f\"../../flickr_data/{dataset}_Dataset/Images\"\n",
    "captions_file = f\"../../flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = dataset_dir\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "bleu_scores = []\n",
    "meteor_scores = []\n",
    "cider_scores = []\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file).dropna().drop_duplicates()\n",
    "print(f\"Total captions loaded: {len(caption_df)}\")\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=10000)\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "# Get data transformations\n",
    "train_transform = get_transform(train=True)\n",
    "val_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "image_names = list(image_captions.keys())\n",
    "train_images, val_images, _ = get_splits(image_names, test_size=0.2)\n",
    "print(f\"Training samples: {len(train_images)}\")\n",
    "print(f\"Validation samples: {len(val_images)}\")\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = FlickrDataset(\n",
    "    image_dir, train_images, captions_seqs, transform=train_transform\n",
    ")\n",
    "val_dataset = FlickrDataset(\n",
    "    image_dir, val_images, captions_seqs, transform=val_transform\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    ")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d0b7c30cef4bc29ba0a36dbeef16b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d403a03c5f41fc90b310ed1c18a66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542ca7506bf449e3a7708d3e6aa6af9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e5582ff8b2414ca9003a78cd0147c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446e32f869a4477ea9b58f7aa94e4532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 special tokens to the tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cc41fb33b748d68d95afb42db032f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc20aadf96994419a9c57fe3fb9a7ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8bb5e9111c4773a945819b8eaa25ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccd2d6ef9c74cd69eb7adc533fde544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel(\n",
      "  (encoder): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTSdpaAttention(\n",
      "            (attention): ViTSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): ViTPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50260, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2SdpaAttention(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (crossattention): GPT2SdpaAttention(\n",
      "            (c_attn): Conv1D(nf=1536, nx=768)\n",
      "            (q_attn): Conv1D(nf=768, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=3072, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=3072)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPT2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add special tokens if not present\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<start>\",\n",
    "    \"eos_token\": \"<end>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "if num_added_tokens > 0:\n",
    "    print(f\"Added {num_added_tokens} special tokens to the tokenizer.\")\n",
    "\n",
    "# Initialize VisionEncoderDecoderModel\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-224\", \"gpt2\"\n",
    ")\n",
    "\n",
    "# Resize token embeddings to accommodate the new special tokens\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set special tokens for the model\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Set generation parameters\n",
    "model.config.max_length = hyperparams[\"max_length\"]\n",
    "model.config.num_beams = hyperparams[\"num_beams\"]\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device, tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluate the VisionEncoderDecoderModel on the validation set.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        data_loader: DataLoader for the validation set.\n",
    "        criterion: Loss function.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        tokenizer: Tokenizer used for encoding captions.\n",
    "    Returns:\n",
    "        average_loss: Average validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images, captions, lengths = batch\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=images, labels=captions)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "    # Calculate average loss\n",
    "    average_loss = total_loss / total_samples\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "def calculate_bleu_score(\n",
    "    model,\n",
    "    image_dir,\n",
    "    image_ids,\n",
    "    image2captions,\n",
    "    transform,\n",
    "    tokenizer,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for the generated captions.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        image_dir: Directory containing images.\n",
    "        image_ids: List of image IDs.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        tokenizer: Tokenizer used for encoding/decoding captions.\n",
    "        device: Computation device (CPU or GPU).\n",
    "    Returns:\n",
    "        bleu_score: Corpus BLEU score for generated captions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(image_ids, desc=\"Calculating BLEU\"):\n",
    "            img_path = os.path.join(image_dir, img_id)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption with greedy search (num_beams=1)\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=image,\n",
    "                max_length=hyperparams[\"max_length\"],\n",
    "                num_beams=1,  # Greedy search\n",
    "                do_sample=False,\n",
    "            )\n",
    "            generated_caption = tokenizer.decode(\n",
    "                generated_ids[0], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Prepare hypothesis\n",
    "            hypothesis = word_tokenize(generated_caption.lower())\n",
    "            hypotheses.append(hypothesis)\n",
    "\n",
    "            # Prepare references\n",
    "            ref_captions = image2captions.get(img_id, [])\n",
    "            refs = [word_tokenize(\" \".join(ref).lower()) for ref in ref_captions]\n",
    "            references.append(refs)\n",
    "\n",
    "    # Compute corpus BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def calculate_meteor_score(\n",
    "    model,\n",
    "    image_dir,\n",
    "    image_ids,\n",
    "    image2captions,\n",
    "    transform,\n",
    "    tokenizer,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate METEOR score for the generated captions.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        image_dir: Directory containing images.\n",
    "        image_ids: List of image IDs.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        tokenizer: Tokenizer used for encoding/decoding captions.\n",
    "        device: Computation device (CPU or GPU).\n",
    "    Returns:\n",
    "        average_meteor: Average METEOR score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    meteor_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(image_ids, desc=\"Calculating METEOR\"):\n",
    "            img_path = os.path.join(image_dir, img_id)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption with greedy search (num_beams=1)\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=image,\n",
    "                max_length=hyperparams[\"max_length\"],\n",
    "                num_beams=1,  # Greedy search\n",
    "                do_sample=False,\n",
    "            )\n",
    "            generated_caption = tokenizer.decode(\n",
    "                generated_ids[0], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Prepare hypothesis\n",
    "            hypothesis = \" \".join(word_tokenize(generated_caption.lower()))\n",
    "\n",
    "            # Prepare references\n",
    "            ref_captions = [\n",
    "                \" \".join(ref).lower() for ref in image2captions.get(img_id, [])\n",
    "            ]\n",
    "\n",
    "            # Calculate METEOR score for the current image\n",
    "            score = single_meteor_score(ref_captions, hypothesis)\n",
    "            meteor_scores.append(score)\n",
    "\n",
    "    # Compute average METEOR score\n",
    "    average_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "    return average_meteor\n",
    "\n",
    "\n",
    "def calculate_cider_score(\n",
    "    model,\n",
    "    image_dir,\n",
    "    image_ids,\n",
    "    image2captions,\n",
    "    transform,\n",
    "    tokenizer,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate CIDEr score for the generated captions.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        image_dir: Directory containing images.\n",
    "        image_ids: List of image IDs.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        tokenizer: Tokenizer used for encoding/decoding captions.\n",
    "        device: Computation device (CPU or GPU).\n",
    "    Returns:\n",
    "        cider_score: CIDEr score for generated captions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    gts = {}  # Ground truth captions\n",
    "    res = {}  # Generated captions\n",
    "    tokenizer_cider = PTBTokenizer()  # Tokenizer for captions\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(image_ids, desc=\"Calculating CIDEr\"):\n",
    "            img_path = os.path.join(image_dir, img_id)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption with greedy search (num_beams=1)\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=image,\n",
    "                max_length=hyperparams[\"max_length\"],\n",
    "                num_beams=1,  # Greedy search\n",
    "                do_sample=False,\n",
    "            )\n",
    "            generated_caption = tokenizer.decode(\n",
    "                generated_ids[0], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Prepare generated caption\n",
    "            sampled_caption = \" \".join(word_tokenize(generated_caption.lower()))\n",
    "\n",
    "            # Prepare references\n",
    "            references = [\n",
    "                \" \".join(ref).lower() for ref in image2captions.get(img_id, [])\n",
    "            ]\n",
    "\n",
    "            # Update dictionaries with tokenized captions\n",
    "            gts[img_id] = [{\"caption\": ref} for ref in references]\n",
    "            res[img_id] = [{\"caption\": sampled_caption}]\n",
    "\n",
    "    # Tokenize captions\n",
    "    gts = tokenizer_cider.tokenize(gts)\n",
    "    res = tokenizer_cider.tokenize(res)\n",
    "\n",
    "    # Compute CIDEr score\n",
    "    cider_scorer = Cider()\n",
    "    cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "    return cider_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics storage\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "bleu_scores = []\n",
    "meteor_scores = []\n",
    "cider_scores = []\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=hyperparams[\"learning_rate\"],\n",
    "    weight_decay=hyperparams[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# Total training steps\n",
    "num_epochs = hyperparams[\"num_epochs\"]\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",  # We want to minimize the validation loss\n",
    "    factor=0.5,  # Factor by which the learning rate will be reduced\n",
    "    patience=2,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=True,  # Print a message when the learning rate is updated\n",
    ")\n",
    "\n",
    "# Prepare image to captions mapping for evaluation\n",
    "val_image2captions = prepare_image2captions(val_images, captions_seqs, idx2word)\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        images, captions, lengths = batch\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=images, labels=captions)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = evaluate_model(model, val_loader, criterion, device, tokenizer)\n",
    "    scheduler.step(val_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    bleu = calculate_bleu_score(\n",
    "        model,\n",
    "        hyperparams[\"image_dir\"],\n",
    "        val_images,\n",
    "        val_image2captions,\n",
    "        get_transform(train=False),\n",
    "        tokenizer,\n",
    "        device,\n",
    "    )\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    meteor = calculate_meteor_score(\n",
    "        model,\n",
    "        hyperparams[\"image_dir\"],\n",
    "        val_images,\n",
    "        val_image2captions,\n",
    "        get_transform(train=False),\n",
    "        tokenizer,\n",
    "        device,\n",
    "    )\n",
    "    meteor_scores.append(meteor)\n",
    "\n",
    "    cider = calculate_cider_score(\n",
    "        model,\n",
    "        hyperparams[\"image_dir\"],\n",
    "        val_images,\n",
    "        val_image2captions,\n",
    "        get_transform(train=False),\n",
    "        tokenizer,\n",
    "        device,\n",
    "    )\n",
    "    cider_scores.append(cider)\n",
    "\n",
    "    # Print epoch summary\n",
    "    epoch_duration = time.time() - start_time\n",
    "    print(\n",
    "        f\"\\nEpoch [{epoch+1}/{num_epochs}] completed in {epoch_duration:.2f}s\"\n",
    "        f\"\\nTraining Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        f\"\\nBLEU Score: {bleu:.4f}, METEOR Score: {meteor:.4f}, CIDEr Score: {cider:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save directory if it doesn't exist\n",
    "os.makedirs(hyperparams[\"save_dir\"], exist_ok=True)\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "loss_plot_path = os.path.join(hyperparams[\"save_dir\"], \"loss_plot.png\")\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.show()\n",
    "print(f\"Loss plot saved to {loss_plot_path}\")\n",
    "\n",
    "# Plot Evaluation Metrics\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), bleu_scores, label=\"BLEU Score\")\n",
    "plt.plot(range(1, num_epochs + 1), meteor_scores, label=\"METEOR Score\")\n",
    "plt.plot(range(1, num_epochs + 1), cider_scores, label=\"CIDEr Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Evaluation Metrics over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "metrics_plot_path = os.path.join(hyperparams[\"save_dir\"], \"metrics_plot.png\")\n",
    "plt.savefig(metrics_plot_path)\n",
    "plt.show()\n",
    "print(f\"Metrics plot saved to {metrics_plot_path}\")\n",
    "\n",
    "# Cell 9: Saving the Model\n",
    "\n",
    "# Save the trained model's state dictionary\n",
    "model_save_path = os.path.join(\n",
    "    hyperparams[\"save_dir\"], f\"vision_encoder_decoder_{hyperparams['dataset']}.pth\"\n",
    ")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_save_path = os.path.join(hyperparams[\"save_dir\"], \"tokenizer\")\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "print(f\"Tokenizer saved to {tokenizer_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Flickr8k\"\n",
    "\n",
    "captions_file_path = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file_path).dropna().drop_duplicates()\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=5000)\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "\n",
    "# Get data transformations\n",
    "test_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "image_names = list(image_captions.keys())\n",
    "_, _, test_images = get_splits(image_names, test_size=0.2)\n",
    "\n",
    "# Prepare image to captions mapping for ground truth captions\n",
    "test_image2captions = prepare_image2captions(test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Create test dataset and data loader\n",
    "test_dataset = FlickrDataset(\n",
    "    image_dir, test_images, captions_seqs, transform=test_transform, mode='test'\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Process one image at a time\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Function to generate and display captions for a given number of test images\n",
    "def generate_captions(model, test_loader, image2captions, transform, tokenizer, device, num_images=6):\n",
    "    \"\"\"\n",
    "    Generate captions for test images using greedy search and display them alongside ground truth captions.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        test_loader: DataLoader for the test set.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        tokenizer: Tokenizer used for encoding/decoding captions.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        num_images (int): Number of test images to process.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, image_ids) in enumerate(test_loader):\n",
    "            if i >= num_images:\n",
    "                break  # Stop after processing 'num_images' images\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Generate caption with greedy search (num_beams=1)\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=images,\n",
    "                max_length=hyperparams['max_length'],\n",
    "                num_beams=1,  # Greedy search\n",
    "                do_sample=False\n",
    "            )\n",
    "            generated_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Prepare hypothesis\n",
    "            hypothesis = word_tokenize(generated_caption.lower())\n",
    "\n",
    "            # Retrieve ground truth captions\n",
    "            image_name = image_ids[0]\n",
    "            gt_captions = image2captions.get(image_name, [])\n",
    "\n",
    "            # Remove unknown tokens and punctuation (optional)\n",
    "            # Here, assuming <unk>, <start>, <end> are already handled in preprocessing\n",
    "\n",
    "            # Print generated and ground truth captions\n",
    "            print(f\"Image ID: {image_name}\")\n",
    "            print(f\"Generated Caption: {generated_caption}\")\n",
    "            print(\"Ground Truth Captions:\")\n",
    "            for gt_caption in gt_captions:\n",
    "                gt_caption_str = ' '.join(gt_caption)\n",
    "                print(f\"- {gt_caption_str}\")\n",
    "            print('-' * 50)\n",
    "\n",
    "# Choose the number of images to generate captions for\n",
    "num_test_images = 6\n",
    "\n",
    "# Generate captions\n",
    "generate_captions(\n",
    "    model,\n",
    "    test_loader,\n",
    "    test_image2captions,\n",
    "    get_transform(train=False),\n",
    "    tokenizer,\n",
    "    device,\n",
    "    num_images=num_test_images\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-captioning-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
