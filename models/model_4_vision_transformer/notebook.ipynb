{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:28:37.798532Z",
     "iopub.status.busy": "2024-12-06T14:28:37.797714Z",
     "iopub.status.idle": "2024-12-06T14:28:50.490846Z",
     "shell.execute_reply": "2024-12-06T14:28:50.490056Z",
     "shell.execute_reply.started": "2024-12-06T14:28:37.798502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:28:46.360219: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-06 14:28:46.360293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-06 14:28:46.361527: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 14:28:46.368724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 14:28:47.221858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "! pip install pycocoevalcap -q\n",
    "! pip install evaluate -q\n",
    "# ! pip install --upgrade transformers==4.46.3\n",
    "\n",
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    # ViTImageProcessorFast,\n",
    "    AutoImageProcessor,\n",
    "    GPT2TokenizerFast,\n",
    ")\n",
    "import evaluate\n",
    "import wandb\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "notebook_dir = Path(os.getcwd()).resolve()  # Get the current working directory\n",
    "project_root = notebook_dir.parents[1]  # Adjust the number to go up to the project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "from data.dataset import FlickrDataset\n",
    "from data.preprocessing import *\n",
    "from metrics import calculate_metrics\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "_, _ = evaluate.load(\"bleu\"), evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:28:50.492849Z",
     "iopub.status.busy": "2024-12-06T14:28:50.492083Z",
     "iopub.status.idle": "2024-12-06T14:28:50.569916Z",
     "shell.execute_reply": "2024-12-06T14:28:50.569423Z",
     "shell.execute_reply.started": "2024-12-06T14:28:50.492823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Flickr8k\"\n",
    "encoder_name = \"google/vit-base-patch16-224-in21k\" # Vision encoder model\n",
    "decoder_name = \"openai-community/gpt2\"  # Text decoder model\n",
    "model_name = \"vit_gpt2\"  # Vision encoder-decoder model\n",
    "\n",
    "# Hyperparameters\n",
    "config = {\n",
    "    \"encoder_name\": encoder_name,  # Vision encoder model\n",
    "    \"decoder_name\": decoder_name,  # Text decoder model\n",
    "    \"image_dir\": f\"../../flickr_data/{dataset}_Dataset/Images\",  # Path to images\n",
    "    \"captions_file\": f\"../../flickr_data/{dataset}_Dataset/captions.txt\",  # Path to captions\n",
    "    \"model_save_dir\": f\"{model_name}\",  # Directory to save models\n",
    "    \"vocab_size\": 5000,  # Maximum vocabulary size\n",
    "    \"embed_size\": 256,  # Embedding size (optional if not using separate embeddings)\n",
    "    \"hidden_size\": 512,  # Hidden size for decoder (not directly used with Hugging Face models)\n",
    "    \"batch_size\": 32,  # Batch size\n",
    "    \"num_epochs\": 10,  # Number of training epochs\n",
    "    \"learning_rate\": 5e-5,  # Learning rate\n",
    "    \"weight_decay\": 1e-4,  # Weight decay for optimizer\n",
    "    \"num_beams\": 1,  # Number of beams for beam search, 1 means greedy search\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Some additional settings for full reproducibility (optional)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:28:50.571515Z",
     "iopub.status.busy": "2024-12-06T14:28:50.570774Z",
     "iopub.status.idle": "2024-12-06T14:29:00.583519Z",
     "shell.execute_reply": "2024-12-06T14:29:00.582689Z",
     "shell.execute_reply.started": "2024-12-06T14:28:50.571489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions loaded: 40445\n",
      "{'start': '<|endoftext|>', 'end': '<|endoftext|>', 'unk': '<|endoftext|>', 'pad': '<pad>'}\n",
      "Vocabulary size: 8519\n",
      "Maximum caption length: 43\n",
      "\n",
      "      Summary of special tokens replaced:\n",
      "      <start>: <|endoftext|>\n",
      "      <end>: <|endoftext|>\n",
      "      <pad>: <pad>\n",
      "      <unk>: <|endoftext|>\n",
      "\n",
      "\n",
      "    id of special tokens in vocab:\n",
      "    <start>: 1\n",
      "    <end>: 1\n",
      "    <pad>: 0\n",
      "    <unk>: 1\n",
      "\n",
      "\n",
      "    id of special tokens in tokenizer:\n",
      "    <start>: 50256\n",
      "    <end>: 50256\n",
      "    <pad>: 50257\n",
      "    <unk>: 50256\n",
      "\n",
      "\n",
      "    example sequences and captions:\n",
      "    1000268201_693b08cb0e.jpg\n",
      "    [[50256, 32, 1200, 287, 257, 11398, 6576, 318, 14281, 510, 257, 900, 286, 16046, 287, 281, 5726, 835, 764, 50256], [50256, 32, 2576, 1016, 656, 257, 13510, 2615, 764, 50256], [50256, 32, 1310, 2576, 14281, 656, 257, 13510, 711, 4803, 764, 50256], [50256, 32, 1310, 2576, 14281, 262, 16046, 284, 607, 711, 4803, 764, 50256], [50256, 32, 1310, 2576, 287, 257, 11398, 6576, 1016, 656, 257, 13510, 9351, 764, 50256]]\n",
      "    ['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load captions\n",
    "caption_df = pd.read_csv(config[\"captions_file\"]).dropna().drop_duplicates()\n",
    "print(f\"Total captions loaded: {len(caption_df)}\")\n",
    "\n",
    "# Build vocabulary\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(config[\"decoder_name\"])\n",
    "# https://github.com/huggingface/transformers/issues/2630\n",
    "# tokenizer.pad_token = (\n",
    "#     tokenizer.eos_token\n",
    "# )  # gpt2 does not have a pad token so we use eos token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "# special token mapping\n",
    "# for gpt2 tokenizer\n",
    "special_token_mapping = {\n",
    "    \"start\": tokenizer.bos_token,\n",
    "    \"end\": tokenizer.eos_token,\n",
    "    \"unk\": tokenizer.unk_token,\n",
    "    \"pad\": tokenizer.pad_token,\n",
    "}\n",
    "\n",
    "print(special_token_mapping)\n",
    "\n",
    "word2idx, idx2word, image_captions = build_vocabulary(\n",
    "    caption_df,\n",
    "    vocab_size=10000,\n",
    "    tokenizing_fn=lambda x: tokenizer.tokenize(x, add_special_tokens=False),\n",
    "    special_tokens=list(set(special_token_mapping.values())),\n",
    ")\n",
    "word2idx_nltk, idx2word_nltk, image_captions_nltk = build_vocabulary(\n",
    "    caption_df,\n",
    "    vocab_size=10000,\n",
    "    tokenizing_fn=word_tokenize,\n",
    ")\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "# set as vocab_size in config\n",
    "config[\"vocab_size\"] = len(word2idx)\n",
    "\n",
    "\n",
    "def _convert_captions_to_sequences(\n",
    "    image_captions, word2idx, special_token_mapping=None, tokenizing_fn=tokenize\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts captions to sequences of word indices.\n",
    "    Args:\n",
    "        image_captions (dict): Mapping from image filenames to their captions.\n",
    "        word2idx (dict): Mapping from words to indices.\n",
    "    Returns:\n",
    "        captions_seqs (dict): Mapping from image filenames to sequences of word indices.\n",
    "        max_length (int): Maximum length of the captions.\n",
    "    \"\"\"\n",
    "    captions_seqs = {}\n",
    "    max_length = 0\n",
    "\n",
    "    if not special_token_mapping:\n",
    "        special_token_mapping = {\n",
    "            \"start\": \"<start>\",\n",
    "            \"end\": \"<end>\",\n",
    "            \"pad\": \"<pad>\",\n",
    "            \"unk\": \"<unk>\",\n",
    "        }\n",
    "\n",
    "    for img_name, captions in image_captions.items():\n",
    "        seqs = []\n",
    "        for caption in captions:\n",
    "            # Tokenize and add start and end tokens\n",
    "            tokens = (\n",
    "                [special_token_mapping[\"start\"]]\n",
    "                + tokenizing_fn(caption)\n",
    "                + [special_token_mapping[\"end\"]]\n",
    "            )\n",
    "            # Convert tokens to indices based on gpt2 tokenizer\n",
    "            seq = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            seqs.append(seq)\n",
    "            # Update maximum caption length\n",
    "            max_length = max(max_length, len(seq))\n",
    "        captions_seqs[img_name] = seqs\n",
    "\n",
    "    return captions_seqs, max_length\n",
    "\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = _convert_captions_to_sequences(\n",
    "    image_captions,\n",
    "    word2idx,\n",
    "    special_token_mapping=special_token_mapping,\n",
    "    tokenizing_fn=lambda x: tokenizer.tokenize(x, add_special_tokens=False),\n",
    ")\n",
    "captions_seqs_nltk, _ = convert_captions_to_sequences(\n",
    "    image_captions_nltk,\n",
    "    word2idx_nltk,\n",
    "    tokenizing_fn=word_tokenize,\n",
    ")\n",
    "print(f\"Maximum caption length: {max_length}\")\n",
    "# set as max_length in config\n",
    "config[\"max_length\"] = max_length\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "      Summary of special tokens replaced:\n",
    "      <start>: {tokenizer.bos_token}\n",
    "      <end>: {tokenizer.eos_token}\n",
    "      <pad>: {tokenizer.pad_token}\n",
    "      <unk>: {tokenizer.unk_token}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    id of special tokens in vocab:\n",
    "    <start>: {word2idx[tokenizer.bos_token]}\n",
    "    <end>: {word2idx[tokenizer.eos_token]}\n",
    "    <pad>: {word2idx[tokenizer.pad_token]}\n",
    "    <unk>: {word2idx[tokenizer.unk_token]}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    id of special tokens in tokenizer:\n",
    "    <start>: {tokenizer.bos_token_id}\n",
    "    <end>: {tokenizer.eos_token_id}\n",
    "    <pad>: {tokenizer.pad_token_id}\n",
    "    <unk>: {tokenizer.unk_token_id}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    example sequences and captions:\n",
    "    {list(image_captions.keys())[0]}\n",
    "    {captions_seqs[list(image_captions.keys())[0]]}\n",
    "    {list(image_captions.values())[0]}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:29:00.584691Z",
     "iopub.status.busy": "2024-12-06T14:29:00.584494Z",
     "iopub.status.idle": "2024-12-06T14:29:00.697992Z",
     "shell.execute_reply": "2024-12-06T14:29:00.697116Z",
     "shell.execute_reply.started": "2024-12-06T14:29:00.584642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit mean: [0.5, 0.5, 0.5], vit std: [0.5, 0.5, 0.5]\n",
      "Training samples: 6472\n",
      "Validation samples: 1457\n"
     ]
    }
   ],
   "source": [
    "# Get data transformations - slightly modified for ViT\n",
    "processor = AutoImageProcessor.from_pretrained(config[\"encoder_name\"])\n",
    "# we'll keep processor for inference\n",
    "print(f\"vit mean: {processor.image_mean}, vit std: {processor.image_std}\")\n",
    "\n",
    "\n",
    "def get_transform(train=True)\n",
    "    \"\"\"\n",
    "    Returns the image transformations for training or evaluation.\n",
    "    Args:\n",
    "        train (bool): Flag indicating whether transformations are for training or evaluation.\n",
    "    Returns:\n",
    "        transform (callable): Composed transformations.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=processor.image_mean, std=processor.image_std\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=processor.image_mean, std=processor.image_std\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    return transform\n",
    "\n",
    "\n",
    "train_transform = get_transform(train=True)\n",
    "val_transform = get_transform(train=False)\n",
    "# Split data into training and validation sets\n",
    "image_names = list(image_captions.keys())\n",
    "train_images, val_images, _ = get_splits(image_names, test_size=0.2)\n",
    "print(f\"Training samples: {len(train_images)}\")\n",
    "print(f\"Validation samples: {len(val_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:29:00.699859Z",
     "iopub.status.busy": "2024-12-06T14:29:00.699478Z",
     "iopub.status.idle": "2024-12-06T14:29:00.766419Z",
     "shell.execute_reply": "2024-12-06T14:29:00.765818Z",
     "shell.execute_reply.started": "2024-12-06T14:29:00.699841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 1011\n",
      "Number of validation batches: 228\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# modify the collate_fn\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length captions.\n",
    "    This function pads captions to the length of the longest caption in the batch.\n",
    "    Args:\n",
    "        batch (list): List of tuples (image, caption_seq) or (image, caption_seq, image_id)\n",
    "    Returns:\n",
    "        If training:\n",
    "            images (Tensor): Batch of images.\n",
    "            targets (Tensor): Padded caption sequences.\n",
    "            lengths (list): Original lengths of each caption before padding.\n",
    "        If testing:\n",
    "            images (Tensor): Batch of images.\n",
    "            targets (Tensor): Padded caption sequences.\n",
    "            image_ids (list): List of image filenames.\n",
    "    \"\"\"\n",
    "    if len(batch[0]) == 3:\n",
    "        # Test mode\n",
    "        images, captions, image_ids = zip(*batch)\n",
    "    else:\n",
    "        # Train mode\n",
    "        images, captions = zip(*batch)\n",
    "        image_ids = None\n",
    "\n",
    "    # Stack images\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Get lengths of each caption\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "\n",
    "    # Pad captions to the length of the longest caption\n",
    "    max_length = max(lengths)\n",
    "    # targets = torch.zeros(len(captions), max_length).long()\n",
    "    targets = torch.full((len(captions), max_length), tokenizer.pad_token_id).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end] # pad from the left\n",
    "\n",
    "    if image_ids is not None:\n",
    "        return images, targets, image_ids\n",
    "    else:\n",
    "        return images, targets, lengths\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = FlickrDataset(\n",
    "    config[\"image_dir\"],\n",
    "    train_images,\n",
    "    captions_seqs,\n",
    "    transform=train_transform,\n",
    ")\n",
    "val_dataset = FlickrDataset(\n",
    "    config[\"image_dir\"], val_images, captions_seqs, transform=val_transform\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    # pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    # pin_memory=True,\n",
    ")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:29:00.822606Z",
     "iopub.status.busy": "2024-12-06T14:29:00.822371Z",
     "iopub.status.idle": "2024-12-06T14:29:09.325622Z",
     "shell.execute_reply": "2024-12-06T14:29:09.325038Z",
     "shell.execute_reply.started": "2024-12-06T14:29:00.822582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading encoder: google/vit-base-patch16-224-in21k and decoder: openai-community/gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['h.5.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.6.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.weight', 'h.5.ln_cross_attn.bias', 'h.4.ln_cross_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.5.ln_cross_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.9.ln_cross_attn.weight', 'h.2.ln_cross_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.5.crossattention.q_attn.weight', 'h.0.ln_cross_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.10.ln_cross_attn.weight', 'h.0.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.bias', 'h.2.ln_cross_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.weight', 'h.8.ln_cross_attn.bias', 'h.1.crossattention.c_attn.bias', 'h.10.ln_cross_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.7.ln_cross_attn.bias', 'h.11.crossattention.c_proj.bias', 'h.3.crossattention.q_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.1.ln_cross_attn.weight', 'h.3.ln_cross_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.4.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.11.crossattention.q_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.2.crossattention.q_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.bias', 'h.9.ln_cross_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.4.ln_cross_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.6.ln_cross_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.11.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.1.ln_cross_attn.bias', 'h.4.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.5.crossattention.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50258, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (q_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"loading encoder: {encoder_name} and decoder: {decoder_name}...\")\n",
    "# Load the model\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder_name,\n",
    "    decoder_name,\n",
    ")\n",
    "\n",
    "# slight modification for gpt2 tokenizer\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.vocab_size = len(tokenizer) #config[\"vocab_size\"]\n",
    "model.config.beam_size = config[\"num_beams\"]\n",
    "model.config.max_length = config[\"max_length\"]\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:29:09.328275Z",
     "iopub.status.busy": "2024-12-06T14:29:09.328077Z",
     "iopub.status.idle": "2024-12-06T14:29:10.863391Z",
     "shell.execute_reply": "2024-12-06T14:29:10.862732Z",
     "shell.execute_reply.started": "2024-12-06T14:29:09.328257Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivankohquanwei\u001b[0m (\u001b[33mivan-koh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/models/model_vision_transformer/wandb/run-20241206_142910-q8mrwc50</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ivan-koh/image-captioning/runs/q8mrwc50' target=\"_blank\">cool-night-19</a></strong> to <a href='https://wandb.ai/ivan-koh/image-captioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ivan-koh/image-captioning' target=\"_blank\">https://wandb.ai/ivan-koh/image-captioning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ivan-koh/image-captioning/runs/q8mrwc50' target=\"_blank\">https://wandb.ai/ivan-koh/image-captioning/runs/q8mrwc50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f7b515db890>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f7b525d8490, execution_count=8 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f7b523680d0, raw_cell=\"wandb.finish()\n",
      "# wandb login\n",
      "# wandb.login()\n",
      "wandb..\" store_history=True silent=False shell_futures=True cell_id=49237fd9-2ed0-47a6-b03a-2c63299e0f68> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "# wandb login\n",
    "# wandb.login()\n",
    "wandb.init(project=\"image-captioning\")\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:29:10.938103Z",
     "iopub.status.busy": "2024-12-06T14:29:10.937938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f7b515db890>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f7b5b0b2d10, raw_cell=\"# % load_ext autoreload   \n",
      "# % autoreload 2\n",
      "# Init..\" store_history=True silent=False shell_futures=True cell_id=ce5dbd41-f2e7-4a21-8759-9ee40d742e69>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1011/1011 [12:27<00:00,  1.35it/s, loss=2.28]\n",
      "Validation: 100%|██████████| 228/228 [00:57<00:00,  3.94it/s, val_loss=2.3] \n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Eval Metrics..:   0%|          | 0/1457 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Eval Metrics..: 100%|██████████| 1457/1457 [04:40<00:00,  5.19it/s]\n",
      "PTBTokenizer tokenized 92805 tokens at 641642.78 tokens per second.\n",
      "PTBTokenizer tokenized 18896 tokens at 204956.81 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "Prediction: \n",
      "A man in a blue shirt is standing in front of a large building.\n",
      "References: [['a', 'man', 'inside', 'of', 'a', 'white', 'subway', 'train'], ['a', 'man', 'sits', 'by', 'a', 'window', 'on', 'a', 'train', '.'], ['a', 'man', 'sits', 'by', 'the', 'window', 'in', 'a', 'train', '.'], ['a', 'man', 'sits', 'in', 'the', 'window', 'of', 'a', 'train', '.'], ['the', 'side', 'of', 'a', 'subway', 'cart', ',', 'with', 'one', 'man', 'in', 'the', 'window', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: ​A man in a blue shirt is driving a red car.\n",
      "References: [['a', 'man', 'rides', 'a', 'motorcycle', 'on', 'a', 'track', '.'], ['a', 'motorcycle', 'racer', 'is', 'producing', 'sparks', 'by', 'leaning', 'during', 'a', 'turn', '.'], ['a', 'motorcyclist', 'is', 'turning', 'a', 'sharp', 'corner', 'on', 'his', 'red', 'motorbike', 'and', 'is', 'scraping', 'it', 'on', 'the', 'road', '.'], ['a', 'motorcyclist', 'making', 'a', 'sharp', 'turn', '.'], ['the', 'motorcycle', 'rider', 'makes', 'sparks', 'as', 'he', 'leans', 'in', 'for', 'a', 'turn', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: \n",
      "A group of people are standing in a line in front of a large building.\n",
      "References: [['a', 'group', 'of', '6', 'boys', 'are', 'wearing', 'yellow', 'life', 'vests', 'and', 'are', 'on', 'a', 'make-shift', 'raft', '.'], ['a', 'group', 'of', 'people', 'on', 'a', 'boat', '.'], ['boys', 'in', 'life', 'jackets', 'on', 'a', 'watercraft', '.'], ['several', 'young', 'people', 'in', 'life', 'jackets', 'are', 'sitting', 'on', 'something', 'floating', 'in', 'water', '.'], ['the', 'boys', 'in', 'life', 'jackets', 'smile', 'and', 'laugh', 'while', 'riding', 'in', 'a', 'boat', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: ​A man is standing on a rocky shore with a dog.\n",
      "References: [['a', 'man', 'and', 'a', 'dog', 'stand', 'on', 'a', 'hill', 'overlooking', 'water', '.'], ['a', 'man', 'walks', 'his', 'dog', 'near', 'the', 'ocean', '.'], ['an', 'elderly', 'man', 'is', 'walking', 'a', 'brown', 'dog', 'using', 'a', 'red', 'leash', '.'], ['an', 'elderly', 'man', 'takes', 'his', 'dog', 'for', 'a', 'walk', 'near', 'the', 'water', '.'], ['an', 'old', 'man', 'holding', 'a', 'camera', 'while', 'walking', 'a', 'small', 'brown', 'dog', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: ​A boy in a red shirt is running on a grassy field.\n",
      "References: [['a', 'soccer', 'player', 'in', 'blue', 'is', 'chasing', 'after', 'the', 'player', 'in', 'black', 'and', 'white', '.'], ['the', 'girl', 'in', 'the', 'white', 'strip', 'is', 'falling', 'down', 'as', 'the', 'girl', 'in', 'the', 'blue', 'strip', 'challenges', 'for', 'the', 'soccer', 'ball', '.'], ['the', 'girls', 'are', 'playing', 'soccer', '.'], ['two', 'women', 'in', 'soccer', 'uniforms', 'playing', 'soccer', '.'], ['two', 'young', 'women', 'on', 'different', 'teams', 'are', 'playing', 'soccer', 'on', 'a', 'field', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Epoch [1/10] completed in 1092.42s\n",
      "Training Loss: 2.7163, Validation Loss: 2.2385\n",
      "BLEU Score: 0.1031, METEOR Score: 0.3698, CIDEr Score: 0.3832\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1011/1011 [12:29<00:00,  1.35it/s, loss=2.19]\n",
      "Validation: 100%|██████████| 228/228 [00:57<00:00,  3.94it/s, val_loss=2.34]\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Eval Metrics..: 100%|██████████| 1457/1457 [04:44<00:00,  5.11it/s]\n",
      "PTBTokenizer tokenized 92805 tokens at 615126.33 tokens per second.\n",
      "PTBTokenizer tokenized 19464 tokens at 193644.87 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "Prediction: \n",
      "\n",
      "A man in a black shirt is standing in front of a large white wall.\n",
      "References: [['a', 'man', 'inside', 'of', 'a', 'white', 'subway', 'train'], ['a', 'man', 'sits', 'by', 'a', 'window', 'on', 'a', 'train', '.'], ['a', 'man', 'sits', 'by', 'the', 'window', 'in', 'a', 'train', '.'], ['a', 'man', 'sits', 'in', 'the', 'window', 'of', 'a', 'train', '.'], ['the', 'side', 'of', 'a', 'subway', 'cart', ',', 'with', 'one', 'man', 'in', 'the', 'window', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: \n",
      "\n",
      "A man is driving a motorcycle on a dirt track.\n",
      "References: [['a', 'man', 'rides', 'a', 'motorcycle', 'on', 'a', 'track', '.'], ['a', 'motorcycle', 'racer', 'is', 'producing', 'sparks', 'by', 'leaning', 'during', 'a', 'turn', '.'], ['a', 'motorcyclist', 'is', 'turning', 'a', 'sharp', 'corner', 'on', 'his', 'red', 'motorbike', 'and', 'is', 'scraping', 'it', 'on', 'the', 'road', '.'], ['a', 'motorcyclist', 'making', 'a', 'sharp', 'turn', '.'], ['the', 'motorcycle', 'rider', 'makes', 'sparks', 'as', 'he', 'leans', 'in', 'for', 'a', 'turn', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: \n",
      "\n",
      "A group of men are playing soccer in a field.\n",
      "References: [['a', 'group', 'of', '6', 'boys', 'are', 'wearing', 'yellow', 'life', 'vests', 'and', 'are', 'on', 'a', 'make-shift', 'raft', '.'], ['a', 'group', 'of', 'people', 'on', 'a', 'boat', '.'], ['boys', 'in', 'life', 'jackets', 'on', 'a', 'watercraft', '.'], ['several', 'young', 'people', 'in', 'life', 'jackets', 'are', 'sitting', 'on', 'something', 'floating', 'in', 'water', '.'], ['the', 'boys', 'in', 'life', 'jackets', 'smile', 'and', 'laugh', 'while', 'riding', 'in', 'a', 'boat', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: \n",
      "\n",
      "A man is standing on a rock overlooking a lake.\n",
      "References: [['a', 'man', 'and', 'a', 'dog', 'stand', 'on', 'a', 'hill', 'overlooking', 'water', '.'], ['a', 'man', 'walks', 'his', 'dog', 'near', 'the', 'ocean', '.'], ['an', 'elderly', 'man', 'is', 'walking', 'a', 'brown', 'dog', 'using', 'a', 'red', 'leash', '.'], ['an', 'elderly', 'man', 'takes', 'his', 'dog', 'for', 'a', 'walk', 'near', 'the', 'water', '.'], ['an', 'old', 'man', 'holding', 'a', 'camera', 'while', 'walking', 'a', 'small', 'brown', 'dog', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: \n",
      "\n",
      "Two soccer players are playing soccer..\n",
      "References: [['a', 'soccer', 'player', 'in', 'blue', 'is', 'chasing', 'after', 'the', 'player', 'in', 'black', 'and', 'white', '.'], ['the', 'girl', 'in', 'the', 'white', 'strip', 'is', 'falling', 'down', 'as', 'the', 'girl', 'in', 'the', 'blue', 'strip', 'challenges', 'for', 'the', 'soccer', 'ball', '.'], ['the', 'girls', 'are', 'playing', 'soccer', '.'], ['two', 'women', 'in', 'soccer', 'uniforms', 'playing', 'soccer', '.'], ['two', 'young', 'women', 'on', 'different', 'teams', 'are', 'playing', 'soccer', 'on', 'a', 'field', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Epoch [2/10] completed in 1096.68s\n",
      "Training Loss: 2.1602, Validation Loss: 2.2510\n",
      "BLEU Score: 0.1117, METEOR Score: 0.4104, CIDEr Score: 0.4084\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1011/1011 [12:28<00:00,  1.35it/s, loss=1.87]\n",
      "Validation: 100%|██████████| 228/228 [00:58<00:00,  3.93it/s, val_loss=2.41]\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Eval Metrics..: 100%|██████████| 1457/1457 [04:53<00:00,  4.97it/s]\n",
      "PTBTokenizer tokenized 92805 tokens at 667711.21 tokens per second.\n",
      "PTBTokenizer tokenized 21812 tokens at 209572.61 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "Prediction: , a man in a black shirt is standing in front of a building with a sign.\n",
      "References: [['a', 'man', 'inside', 'of', 'a', 'white', 'subway', 'train'], ['a', 'man', 'sits', 'by', 'a', 'window', 'on', 'a', 'train', '.'], ['a', 'man', 'sits', 'by', 'the', 'window', 'in', 'a', 'train', '.'], ['a', 'man', 'sits', 'in', 'the', 'window', 'of', 'a', 'train', '.'], ['the', 'side', 'of', 'a', 'subway', 'cart', ',', 'with', 'one', 'man', 'in', 'the', 'window', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a man in a red shirt and a black jacket is riding a motorcycle.\n",
      "References: [['a', 'man', 'rides', 'a', 'motorcycle', 'on', 'a', 'track', '.'], ['a', 'motorcycle', 'racer', 'is', 'producing', 'sparks', 'by', 'leaning', 'during', 'a', 'turn', '.'], ['a', 'motorcyclist', 'is', 'turning', 'a', 'sharp', 'corner', 'on', 'his', 'red', 'motorbike', 'and', 'is', 'scraping', 'it', 'on', 'the', 'road', '.'], ['a', 'motorcyclist', 'making', 'a', 'sharp', 'turn', '.'], ['the', 'motorcycle', 'rider', 'makes', 'sparks', 'as', 'he', 'leans', 'in', 'for', 'a', 'turn', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a group of people in a tent, are sitting in a tent.\n",
      "References: [['a', 'group', 'of', '6', 'boys', 'are', 'wearing', 'yellow', 'life', 'vests', 'and', 'are', 'on', 'a', 'make-shift', 'raft', '.'], ['a', 'group', 'of', 'people', 'on', 'a', 'boat', '.'], ['boys', 'in', 'life', 'jackets', 'on', 'a', 'watercraft', '.'], ['several', 'young', 'people', 'in', 'life', 'jackets', 'are', 'sitting', 'on', 'something', 'floating', 'in', 'water', '.'], ['the', 'boys', 'in', 'life', 'jackets', 'smile', 'and', 'laugh', 'while', 'riding', 'in', 'a', 'boat', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a man and a dog are standing in front of a tree.\n",
      "References: [['a', 'man', 'and', 'a', 'dog', 'stand', 'on', 'a', 'hill', 'overlooking', 'water', '.'], ['a', 'man', 'walks', 'his', 'dog', 'near', 'the', 'ocean', '.'], ['an', 'elderly', 'man', 'is', 'walking', 'a', 'brown', 'dog', 'using', 'a', 'red', 'leash', '.'], ['an', 'elderly', 'man', 'takes', 'his', 'dog', 'for', 'a', 'walk', 'near', 'the', 'water', '.'], ['an', 'old', 'man', 'holding', 'a', 'camera', 'while', 'walking', 'a', 'small', 'brown', 'dog', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , two boys in a soccer game. one is holding a soccer ball.\n",
      "References: [['a', 'soccer', 'player', 'in', 'blue', 'is', 'chasing', 'after', 'the', 'player', 'in', 'black', 'and', 'white', '.'], ['the', 'girl', 'in', 'the', 'white', 'strip', 'is', 'falling', 'down', 'as', 'the', 'girl', 'in', 'the', 'blue', 'strip', 'challenges', 'for', 'the', 'soccer', 'ball', '.'], ['the', 'girls', 'are', 'playing', 'soccer', '.'], ['two', 'women', 'in', 'soccer', 'uniforms', 'playing', 'soccer', '.'], ['two', 'young', 'women', 'on', 'different', 'teams', 'are', 'playing', 'soccer', 'on', 'a', 'field', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Epoch [3/10] completed in 1105.51s\n",
      "Training Loss: 2.0814, Validation Loss: 2.2949\n",
      "BLEU Score: 0.0970, METEOR Score: 0.3471, CIDEr Score: 0.2822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1011/1011 [12:28<00:00,  1.35it/s, loss=1.95]\n",
      "Validation: 100%|██████████| 228/228 [00:57<00:00,  3.93it/s, val_loss=2.44]\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Eval Metrics..: 100%|██████████| 1457/1457 [04:55<00:00,  4.93it/s]\n",
      "PTBTokenizer tokenized 92805 tokens at 600194.29 tokens per second.\n",
      "PTBTokenizer tokenized 22237 tokens at 187054.32 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "Prediction: , a man in a black shirt and jeans is standing on a sidewalk.\n",
      "References: [['a', 'man', 'inside', 'of', 'a', 'white', 'subway', 'train'], ['a', 'man', 'sits', 'by', 'a', 'window', 'on', 'a', 'train', '.'], ['a', 'man', 'sits', 'by', 'the', 'window', 'in', 'a', 'train', '.'], ['a', 'man', 'sits', 'in', 'the', 'window', 'of', 'a', 'train', '.'], ['the', 'side', 'of', 'a', 'subway', 'cart', ',', 'with', 'one', 'man', 'in', 'the', 'window', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , two children, and a dog are riding on a red and white vehicle.\n",
      "References: [['a', 'man', 'rides', 'a', 'motorcycle', 'on', 'a', 'track', '.'], ['a', 'motorcycle', 'racer', 'is', 'producing', 'sparks', 'by', 'leaning', 'during', 'a', 'turn', '.'], ['a', 'motorcyclist', 'is', 'turning', 'a', 'sharp', 'corner', 'on', 'his', 'red', 'motorbike', 'and', 'is', 'scraping', 'it', 'on', 'the', 'road', '.'], ['a', 'motorcyclist', 'making', 'a', 'sharp', 'turn', '.'], ['the', 'motorcycle', 'rider', 'makes', 'sparks', 'as', 'he', 'leans', 'in', 'for', 'a', 'turn', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , two children are playing in a water fountain.\n",
      "References: [['a', 'group', 'of', '6', 'boys', 'are', 'wearing', 'yellow', 'life', 'vests', 'and', 'are', 'on', 'a', 'make-shift', 'raft', '.'], ['a', 'group', 'of', 'people', 'on', 'a', 'boat', '.'], ['boys', 'in', 'life', 'jackets', 'on', 'a', 'watercraft', '.'], ['several', 'young', 'people', 'in', 'life', 'jackets', 'are', 'sitting', 'on', 'something', 'floating', 'in', 'water', '.'], ['the', 'boys', 'in', 'life', 'jackets', 'smile', 'and', 'laugh', 'while', 'riding', 'in', 'a', 'boat', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: The man is holding a stick and pointing at the camera.\n",
      "References: [['a', 'man', 'and', 'a', 'dog', 'stand', 'on', 'a', 'hill', 'overlooking', 'water', '.'], ['a', 'man', 'walks', 'his', 'dog', 'near', 'the', 'ocean', '.'], ['an', 'elderly', 'man', 'is', 'walking', 'a', 'brown', 'dog', 'using', 'a', 'red', 'leash', '.'], ['an', 'elderly', 'man', 'takes', 'his', 'dog', 'for', 'a', 'walk', 'near', 'the', 'water', '.'], ['an', 'old', 'man', 'holding', 'a', 'camera', 'while', 'walking', 'a', 'small', 'brown', 'dog', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , two boys in uniforms, run down a grassy field.\n",
      "References: [['a', 'soccer', 'player', 'in', 'blue', 'is', 'chasing', 'after', 'the', 'player', 'in', 'black', 'and', 'white', '.'], ['the', 'girl', 'in', 'the', 'white', 'strip', 'is', 'falling', 'down', 'as', 'the', 'girl', 'in', 'the', 'blue', 'strip', 'challenges', 'for', 'the', 'soccer', 'ball', '.'], ['the', 'girls', 'are', 'playing', 'soccer', '.'], ['two', 'women', 'in', 'soccer', 'uniforms', 'playing', 'soccer', '.'], ['two', 'young', 'women', 'on', 'different', 'teams', 'are', 'playing', 'soccer', 'on', 'a', 'field', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 00004: reducing learning rate of group 0 to 2.5000e-05.\n",
      "\n",
      "Epoch [4/10] completed in 1107.68s\n",
      "Training Loss: 2.0480, Validation Loss: 2.3778\n",
      "BLEU Score: 0.0942, METEOR Score: 0.3448, CIDEr Score: 0.2930\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1011/1011 [12:27<00:00,  1.35it/s, loss=1.72]\n",
      "Validation: 100%|██████████| 228/228 [00:57<00:00,  3.96it/s, val_loss=2.42]\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Eval Metrics..: 100%|██████████| 1457/1457 [04:33<00:00,  5.33it/s]\n",
      "PTBTokenizer tokenized 92805 tokens at 635296.56 tokens per second.\n",
      "PTBTokenizer tokenized 20222 tokens at 195348.22 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "Prediction: , a man in a black shirt is standing on a ledge overlooking a city.\n",
      "References: [['a', 'man', 'inside', 'of', 'a', 'white', 'subway', 'train'], ['a', 'man', 'sits', 'by', 'a', 'window', 'on', 'a', 'train', '.'], ['a', 'man', 'sits', 'by', 'the', 'window', 'in', 'a', 'train', '.'], ['a', 'man', 'sits', 'in', 'the', 'window', 'of', 'a', 'train', '.'], ['the', 'side', 'of', 'a', 'subway', 'cart', ',', 'with', 'one', 'man', 'in', 'the', 'window', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a group of people riding a motorbike.\n",
      "References: [['a', 'man', 'rides', 'a', 'motorcycle', 'on', 'a', 'track', '.'], ['a', 'motorcycle', 'racer', 'is', 'producing', 'sparks', 'by', 'leaning', 'during', 'a', 'turn', '.'], ['a', 'motorcyclist', 'is', 'turning', 'a', 'sharp', 'corner', 'on', 'his', 'red', 'motorbike', 'and', 'is', 'scraping', 'it', 'on', 'the', 'road', '.'], ['a', 'motorcyclist', 'making', 'a', 'sharp', 'turn', '.'], ['the', 'motorcycle', 'rider', 'makes', 'sparks', 'as', 'he', 'leans', 'in', 'for', 'a', 'turn', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a group of people are playing soccer.\n",
      "References: [['a', 'group', 'of', '6', 'boys', 'are', 'wearing', 'yellow', 'life', 'vests', 'and', 'are', 'on', 'a', 'make-shift', 'raft', '.'], ['a', 'group', 'of', 'people', 'on', 'a', 'boat', '.'], ['boys', 'in', 'life', 'jackets', 'on', 'a', 'watercraft', '.'], ['several', 'young', 'people', 'in', 'life', 'jackets', 'are', 'sitting', 'on', 'something', 'floating', 'in', 'water', '.'], ['the', 'boys', 'in', 'life', 'jackets', 'smile', 'and', 'laugh', 'while', 'riding', 'in', 'a', 'boat', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , two dogs, are running towards the camera.\n",
      "References: [['a', 'man', 'and', 'a', 'dog', 'stand', 'on', 'a', 'hill', 'overlooking', 'water', '.'], ['a', 'man', 'walks', 'his', 'dog', 'near', 'the', 'ocean', '.'], ['an', 'elderly', 'man', 'is', 'walking', 'a', 'brown', 'dog', 'using', 'a', 'red', 'leash', '.'], ['an', 'elderly', 'man', 'takes', 'his', 'dog', 'for', 'a', 'walk', 'near', 'the', 'water', '.'], ['an', 'old', 'man', 'holding', 'a', 'camera', 'while', 'walking', 'a', 'small', 'brown', 'dog', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a boy in a blue shirt runs with a soccer ball.\n",
      "References: [['a', 'soccer', 'player', 'in', 'blue', 'is', 'chasing', 'after', 'the', 'player', 'in', 'black', 'and', 'white', '.'], ['the', 'girl', 'in', 'the', 'white', 'strip', 'is', 'falling', 'down', 'as', 'the', 'girl', 'in', 'the', 'blue', 'strip', 'challenges', 'for', 'the', 'soccer', 'ball', '.'], ['the', 'girls', 'are', 'playing', 'soccer', '.'], ['two', 'women', 'in', 'soccer', 'uniforms', 'playing', 'soccer', '.'], ['two', 'young', 'women', 'on', 'different', 'teams', 'are', 'playing', 'soccer', 'on', 'a', 'field', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Epoch [5/10] completed in 1084.01s\n",
      "Training Loss: 1.9150, Validation Loss: 2.3892\n",
      "BLEU Score: 0.1103, METEOR Score: 0.3794, CIDEr Score: 0.3671\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1011/1011 [12:25<00:00,  1.36it/s, loss=1.89]\n",
      "Validation: 100%|██████████| 228/228 [00:57<00:00,  3.99it/s, val_loss=2.41]\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Eval Metrics..: 100%|██████████| 1457/1457 [04:43<00:00,  5.14it/s]\n",
      "PTBTokenizer tokenized 92805 tokens at 631356.66 tokens per second.\n",
      "PTBTokenizer tokenized 20438 tokens at 199807.99 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "Prediction: , a woman in a black shirt and a man in a white shirt are standing in front of a brick building.\n",
      "References: [['a', 'man', 'inside', 'of', 'a', 'white', 'subway', 'train'], ['a', 'man', 'sits', 'by', 'a', 'window', 'on', 'a', 'train', '.'], ['a', 'man', 'sits', 'by', 'the', 'window', 'in', 'a', 'train', '.'], ['a', 'man', 'sits', 'in', 'the', 'window', 'of', 'a', 'train', '.'], ['the', 'side', 'of', 'a', 'subway', 'cart', ',', 'with', 'one', 'man', 'in', 'the', 'window', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a group of people are riding bicycles on a track.\n",
      "References: [['a', 'man', 'rides', 'a', 'motorcycle', 'on', 'a', 'track', '.'], ['a', 'motorcycle', 'racer', 'is', 'producing', 'sparks', 'by', 'leaning', 'during', 'a', 'turn', '.'], ['a', 'motorcyclist', 'is', 'turning', 'a', 'sharp', 'corner', 'on', 'his', 'red', 'motorbike', 'and', 'is', 'scraping', 'it', 'on', 'the', 'road', '.'], ['a', 'motorcyclist', 'making', 'a', 'sharp', 'turn', '.'], ['the', 'motorcycle', 'rider', 'makes', 'sparks', 'as', 'he', 'leans', 'in', 'for', 'a', 'turn', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a group of dogs, are playing in a fenced in area.\n",
      "References: [['a', 'group', 'of', '6', 'boys', 'are', 'wearing', 'yellow', 'life', 'vests', 'and', 'are', 'on', 'a', 'make-shift', 'raft', '.'], ['a', 'group', 'of', 'people', 'on', 'a', 'boat', '.'], ['boys', 'in', 'life', 'jackets', 'on', 'a', 'watercraft', '.'], ['several', 'young', 'people', 'in', 'life', 'jackets', 'are', 'sitting', 'on', 'something', 'floating', 'in', 'water', '.'], ['the', 'boys', 'in', 'life', 'jackets', 'smile', 'and', 'laugh', 'while', 'riding', 'in', 'a', 'boat', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , two boys and a girl are playing on the beach.\n",
      "References: [['a', 'man', 'and', 'a', 'dog', 'stand', 'on', 'a', 'hill', 'overlooking', 'water', '.'], ['a', 'man', 'walks', 'his', 'dog', 'near', 'the', 'ocean', '.'], ['an', 'elderly', 'man', 'is', 'walking', 'a', 'brown', 'dog', 'using', 'a', 'red', 'leash', '.'], ['an', 'elderly', 'man', 'takes', 'his', 'dog', 'for', 'a', 'walk', 'near', 'the', 'water', '.'], ['an', 'old', 'man', 'holding', 'a', 'camera', 'while', 'walking', 'a', 'small', 'brown', 'dog', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a boy in a blue shirt is running with a soccer ball.\n",
      "References: [['a', 'soccer', 'player', 'in', 'blue', 'is', 'chasing', 'after', 'the', 'player', 'in', 'black', 'and', 'white', '.'], ['the', 'girl', 'in', 'the', 'white', 'strip', 'is', 'falling', 'down', 'as', 'the', 'girl', 'in', 'the', 'blue', 'strip', 'challenges', 'for', 'the', 'soccer', 'ball', '.'], ['the', 'girls', 'are', 'playing', 'soccer', '.'], ['two', 'women', 'in', 'soccer', 'uniforms', 'playing', 'soccer', '.'], ['two', 'young', 'women', 'on', 'different', 'teams', 'are', 'playing', 'soccer', 'on', 'a', 'field', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Epoch [6/10] completed in 1090.74s\n",
      "Training Loss: 1.8832, Validation Loss: 2.4189\n",
      "BLEU Score: 0.0939, METEOR Score: 0.3597, CIDEr Score: 0.3072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1011/1011 [12:26<00:00,  1.35it/s, loss=2.01]\n",
      "Validation: 100%|██████████| 228/228 [00:57<00:00,  3.97it/s, val_loss=2.6] \n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Eval Metrics..: 100%|██████████| 1457/1457 [04:38<00:00,  5.24it/s]\n",
      "PTBTokenizer tokenized 92805 tokens at 537089.98 tokens per second.\n",
      "PTBTokenizer tokenized 19654 tokens at 174518.30 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "Prediction: A man in a red shirt is standing on a sidewalk with a sign advertising a free soda.\n",
      "References: [['a', 'man', 'inside', 'of', 'a', 'white', 'subway', 'train'], ['a', 'man', 'sits', 'by', 'a', 'window', 'on', 'a', 'train', '.'], ['a', 'man', 'sits', 'by', 'the', 'window', 'in', 'a', 'train', '.'], ['a', 'man', 'sits', 'in', 'the', 'window', 'of', 'a', 'train', '.'], ['the', 'side', 'of', 'a', 'subway', 'cart', ',', 'with', 'one', 'man', 'in', 'the', 'window', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: The man is riding a motorcycle on a dirt track.\n",
      "References: [['a', 'man', 'rides', 'a', 'motorcycle', 'on', 'a', 'track', '.'], ['a', 'motorcycle', 'racer', 'is', 'producing', 'sparks', 'by', 'leaning', 'during', 'a', 'turn', '.'], ['a', 'motorcyclist', 'is', 'turning', 'a', 'sharp', 'corner', 'on', 'his', 'red', 'motorbike', 'and', 'is', 'scraping', 'it', 'on', 'the', 'road', '.'], ['a', 'motorcyclist', 'making', 'a', 'sharp', 'turn', '.'], ['the', 'motorcycle', 'rider', 'makes', 'sparks', 'as', 'he', 'leans', 'in', 'for', 'a', 'turn', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , two dogs, are playing in a fenced in area.\n",
      "References: [['a', 'group', 'of', '6', 'boys', 'are', 'wearing', 'yellow', 'life', 'vests', 'and', 'are', 'on', 'a', 'make-shift', 'raft', '.'], ['a', 'group', 'of', 'people', 'on', 'a', 'boat', '.'], ['boys', 'in', 'life', 'jackets', 'on', 'a', 'watercraft', '.'], ['several', 'young', 'people', 'in', 'life', 'jackets', 'are', 'sitting', 'on', 'something', 'floating', 'in', 'water', '.'], ['the', 'boys', 'in', 'life', 'jackets', 'smile', 'and', 'laugh', 'while', 'riding', 'in', 'a', 'boat', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: A woman and a dog are walking on a beach.\n",
      "References: [['a', 'man', 'and', 'a', 'dog', 'stand', 'on', 'a', 'hill', 'overlooking', 'water', '.'], ['a', 'man', 'walks', 'his', 'dog', 'near', 'the', 'ocean', '.'], ['an', 'elderly', 'man', 'is', 'walking', 'a', 'brown', 'dog', 'using', 'a', 'red', 'leash', '.'], ['an', 'elderly', 'man', 'takes', 'his', 'dog', 'for', 'a', 'walk', 'near', 'the', 'water', '.'], ['an', 'old', 'man', 'holding', 'a', 'camera', 'while', 'walking', 'a', 'small', 'brown', 'dog', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: The little boy is running through the grass with a soccer ball.\n",
      "References: [['a', 'soccer', 'player', 'in', 'blue', 'is', 'chasing', 'after', 'the', 'player', 'in', 'black', 'and', 'white', '.'], ['the', 'girl', 'in', 'the', 'white', 'strip', 'is', 'falling', 'down', 'as', 'the', 'girl', 'in', 'the', 'blue', 'strip', 'challenges', 'for', 'the', 'soccer', 'ball', '.'], ['the', 'girls', 'are', 'playing', 'soccer', '.'], ['two', 'women', 'in', 'soccer', 'uniforms', 'playing', 'soccer', '.'], ['two', 'young', 'women', 'on', 'different', 'teams', 'are', 'playing', 'soccer', 'on', 'a', 'field', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.2500e-05.\n",
      "\n",
      "Epoch [7/10] completed in 1087.45s\n",
      "Training Loss: 1.8669, Validation Loss: 2.4596\n",
      "BLEU Score: 0.0890, METEOR Score: 0.3506, CIDEr Score: 0.3331\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1011/1011 [12:25<00:00,  1.36it/s, loss=1.92]\n",
      "Validation: 100%|██████████| 228/228 [00:57<00:00,  3.98it/s, val_loss=2.49]\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Eval Metrics..: 100%|██████████| 1457/1457 [04:59<00:00,  4.86it/s]\n",
      "PTBTokenizer tokenized 92805 tokens at 556553.45 tokens per second.\n",
      "PTBTokenizer tokenized 20093 tokens at 147596.47 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "Prediction: , a woman is standing in front of a window that says \" HELL \".\n",
      "References: [['a', 'man', 'inside', 'of', 'a', 'white', 'subway', 'train'], ['a', 'man', 'sits', 'by', 'a', 'window', 'on', 'a', 'train', '.'], ['a', 'man', 'sits', 'by', 'the', 'window', 'in', 'a', 'train', '.'], ['a', 'man', 'sits', 'in', 'the', 'window', 'of', 'a', 'train', '.'], ['the', 'side', 'of', 'a', 'subway', 'cart', ',', 'with', 'one', 'man', 'in', 'the', 'window', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , two black motorcycles racing around a track.\n",
      "References: [['a', 'man', 'rides', 'a', 'motorcycle', 'on', 'a', 'track', '.'], ['a', 'motorcycle', 'racer', 'is', 'producing', 'sparks', 'by', 'leaning', 'during', 'a', 'turn', '.'], ['a', 'motorcyclist', 'is', 'turning', 'a', 'sharp', 'corner', 'on', 'his', 'red', 'motorbike', 'and', 'is', 'scraping', 'it', 'on', 'the', 'road', '.'], ['a', 'motorcyclist', 'making', 'a', 'sharp', 'turn', '.'], ['the', 'motorcycle', 'rider', 'makes', 'sparks', 'as', 'he', 'leans', 'in', 'for', 'a', 'turn', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a group of people playing soccer. one boy is jumping in the air while the other boy in the red jersey is in the middle of the ball.\n",
      "References: [['a', 'group', 'of', '6', 'boys', 'are', 'wearing', 'yellow', 'life', 'vests', 'and', 'are', 'on', 'a', 'make-shift', 'raft', '.'], ['a', 'group', 'of', 'people', 'on', 'a', 'boat', '.'], ['boys', 'in', 'life', 'jackets', 'on', 'a', 'watercraft', '.'], ['several', 'young', 'people', 'in', 'life', 'jackets', 'are', 'sitting', 'on', 'something', 'floating', 'in', 'water', '.'], ['the', 'boys', 'in', 'life', 'jackets', 'smile', 'and', 'laugh', 'while', 'riding', 'in', 'a', 'boat', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: A woman and a dog are standing on a rocky beach.\n",
      "References: [['a', 'man', 'and', 'a', 'dog', 'stand', 'on', 'a', 'hill', 'overlooking', 'water', '.'], ['a', 'man', 'walks', 'his', 'dog', 'near', 'the', 'ocean', '.'], ['an', 'elderly', 'man', 'is', 'walking', 'a', 'brown', 'dog', 'using', 'a', 'red', 'leash', '.'], ['an', 'elderly', 'man', 'takes', 'his', 'dog', 'for', 'a', 'walk', 'near', 'the', 'water', '.'], ['an', 'old', 'man', 'holding', 'a', 'camera', 'while', 'walking', 'a', 'small', 'brown', 'dog', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction: , a boy in a blue shirt is running with a soccer ball in his mouth.\n",
      "References: [['a', 'soccer', 'player', 'in', 'blue', 'is', 'chasing', 'after', 'the', 'player', 'in', 'black', 'and', 'white', '.'], ['the', 'girl', 'in', 'the', 'white', 'strip', 'is', 'falling', 'down', 'as', 'the', 'girl', 'in', 'the', 'blue', 'strip', 'challenges', 'for', 'the', 'soccer', 'ball', '.'], ['the', 'girls', 'are', 'playing', 'soccer', '.'], ['two', 'women', 'in', 'soccer', 'uniforms', 'playing', 'soccer', '.'], ['two', 'young', 'women', 'on', 'different', 'teams', 'are', 'playing', 'soccer', 'on', 'a', 'field', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Epoch [8/10] completed in 1107.44s\n",
      "Training Loss: 1.7735, Validation Loss: 2.4801\n",
      "BLEU Score: 0.0994, METEOR Score: 0.3719, CIDEr Score: 0.3479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  24%|██▍       | 243/1011 [02:58<09:22,  1.37it/s, loss=1.47]"
     ]
    }
   ],
   "source": [
    "# % load_ext autoreload   \n",
    "# % autoreload 2\n",
    "# Initialize metrics storage\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "bleu_scores = []\n",
    "meteor_scores = []\n",
    "cider_scores = []\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# Total training steps\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Prepare image to captions mapping for evaluation\n",
    "val_image2captions = prepare_image2captions(val_images, captions_seqs_nltk, idx2word_nltk)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for i, (images, captions, _) in enumerate(pbar):\n",
    "        # if i == 5: break # comment out later\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        captions[captions == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=images, labels=captions)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        pbar_val = tqdm(val_loader, desc=\"Validation\")\n",
    "        for i, (images, captions, _) in enumerate(pbar_val):\n",
    "            # if i == 5: break # comment out later\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            captions[captions == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            outputs = model(pixel_values=images, labels=captions)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            pbar_val.set_postfix({\"val_loss\": loss.item()})\n",
    "\n",
    "    val_loss = total_val_loss / len(val_loader)  # average validation loss\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    enable_wandb = True\n",
    "    # Calculate evaluation metrics\n",
    "    metrics = calculate_metrics(\n",
    "        model=model,\n",
    "        image_dir=config[\"image_dir\"],\n",
    "        image_ids=val_images,\n",
    "        image2captions=val_image2captions,\n",
    "        transform=val_transform,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        max_length=config[\"max_length\"],\n",
    "        verbose=True,\n",
    "        enable_wandb=enable_wandb\n",
    "    )\n",
    "    bleu_scores.append(metrics[\"bleu\"])\n",
    "    meteor_scores.append(metrics[\"meteor\"])\n",
    "    cider_scores.append(metrics[\"cider\"])\n",
    "    \n",
    "\n",
    "    log_data = {\n",
    "    \"epoch\": epoch + 1,\n",
    "    \"train_loss\": avg_train_loss,\n",
    "    \"val_loss\": val_loss,\n",
    "    \"bleu\": metrics[\"bleu\"],\n",
    "    \"meteor\": metrics[\"meteor\"],\n",
    "    \"cider\": metrics[\"cider\"],\n",
    "    }\n",
    "    \n",
    "    if enable_wandb:\n",
    "        log_data[f\"Sample Predictions/Epoch_{epoch + 1}\"] = metrics[\"sample_table\"]\n",
    "    \n",
    "    wandb.log(log_data)\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Print epoch summary including new metrics\n",
    "    epoch_duration = time.time() - start_time\n",
    "    print(\n",
    "        f\"\\nEpoch [{epoch+1}/{num_epochs}] completed in {epoch_duration:.2f}s\"\n",
    "        f\"\\nTraining Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        f\"\\nBLEU Score: {metrics['bleu']:.4f}, METEOR Score: {metrics['meteor']:.4f}, CIDEr Score: {metrics['cider']:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix = val_transform(Image.open(os.path.join(config['image_dir'], val_images[0])).convert('RGB')).unsqueeze(0).to(device)\n",
    "model.generate(pix,return_dict_in_generate=True,pad_token_id=tokenizer.pad_token_id)\n",
    "# model.val_transformgenerate("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply save the model\n",
    "model.save_pretrained(config['model_save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
