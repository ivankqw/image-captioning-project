{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T02:58:16.025754Z",
     "iopub.status.busy": "2024-12-05T02:58:16.025613Z",
     "iopub.status.idle": "2024-12-05T02:58:18.165017Z",
     "shell.execute_reply": "2024-12-05T02:58:18.164562Z",
     "shell.execute_reply.started": "2024-12-05T02:58:16.025738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocoevalcap in /usr/local/lib/python3.11/dist-packages (1.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.26.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ! pip install -q flash-attn --no-build-isolation\n",
    "# ! pip install transformers\n",
    "! pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T03:12:31.748258Z",
     "iopub.status.busy": "2024-12-05T03:12:31.747772Z",
     "iopub.status.idle": "2024-12-05T03:12:31.751857Z",
     "shell.execute_reply": "2024-12-05T03:12:31.751372Z",
     "shell.execute_reply.started": "2024-12-05T03:12:31.748253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Hyperparameters\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    'dataset': 'Flickr8k',             # Options: 'Flickr8k', 'Flickr30k'\n",
    "    'image_dir': './flickr_data/Flickr8k_Dataset/Images',  # Path to images\n",
    "    'captions_file': './flickr_data/Flickr8k_Dataset/captions.txt',  # Path to captions\n",
    "    'vocab_size': 5000,                # Maximum vocabulary size\n",
    "    'embed_size': 256,                  # Embedding size (optional if not using separate embeddings)\n",
    "    'hidden_size': 512,                 # Hidden size for decoder (not directly used with Hugging Face models)\n",
    "    'batch_size': 32,                   # Batch size\n",
    "    'num_epochs': 10,                   # Number of training epochs\n",
    "    'learning_rate': 5e-5,              # Learning rate\n",
    "    'weight_decay': 1e-4,               # Weight decay for optimizer\n",
    "    'max_length': 50,                   # Maximum caption length for generation\n",
    "    'num_beams': 1,                     # Number of beams for beam search\n",
    "    # 'save_dir': 'models/',              # Directory to save models and plots\n",
    "    'seed': 42                          # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Image.open(os.path.join(image_dir, \"2967549094_d32422eb01.jpg\"))# in os.listdir(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T04:15:26.584259Z",
     "iopub.status.busy": "2024-12-05T04:15:26.583488Z",
     "iopub.status.idle": "2024-12-05T04:15:26.700739Z",
     "shell.execute_reply": "2024-12-05T04:15:26.700332Z",
     "shell.execute_reply.started": "2024-12-05T04:15:26.584237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /notebooks\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    GPT2Tokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "notebook_dir = Path(os.getcwd()).resolve()  # Get the current working directory\n",
    "project_root = notebook_dir.parents[1]  # Adjust the number to go up to the project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Some additional settings for full reproducibility (optional)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(hyperparams[\"seed\"])\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T04:18:01.113245Z",
     "iopub.status.busy": "2024-12-05T04:18:01.112673Z",
     "iopub.status.idle": "2024-12-05T04:18:05.811316Z",
     "shell.execute_reply": "2024-12-05T04:18:05.810695Z",
     "shell.execute_reply.started": "2024-12-05T04:18:01.113223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions loaded: 40445\n",
      "Vocabulary size: 8921\n",
      "Maximum caption length: 40\n",
      "Training samples: 6472\n",
      "Validation samples: 1457\n",
      "Number of training batches: 1011\n",
      "Number of validation batches: 228\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import *\n",
    "from data.preprocessing import *\n",
    "\n",
    "\n",
    "dataset = \"Flickr8k\"  # Change to \"Flickr30k\" if needed\n",
    "\n",
    "# Paths\n",
    "dataset_dir = f\"../../flickr_data/{dataset}_Dataset/Images\"\n",
    "captions_file = f\"../../flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = dataset_dir\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "bleu_scores = []\n",
    "meteor_scores = []\n",
    "cider_scores = []\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file).dropna().drop_duplicates()\n",
    "print(f\"Total captions loaded: {len(caption_df)}\")\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=10000)\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "# Get data transformations\n",
    "train_transform = get_transform(train=True)\n",
    "val_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "image_names = list(image_captions.keys())\n",
    "train_images, val_images, _ = get_splits(image_names, test_size=0.2)\n",
    "print(f\"Training samples: {len(train_images)}\")\n",
    "print(f\"Validation samples: {len(val_images)}\")\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = FlickrDataset(\n",
    "    image_dir, train_images, captions_seqs, transform=train_transform\n",
    ")\n",
    "val_dataset = FlickrDataset(\n",
    "    image_dir, val_images, captions_seqs, transform=val_transform\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=hyperparams['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=hyperparams['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T03:22:31.588529Z",
     "iopub.status.busy": "2024-12-05T03:22:31.587928Z",
     "iopub.status.idle": "2024-12-05T03:22:36.821150Z",
     "shell.execute_reply": "2024-12-05T03:22:36.820756Z",
     "shell.execute_reply.started": "2024-12-05T03:22:31.588511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 special tokens to the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.8.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.bias', 'h.7.ln_cross_attn.weight', 'h.10.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.7.ln_cross_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.3.ln_cross_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.ln_cross_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.bias', 'h.8.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.1.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.0.ln_cross_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.2.crossattention.c_proj.bias', 'h.4.ln_cross_attn.weight', 'h.11.ln_cross_attn.weight', 'h.4.ln_cross_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.5.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.4.crossattention.c_attn.bias', 'h.3.crossattention.q_attn.bias', 'h.8.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias', 'h.5.ln_cross_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.5.ln_cross_attn.weight', 'h.8.ln_cross_attn.bias', 'h.4.crossattention.c_proj.bias', 'h.6.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.11.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.3.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.10.crossattention.c_attn.weight', 'h.1.ln_cross_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.10.ln_cross_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.11.crossattention.q_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.11.ln_cross_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.0.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel(\n",
      "  (encoder): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): ViTPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50260, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (crossattention): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (q_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPT2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add special tokens if not present\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<start>\",\n",
    "    \"eos_token\": \"<end>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "if num_added_tokens > 0:\n",
    "    print(f\"Added {num_added_tokens} special tokens to the tokenizer.\")\n",
    "\n",
    "# Initialize VisionEncoderDecoderModel\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-224\", \"gpt2\"\n",
    ")\n",
    "\n",
    "# Resize token embeddings to accommodate the new special tokens\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set special tokens for the model\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Set generation parameters\n",
    "model.config.max_length = hyperparams[\"max_length\"]\n",
    "model.config.num_beams = hyperparams[\"num_beams\"]\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T04:18:19.820594Z",
     "iopub.status.busy": "2024-12-05T04:18:19.820131Z",
     "iopub.status.idle": "2024-12-05T04:18:19.835382Z",
     "shell.execute_reply": "2024-12-05T04:18:19.834784Z",
     "shell.execute_reply.started": "2024-12-05T04:18:19.820574Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device, tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluate the VisionEncoderDecoderModel on the validation set.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        data_loader: DataLoader for the validation set.\n",
    "        criterion: Loss function.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        tokenizer: Tokenizer used for encoding captions.\n",
    "    Returns:\n",
    "        average_loss: Average validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images, captions, lengths = batch\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=images, labels=captions)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "    # Calculate average loss\n",
    "    average_loss = total_loss / total_samples\n",
    "    return average_loss\n",
    "\n",
    "def calculate_bleu_score(\n",
    "    model,\n",
    "    image_dir,\n",
    "    image_ids,\n",
    "    image2captions,\n",
    "    transform,\n",
    "    tokenizer,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for the generated captions.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        image_dir: Directory containing images.\n",
    "        image_ids: List of image IDs.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        tokenizer: Tokenizer used for encoding/decoding captions.\n",
    "        device: Computation device (CPU or GPU).\n",
    "    Returns:\n",
    "        bleu_score: Corpus BLEU score for generated captions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(image_ids, desc=\"Calculating BLEU\"):\n",
    "            img_path = os.path.join(image_dir, img_id)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption with greedy search (num_beams=1)\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=image,\n",
    "                max_length=hyperparams['max_length'],\n",
    "                num_beams=1,  # Greedy search\n",
    "                do_sample=False\n",
    "            )\n",
    "            generated_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Prepare hypothesis\n",
    "            hypothesis = word_tokenize(generated_caption.lower())\n",
    "            hypotheses.append(hypothesis)\n",
    "\n",
    "            # Prepare references\n",
    "            ref_captions = image2captions.get(img_id, [])\n",
    "            refs = [word_tokenize(' '.join(ref).lower()) for ref in ref_captions]\n",
    "            references.append(refs)\n",
    "\n",
    "    # Compute corpus BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "    return bleu_score\n",
    "\n",
    "def calculate_meteor_score(\n",
    "    model,\n",
    "    image_dir,\n",
    "    image_ids,\n",
    "    image2captions,\n",
    "    transform,\n",
    "    tokenizer,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate METEOR score for the generated captions.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        image_dir: Directory containing images.\n",
    "        image_ids: List of image IDs.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        tokenizer: Tokenizer used for encoding/decoding captions.\n",
    "        device: Computation device (CPU or GPU).\n",
    "    Returns:\n",
    "        average_meteor: Average METEOR score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    meteor_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(image_ids, desc=\"Calculating METEOR\"):\n",
    "            img_path = os.path.join(image_dir, img_id)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption with greedy search (num_beams=1)\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=image,\n",
    "                max_length=hyperparams['max_length'],\n",
    "                num_beams=1,  # Greedy search\n",
    "                do_sample=False\n",
    "            )\n",
    "            # generated_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            generated_caption = tokenizer.decode(\n",
    "                generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "\n",
    "            # Prepare hypothesis\n",
    "            hypothesis = word_tokenize(generated_caption.lower().strip())\n",
    "\n",
    "            # Prepare references\n",
    "            ref_captions = image2captions.get(img_id, [])\n",
    "            tokenized_refs = [\n",
    "                nltk.word_tokenize(\" \".join(ref).lower().strip()) for ref in ref_captions\n",
    "            ]\n",
    "\n",
    "            if tokenized_refs:\n",
    "                # Compute METEOR score for each reference and take the maximum\n",
    "                scores = [\n",
    "                    single_meteor_score(reference, hypothesis) for reference in tokenized_refs\n",
    "                ]\n",
    "                score = max(scores)\n",
    "            else:\n",
    "                print(f\"No reference captions for image {img_id}. Assigning score 0.\")\n",
    "                score = 0  # Assign a default score or handle as needed\n",
    "\n",
    "            meteor_scores.append(score)\n",
    "\n",
    "    # Compute average METEOR score\n",
    "    average_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "    return average_meteor\n",
    "\n",
    "def calculate_cider_score(\n",
    "    model,\n",
    "    image_dir,\n",
    "    image_ids,\n",
    "    image2captions,\n",
    "    transform,\n",
    "    tokenizer,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate CIDEr score for the generated captions.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        image_dir: Directory containing images.\n",
    "        image_ids: List of image IDs.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        tokenizer: Tokenizer used for encoding/decoding captions.\n",
    "        device: Computation device (CPU or GPU).\n",
    "    Returns:\n",
    "        cider_score: CIDEr score for generated captions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    gts = {}  # Ground truth captions\n",
    "    res = {}  # Generated captions\n",
    "    tokenizer_cider = PTBTokenizer()  # Tokenizer for captions\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(image_ids, desc=\"Calculating CIDEr\"):\n",
    "            img_path = os.path.join(image_dir, img_id)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption with greedy search (num_beams=1)\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=image,\n",
    "                max_length=hyperparams['max_length'],\n",
    "                num_beams=1,  # Greedy search\n",
    "                do_sample=False\n",
    "            )\n",
    "            generated_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Prepare generated caption\n",
    "            sampled_caption = ' '.join(word_tokenize(generated_caption.lower()))\n",
    "\n",
    "            # Prepare references\n",
    "            references = [' '.join(ref).lower() for ref in image2captions.get(img_id, [])]\n",
    "\n",
    "            # Update dictionaries with tokenized captions\n",
    "            gts[img_id] = [{'caption': ref} for ref in references]\n",
    "            res[img_id] = [{'caption': sampled_caption}]\n",
    "\n",
    "    # Tokenize captions\n",
    "    gts = tokenizer_cider.tokenize(gts)\n",
    "    res = tokenizer_cider.tokenize(res)\n",
    "\n",
    "    # Compute CIDEr score\n",
    "    cider_scorer = Cider()\n",
    "    cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "    return cider_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T04:18:26.575918Z",
     "iopub.status.busy": "2024-12-05T04:18:26.575152Z",
     "iopub.status.idle": "2024-12-05T05:28:46.332372Z",
     "shell.execute_reply": "2024-12-05T05:28:46.331288Z",
     "shell.execute_reply.started": "2024-12-05T04:18:26.575896Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/10:   1%|          | 10/1011 [00:07<12:19,  1.35it/s, loss=1.87]\n",
      "Evaluating: 100%|██████████| 228/228 [00:49<00:00,  4.58it/s]\n",
      "Calculating BLEU: 100%|██████████| 1457/1457 [14:31<00:00,  1.67it/s]\n",
      "Calculating METEOR: 100%|██████████| 1457/1457 [14:21<00:00,  1.69it/s]\n",
      "Calculating CIDEr: 100%|██████████| 1457/1457 [14:21<00:00,  1.69it/s]\n",
      "PTBTokenizer tokenized 92805 tokens at 555797.20 tokens per second.\n",
      "Dec 05, 2024 5:02:38 AM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "PTBTokenizer tokenized 69805 tokens at 358369.46 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10] completed in 2653.21s\n",
      "Training Loss: 0.0185, Validation Loss: 1.8605\n",
      "BLEU Score: 0.0000, METEOR Score: 0.0092, CIDEr Score: 0.0002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   1%|          | 10/1011 [00:07<12:14,  1.36it/s, loss=1.69]\n",
      "Evaluating: 100%|██████████| 228/228 [00:50<00:00,  4.50it/s]\n",
      "Calculating BLEU: 100%|██████████| 1457/1457 [14:15<00:00,  1.70it/s]\n",
      "Calculating METEOR:  76%|███████▌  | 1110/1457 [10:52<03:23,  1.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 98\u001b[0m\n\u001b[1;32m     86\u001b[0m bleu \u001b[38;5;241m=\u001b[39m calculate_bleu_score(\n\u001b[1;32m     87\u001b[0m     model,\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# hyperparams[\"image_dir\"],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     device,\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     96\u001b[0m bleu_scores\u001b[38;5;241m.\u001b[39mappend(bleu)\n\u001b[0;32m---> 98\u001b[0m meteor \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_meteor_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hyperparams[\"image_dir\"],\u001b[39;49;00m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_image2captions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m meteor_scores\u001b[38;5;241m.\u001b[39mappend(meteor)\n\u001b[1;32m    110\u001b[0m cider \u001b[38;5;241m=\u001b[39m calculate_cider_score(\n\u001b[1;32m    111\u001b[0m     model,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# hyperparams[\"image_dir\"],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     device,\n\u001b[1;32m    119\u001b[0m )\n",
      "Cell \u001b[0;32mIn[24], line 120\u001b[0m, in \u001b[0;36mcalculate_meteor_score\u001b[0;34m(model, image_dir, image_ids, image2captions, transform, tokenizer, device)\u001b[0m\n\u001b[1;32m    117\u001b[0m image \u001b[38;5;241m=\u001b[39m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Generate caption with greedy search (num_beams=1)\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Greedy search\u001b[39;49;00m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# generated_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m generated_caption \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m    128\u001b[0m     generated_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1673\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1657\u001b[0m         input_ids,\n\u001b[1;32m   1658\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1670\u001b[0m     )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2521\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2521\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:604\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[0;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m    600\u001b[0m         labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[1;32m    619\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:426\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    423\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attn_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize metrics storage\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "bleu_scores = []\n",
    "meteor_scores = []\n",
    "cider_scores = []\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=hyperparams[\"learning_rate\"],\n",
    "    weight_decay=hyperparams[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# Total training steps\n",
    "num_epochs = hyperparams[\"num_epochs\"]\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "# total_steps = 10\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',           # We want to minimize the validation loss\n",
    "    factor=0.5,           # Factor by which the learning rate will be reduced\n",
    "    patience=2,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=True          # Print a message when the learning rate is updated\n",
    ")\n",
    "\n",
    "# Prepare image to captions mapping for evaluation\n",
    "val_image2captions = prepare_image2captions(val_images, captions_seqs, idx2word)\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # train_steps = 0 # comment this out later\n",
    "\n",
    "    for batch in pbar:\n",
    "        # if train_steps >= 10: # comment this out later\n",
    "        #     break\n",
    "        \n",
    "        images, captions, lengths = batch\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=images, labels=captions)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # train_steps += 1 # comment this out later\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = evaluate_model(model, val_loader, criterion, device, tokenizer)\n",
    "    scheduler.step(val_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    bleu = calculate_bleu_score(\n",
    "        model,\n",
    "        # hyperparams[\"image_dir\"],\n",
    "        dataset_dir,\n",
    "        val_images,\n",
    "        val_image2captions,\n",
    "        get_transform(train=False),\n",
    "        tokenizer,\n",
    "        device,\n",
    "    )\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    meteor = calculate_meteor_score(\n",
    "        model,\n",
    "        dataset_dir,\n",
    "        val_images,\n",
    "        val_image2captions,\n",
    "        get_transform(train=False),\n",
    "        tokenizer,\n",
    "        device,\n",
    "    )\n",
    "    meteor_scores.append(meteor)\n",
    "\n",
    "    cider = calculate_cider_score(\n",
    "        model,\n",
    "        dataset_dir,\n",
    "        val_images,\n",
    "        val_image2captions,\n",
    "        get_transform(train=False),\n",
    "        tokenizer,\n",
    "        device,\n",
    "    )\n",
    "    cider_scores.append(cider)\n",
    "\n",
    "    # Print epoch summary\n",
    "    epoch_duration = time.time() - start_time\n",
    "    print(\n",
    "        f\"\\nEpoch [{epoch+1}/{num_epochs}] completed in {epoch_duration:.2f}s\"\n",
    "        f\"\\nTraining Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        f\"\\nBLEU Score: {bleu:.4f}, METEOR Score: {meteor:.4f}, CIDEr Score: {cider:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-05T01:04:18.759592Z",
     "iopub.status.idle": "2024-12-05T01:04:18.759780Z",
     "shell.execute_reply": "2024-12-05T01:04:18.759700Z",
     "shell.execute_reply.started": "2024-12-05T01:04:18.759690Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create save directory if it doesn't exist\n",
    "os.makedirs(hyperparams['save_dir'], exist_ok=True)\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "loss_plot_path = os.path.join(hyperparams['save_dir'], 'loss_plot.png')\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.show()\n",
    "print(f\"Loss plot saved to {loss_plot_path}\")\n",
    "\n",
    "# Plot Evaluation Metrics\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, num_epochs + 1), bleu_scores, label='BLEU Score')\n",
    "plt.plot(range(1, num_epochs + 1), meteor_scores, label='METEOR Score')\n",
    "plt.plot(range(1, num_epochs + 1), cider_scores, label='CIDEr Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Evaluation Metrics over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "metrics_plot_path = os.path.join(hyperparams['save_dir'], 'metrics_plot.png')\n",
    "plt.savefig(metrics_plot_path)\n",
    "plt.show()\n",
    "print(f\"Metrics plot saved to {metrics_plot_path}\")\n",
    "\n",
    "# Save the trained model's state dictionary\n",
    "model_save_path = os.path.join(hyperparams['save_dir'], f\"vision_encoder_decoder_{hyperparams['dataset']}.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_save_path = os.path.join(hyperparams['save_dir'], \"tokenizer\")\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "print(f\"Tokenizer saved to {tokenizer_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T05:37:16.370097Z",
     "iopub.status.busy": "2024-12-05T05:37:16.369836Z",
     "iopub.status.idle": "2024-12-05T05:37:24.950799Z",
     "shell.execute_reply": "2024-12-05T05:37:24.950166Z",
     "shell.execute_reply.started": "2024-12-05T05:37:16.370076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 2714703706_d21c5cb8df.jpg\n",
      "Generated Caption: \"0C4F)%�&#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Ground Truth Captions:\n",
      "- dogs playing\n",
      "- a brown dog is biting a white and tan dog on the <unk> .\n",
      "- the brown dog has a hold of the other dogs cheek with its teeth .\n",
      "- two dogs are nuzzling each other nose to nose .\n",
      "- two dogs bite at each other on the carpet .\n",
      "--------------------------------------------------\n",
      "Image ID: 3532194771_07faf20d76.jpg\n",
      "Generated Caption: \"%,*D)(f&#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Ground Truth Captions:\n",
      "- a man is heading out to see with his surfboard in hand .\n",
      "- a man with a white surfboard is walking into the water .\n",
      "- a person walks into the water carrying a white surfboard .\n",
      "- a surfer walking into the ocean\n",
      "- surfer with board marches out to sea on gray day .\n",
      "--------------------------------------------------\n",
      "Image ID: 2356574282_5078f08b58.jpg\n",
      "Generated Caption: \"0;4'%=+1�&#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Ground Truth Captions:\n",
      "- a arabian dressed lady leans backwards holding a skinny crooked sword .\n",
      "- a costumed woman with a sword does a <unk> .\n",
      "- a woman bending over backwards .\n",
      "- a woman in a belly dancing outfit bending over backwards .\n",
      "- a woman in a dance costume is bending over backward and holding a sword .\n",
      "--------------------------------------------------\n",
      "Image ID: 3526150930_580908dab6.jpg\n",
      "Generated Caption: \"0t4U)%�&#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Ground Truth Captions:\n",
      "- a woman and a young girl pose and smile for a photo .\n",
      "- a woman and a young girl smiling for the camera , in front of some flowers .\n",
      "- a woman and girl pose together in a garden .\n",
      "- a woman poses with a small girl on her lap in front of a flower bush .\n",
      "- a woman with brown hair is sitting with a little girl with short brown hair outside next to some red flowers .\n",
      "--------------------------------------------------\n",
      "Image ID: 2448270671_5e0e391a80.jpg\n",
      "Generated Caption: \"%,iR%�&#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Ground Truth Captions:\n",
      "- a brown dog is leaping over a fallen tree in the woods .\n",
      "- a brown dog with tongue sticking out jumping over a log .\n",
      "- a dog is jumping over a log with ears flying and tongue out .\n",
      "- a dog leaps over a log in the woods .\n",
      "- the dog with big ears is leaping over a fallen tree .\n",
      "--------------------------------------------------\n",
      "Image ID: 3482237861_605b4f0fd9.jpg\n",
      "Generated Caption: \"%3�H%�&#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Ground Truth Captions:\n",
      "- a boy is riding a scooter down the concrete path .\n",
      "- a child on a scooter moving down the sidewalk .\n",
      "- a youth rides a scooter on a sidewalk near a building .\n",
      "- the boy is riding his scooter on the sidewalk .\n",
      "- young boy rides his scooter on drive .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Flickr8k\"\n",
    "\n",
    "captions_file_path = f\"{project_root}/flickr_data/{dataset}_Dataset/captions.txt\"\n",
    "image_dir = f\"{project_root}/flickr_data/{dataset}_Dataset/Images\"\n",
    "\n",
    "# Load captions\n",
    "caption_df = pd.read_csv(captions_file_path).dropna().drop_duplicates()\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word, image_captions = build_vocabulary(caption_df, vocab_size=5000)\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_seqs, max_length = convert_captions_to_sequences(image_captions, word2idx)\n",
    "\n",
    "# Get data transformations\n",
    "test_transform = get_transform(train=False)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "image_names = list(image_captions.keys())\n",
    "_, _, test_images = get_splits(image_names, test_size=0.2)\n",
    "\n",
    "# Prepare image to captions mapping for ground truth captions\n",
    "test_image2captions = prepare_image2captions(test_images, captions_seqs, idx2word)\n",
    "\n",
    "# Create test dataset and data loader\n",
    "test_dataset = FlickrDataset(\n",
    "    image_dir, test_images, captions_seqs, transform=test_transform, mode='test'\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Process one image at a time\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Function to generate and display captions for a given number of test images\n",
    "def generate_captions(model, test_loader, image2captions, transform, tokenizer, device, num_images=6):\n",
    "    \"\"\"\n",
    "    Generate captions for test images using greedy search and display them alongside ground truth captions.\n",
    "    Args:\n",
    "        model: VisionEncoderDecoderModel.\n",
    "        test_loader: DataLoader for the test set.\n",
    "        image2captions: Dictionary mapping image IDs to reference captions.\n",
    "        transform: Preprocessing transformation for images.\n",
    "        tokenizer: Tokenizer used for encoding/decoding captions.\n",
    "        device: Computation device (CPU or GPU).\n",
    "        num_images (int): Number of test images to process.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, image_ids) in enumerate(test_loader):\n",
    "            if i >= num_images:\n",
    "                break  # Stop after processing 'num_images' images\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Generate caption with greedy search (num_beams=1)\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=images,\n",
    "                max_length=hyperparams['max_length'],\n",
    "                num_beams=1,  # Greedy search\n",
    "                do_sample=False\n",
    "            )\n",
    "            generated_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "            # Prepare hypothesis\n",
    "            hypothesis = word_tokenize(generated_caption.lower())\n",
    "\n",
    "            # Retrieve ground truth captions\n",
    "            image_name = image_ids[0]\n",
    "            gt_captions = image2captions.get(image_name, [])\n",
    "\n",
    "            # Remove unknown tokens and punctuation (optional)\n",
    "            # Here, assuming <unk>, <start>, <end> are already handled in preprocessing\n",
    "\n",
    "            # Print generated and ground truth captions\n",
    "            print(f\"Image ID: {image_name}\")\n",
    "            print(f\"Generated Caption: {generated_caption}\")\n",
    "            print(\"Ground Truth Captions:\")\n",
    "            for gt_caption in gt_captions:\n",
    "                gt_caption_str = ' '.join(gt_caption)\n",
    "                print(f\"- {gt_caption_str}\")\n",
    "            print('-' * 50)\n",
    "\n",
    "# Choose the number of images to generate captions for\n",
    "num_test_images = 6\n",
    "\n",
    "# Generate captions\n",
    "generate_captions(\n",
    "    model,\n",
    "    test_loader,\n",
    "    test_image2captions,\n",
    "    get_transform(train=False),\n",
    "    tokenizer,\n",
    "    device,\n",
    "    num_images=num_test_images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
